"""
Rerankeræ‰‹åŠ¨åŠ è½½ç‰ˆ - ç»•è¿‡torchç‰ˆæœ¬æ£€æŸ¥
ç›´æ¥åŠ è½½safetensorsæ–‡ä»¶ï¼Œä¸ä¾èµ–transformersçš„è‡ªåŠ¨åŠ è½½æœºåˆ¶
"""
from typing import List, Dict, Any
import torch
import os
from pathlib import Path


class ManualBCEReranker:
    """æ‰‹åŠ¨åŠ è½½BCE Reranker - ç»•è¿‡torchç‰ˆæœ¬æ£€æŸ¥"""
    
    def __init__(self, model_name: str = "BAAI/bge-reranker-base"):
        """
        åˆå§‹åŒ–Rerankerï¼ˆæ‰‹åŠ¨åŠ è½½æ–¹å¼ï¼‰
        
        Args:
            model_name: Hugging Faceæ¨¡å‹åç§°
        """
        print(f"[Reranker-æ‰‹åŠ¨] æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {model_name}")
        
        self.enabled = False
        
        try:
            # 1. æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½æ¨¡å‹
            cache_dir = self._get_model_cache_dir(model_name)
            if not cache_dir:
                print(f"[Reranker-æ‰‹åŠ¨] æ¨¡å‹æœªä¸‹è½½ï¼Œæ­£åœ¨ä¸‹è½½...")
                cache_dir = self._download_model(model_name)
            
            print(f"[Reranker-æ‰‹åŠ¨] æ¨¡å‹ç¼“å­˜ä½ç½®: {cache_dir}")
            
            # 2. æ‰‹åŠ¨åŠ è½½tokenizer
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                cache_dir,
                local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            print(f"[Reranker-æ‰‹åŠ¨] âœ… TokenizeråŠ è½½å®Œæˆ")
            
            # 3. æ‰‹åŠ¨åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨safetensorsï¼Œç»•è¿‡torch.loadæ£€æŸ¥ï¼‰
            from transformers import AutoModelForSequenceClassification
            from safetensors.torch import load_file
            
            # æŸ¥æ‰¾safetensorsæ–‡ä»¶ï¼ˆæ”¯æŒå¤šç§ç¼“å­˜ç»“æ„ï¼‰
            safetensors_files = list(Path(cache_dir).glob("*.safetensors"))
            
            if not safetensors_files:
                # æ–¹æ³•1: å°è¯•ä»å½“å‰ç›®å½•çš„å­ç›®å½•æŸ¥æ‰¾
                for sub_dir in Path(cache_dir).rglob("*.safetensors"):
                    safetensors_files.append(sub_dir)
                    if safetensors_files:
                        cache_dir = str(sub_dir.parent)
                        break
            
            if not safetensors_files:
                # æ–¹æ³•2: å°è¯•ä»snapshotsç›®å½•æŸ¥æ‰¾
                cache_path = Path(cache_dir)
                if "snapshots" in cache_path.parts:
                    # å·²ç»åœ¨snapshotsç›®å½•ä¸­
                    pass
                else:
                    # å°è¯•æŸ¥æ‰¾snapshotsç›®å½•
                    snapshots_dir = cache_path.parent / "snapshots"
                    if snapshots_dir.exists():
                        for snapshot_dir in snapshots_dir.iterdir():
                            if snapshot_dir.is_dir():
                                safetensors_files = list(snapshot_dir.glob("*.safetensors"))
                                if safetensors_files:
                                    cache_dir = str(snapshot_dir)
                                    break
            
            # å¦‚æœæ²¡æœ‰safetensorsæ–‡ä»¶ï¼Œè‡ªåŠ¨ä¸‹è½½
            if not safetensors_files:
                print(f"[Reranker-æ‰‹åŠ¨] âš ï¸ æœªæ‰¾åˆ°safetensorsæ–‡ä»¶ï¼Œæ­£åœ¨ä¸‹è½½...")
                print(f"[Reranker-æ‰‹åŠ¨] ğŸ’¡ è¿™éœ€è¦çº¦400MBï¼Œé¦–æ¬¡ä¸‹è½½çº¦éœ€1-2åˆ†é’Ÿ")
                cache_dir = self._download_model(model_name)
                safetensors_files = list(Path(cache_dir).glob("*.safetensors"))
                
                if not safetensors_files:
                    raise FileNotFoundError(f"ä¸‹è½½åä»æœªæ‰¾åˆ°safetensorsæ–‡ä»¶")
            
            print(f"[Reranker-æ‰‹åŠ¨] æ‰¾åˆ°safetensorsæ–‡ä»¶: {safetensors_files[0].name}")
            
            # åŠ è½½config
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(cache_dir, local_files_only=True)
            
            # åˆ›å»ºæ¨¡å‹ï¼ˆä¸åŠ è½½æƒé‡ï¼‰
            self.model = AutoModelForSequenceClassification.from_config(config)
            
            # æ‰‹åŠ¨åŠ è½½safetensorsæƒé‡
            print(f"[Reranker-æ‰‹åŠ¨] ğŸ”„ æ‰‹åŠ¨åŠ è½½safetensorsæƒé‡...")
            state_dict = load_file(str(safetensors_files[0]))
            
            # ä½¿ç”¨strict=Falseï¼Œå…è®¸å¿½ç•¥ä¸åŒ¹é…çš„keyï¼ˆå¦‚position_idsï¼‰
            missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
            if unexpected_keys:
                print(f"[Reranker-æ‰‹åŠ¨] â„¹ï¸ å¿½ç•¥çš„é”®: {unexpected_keys[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"[Reranker-æ‰‹åŠ¨] âœ… æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
            
            # 4. ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                self.model = self.model.cuda()
                self.device = 'cuda'
                print(f"[Reranker-æ‰‹åŠ¨] âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name(0)}")
            else:
                self.device = 'cpu'
                print(f"[Reranker-æ‰‹åŠ¨] âš ï¸ ä½¿ç”¨CPUï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
            
            self.model.eval()
            self.enabled = True
            print(f"[Reranker-æ‰‹åŠ¨] âœ… æ‰‹åŠ¨åŠ è½½å®Œæˆï¼")
            
        except Exception as e:
            print(f"[Reranker-æ‰‹åŠ¨] âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.enabled = False
    
    def _get_model_cache_dir(self, model_name: str) -> str:
        """è·å–æ¨¡å‹ç¼“å­˜ç›®å½•"""
        from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        try:
            # æ£€æŸ¥config.jsonæ˜¯å¦å­˜åœ¨
            cached_file = try_to_load_from_cache(
                repo_id=model_name,
                filename="config.json"
            )
            if cached_file and cached_file != _CACHED_NO_EXIST:
                return str(Path(cached_file).parent)
        except:
            pass
        
        return None
    
    def _download_model(self, model_name: str) -> str:
        """ä¸‹è½½æ¨¡å‹ï¼ˆåªä¸‹è½½safetensorsæ–‡ä»¶ï¼‰"""
        from huggingface_hub import snapshot_download
        
        print(f"[Reranker-æ‰‹åŠ¨] æ­£åœ¨ä¸‹è½½æ¨¡å‹ï¼ˆåªä¸‹è½½safetensorså’Œé…ç½®æ–‡ä»¶ï¼‰...")
        print(f"[Reranker-æ‰‹åŠ¨] æç¤º: çº¦400MBï¼Œå¯èƒ½éœ€è¦1-2åˆ†é’Ÿ")
        
        try:
            cache_dir = snapshot_download(
                repo_id=model_name,
                allow_patterns=["*.safetensors", "*.json", "tokenizer*", "vocab.txt", "special_tokens_map.json"],
                ignore_patterns=["*.bin", "*.msgpack", "*.h5", "*.onnx"],  # æ˜ç¡®å¿½ç•¥.binæ–‡ä»¶
                resume_download=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            )
            print(f"[Reranker-æ‰‹åŠ¨] âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {cache_dir}")
            return cache_dir
        except Exception as e:
            print(f"[Reranker-æ‰‹åŠ¨] âŒ ä¸‹è½½å¤±è´¥: {e}")
            print(f"[Reranker-æ‰‹åŠ¨] ğŸ’¡ æ‰‹åŠ¨ä¸‹è½½å‘½ä»¤:")
            print(f"   huggingface-cli download {model_name} --include '*.safetensors' --include '*.json' --include 'tokenizer*'")
            raise
    
    def rerank(
        self, 
        query: str, 
        results: List[Dict[str, Any]], 
        top_k: int = 10,
        query_max_length: int = 512,
        passage_max_length: int = 512,
        batch_size: int = 32
    ) -> List[Dict[str, Any]]:
        """
        å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åºï¼ˆåˆ†æ‰¹å¤„ç†ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            results: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªç»“æœå¿…é¡»æœ‰'content'å­—æ®µï¼‰
            top_k: è¿”å›top-Kç»“æœ
            query_max_length: queryæœ€å¤§é•¿åº¦
            passage_max_length: passageæœ€å¤§é•¿åº¦
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤32ï¼Œ8GBæ˜¾å­˜å»ºè®®16-32ï¼‰
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨ï¼ˆæ·»åŠ äº†'rerank_score'å­—æ®µï¼‰
        """
        if not self.enabled:
            print(f"[Reranker-æ‰‹åŠ¨] âš ï¸ Rerankeræœªå¯ç”¨ï¼Œè¿”å›åŸå§‹ç»“æœ")
            for i, result in enumerate(results):
                result['rerank_score'] = 0.0
                result['original_rank'] = i + 1
            return results[:top_k]
        
        if not results:
            return []
        
        num_batches = (len(results) + batch_size - 1) // batch_size
        print(f"[Reranker-æ‰‹åŠ¨] ğŸ”„ æ­£åœ¨ç²¾æ’ {len(results)} ä¸ªå€™é€‰æ–‡æ¡£ï¼ˆåˆ†{num_batches}æ‰¹ï¼Œæ¯æ‰¹{batch_size}ä¸ªï¼‰...")
        
        try:
            all_scores = []
            
            # â­ åˆ†æ‰¹å¤„ç†ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min((batch_idx + 1) * batch_size, len(results))
                batch_results = results[start_idx:end_idx]
                
                # æ„å»ºquery-passageå¯¹
                pairs = []
                for r in batch_results:
                    passage = r.get('content', '')[:passage_max_length]
                    pairs.append((query[:query_max_length], passage))
                
                # æ‰¹é‡tokenize
                encoded = self.tokenizer(
                    pairs,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                )
                
                # ç§»åŠ¨åˆ°GPU
                if self.device == 'cuda':
                    encoded = {k: v.cuda() for k, v in encoded.items()}
                
                # æ¨ç†
                with torch.no_grad():
                    outputs = self.model(**encoded)
                    batch_scores = outputs.logits.squeeze(-1).cpu().numpy()
                
                # æ”¶é›†åˆ†æ•°
                if batch_scores.ndim == 0:
                    all_scores.append(float(batch_scores))
                else:
                    all_scores.extend(batch_scores.tolist())
                
                # â­ æ¸…ç†GPUç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜ç´¯ç§¯
                if self.device == 'cuda':
                    del encoded, outputs
                    torch.cuda.empty_cache()
            
            # å°†åˆ†æ•°æ·»åŠ åˆ°ç»“æœä¸­
            for i, result in enumerate(results):
                result['rerank_score'] = float(all_scores[i])
                result['original_rank'] = i + 1
            
            # æŒ‰ç²¾æ’åˆ†æ•°æ’åº
            results.sort(key=lambda x: x['rerank_score'], reverse=True)
            
            print(f"[Reranker-æ‰‹åŠ¨] âœ… ç²¾æ’å®Œæˆï¼Œè¿”å›top-{min(top_k, len(results))} ç»“æœ")
            
            # æ‰“å°å‰5ä¸ªç»“æœçš„åˆ†æ•°å¯¹æ¯”ï¼ˆå¢åŠ è°ƒè¯•ä¿¡æ¯ï¼‰
            for i, r in enumerate(results[:5]):
                original_rank = r.get('original_rank', '?')
                rerank_score = r.get('rerank_score', 0)
                original_score = r.get('similarity', 0)
                doc_id = r.get('doc_id', 'N/A')
                title = r.get('title', 'N/A')[:30] if r.get('title') else 'N/A'
                print(f"  [{i+1}] åŸæ’å:{original_rank}, å‘é‡åˆ†:{original_score:.4f}, ç²¾æ’åˆ†:{rerank_score:.4f}, doc_id:{doc_id}, title:{title}...")
            
            return results[:top_k]
            
        except Exception as e:
            print(f"[Reranker-æ‰‹åŠ¨] âŒ ç²¾æ’å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            for i, result in enumerate(results):
                if 'rerank_score' not in result:
                    result['rerank_score'] = 0.0
                if 'original_rank' not in result:
                    result['original_rank'] = i + 1
            return results[:top_k]


# å…¨å±€å•ä¾‹
_manual_reranker_instance = None


def get_manual_reranker() -> ManualBCEReranker:
    """è·å–æ‰‹åŠ¨åŠ è½½çš„Rerankerå•ä¾‹"""
    global _manual_reranker_instance
    if _manual_reranker_instance is None:
        _manual_reranker_instance = ManualBCEReranker()
    return _manual_reranker_instance

