"""
å‘é‡æ•°æ®åº“æ¨¡å— - Milvus + GPU å®ç°
ä½¿ç”¨GPUåŠ é€Ÿçš„å‘é‡æ£€ç´¢ï¼Œæ€§èƒ½è¿œè¶…ChromaDB

æ¶æ„ï¼š
- Milvus: åˆ†å¸ƒå¼å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒå¤§è§„æ¨¡å‘é‡æ£€ç´¢
- GPUåŠ é€Ÿçš„åµŒå…¥æ¨¡å‹ï¼ˆsentence-transformers on GPUï¼‰
- åŒå±‚ç´¢å¼•ï¼šæ–‡æ¡£çº§ï¼ˆç²—æ’ï¼‰+ Chunkçº§ï¼ˆç²¾æ’ï¼‰
- æ—¶é—´æ„ŸçŸ¥ï¼šæ”¯æŒæ—¶é—´è¿‡æ»¤å’Œæ—¶é—´åŠ æƒ
"""
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
import torch
import os
from sentence_transformers import SentenceTransformer
from pymilvus import (
    connections,
    Collection,
    FieldSchema,
    CollectionSchema,
    DataType,
    utility
)

from models import PolicySegment
from config import OUTPUT_DIR
from utils.chunking import PolicyDocumentChunker, DocumentChunk


class MilvusVectorDatabase:
    """
    Milvuså‘é‡æ•°æ®åº“ - GPUåŠ é€Ÿç‰ˆ
    
    ç‰¹æ€§ï¼š
    1. ä½¿ç”¨Milvusè¿›è¡Œåˆ†å¸ƒå¼å‘é‡å­˜å‚¨å’Œæ£€ç´¢
    2. ä½¿ç”¨cuvsè¿›è¡ŒGPUåŠ é€Ÿçš„å‘é‡æœç´¢
    3. æ”¯æŒæ‰¹é‡æ’å…¥å’Œæ‰¹é‡æ£€ç´¢
    4. æ”¯æŒå¤šç§è·ç¦»åº¦é‡ï¼ˆL2, IP, Cosineç­‰ï¼‰
    """
    
    def __init__(self, collection_name: str = "policy_documents", 
                 embedding_model: str = "./models/xiaobu-embedding-v2",
                 dim: int = 1792,
                 enable_chunking: bool = True,
                 chunk_only: bool = True):  # ä¿®æ”¹é»˜è®¤å€¼ä¸ºTrueï¼Œåªä½¿ç”¨chunkçº§åˆ«
        """
        åˆå§‹åŒ–Milvuså‘é‡æ•°æ®åº“
        
        Args:
            collection_name: é›†åˆåç§°
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            dim: å‘é‡ç»´åº¦
            enable_chunking: æ˜¯å¦å¯ç”¨chunkçº§ç´¢å¼•
            chunk_only: æ˜¯å¦åªä½¿ç”¨chunkçº§åˆ«ï¼ˆç®€åŒ–ç‰ˆRAGï¼‰
        """
        self.collection_name = collection_name
        self.chunk_collection_name = f"{collection_name}_chunks"
        self.embedding_dim = dim
        self.enable_chunking = enable_chunking
        self.chunk_only = chunk_only
        
        print(f"[MilvusVectorDB] æ­£åœ¨åˆå§‹åŒ–...")
        print(f"[MilvusVectorDB] ç®€åŒ–ç‰ˆRAG: {'åªä½¿ç”¨chunkçº§åˆ«' if chunk_only else 'åŒå±‚ç´¢å¼•' if enable_chunking else 'ç¦ç”¨'}")
        
        # 1. è¿æ¥åˆ°Milvusï¼ˆè‡ªåŠ¨æ£€æµ‹è¿è¡Œç¯å¢ƒå’ŒIPï¼‰
        connection_configs = []
        
        # æ£€æµ‹è¿è¡Œç¯å¢ƒï¼šæ˜¯å¦åœ¨WSLä¸­è¿è¡Œ
        import platform
        is_wsl = False
        is_windows = platform.system() == 'Windows'
        
        try:
            # æ£€æµ‹æ˜¯å¦åœ¨WSLä¸­è¿è¡Œï¼ˆæ£€æŸ¥/proc/versionæˆ–WSLç›¸å…³ç¯å¢ƒå˜é‡ï¼‰
            if os.path.exists('/proc/version'):
                with open('/proc/version', 'r') as f:
                    version_info = f.read().lower()
                    if 'microsoft' in version_info or 'wsl' in version_info:
                        is_wsl = True
        except:
            pass
        
        # å¦‚æœæ˜ç¡®è®¾ç½®äº†WSL_DISTRO_NAMEï¼Œè¯´æ˜åœ¨WSLä¸­
        if os.environ.get('WSL_DISTRO_NAME'):
            is_wsl = True
        
        print(f"[MilvusVectorDB] ğŸ” è¿è¡Œç¯å¢ƒæ£€æµ‹: Windows={is_windows}, WSL={is_wsl}")
        
        # ä¼˜å…ˆæ£€æŸ¥ç¯å¢ƒå˜é‡ï¼ˆå…è®¸æ‰‹åŠ¨æŒ‡å®šä¸»æœºï¼‰
        milvus_host = os.environ.get('MILVUS_HOST')
        if not milvus_host and is_windows:
            # å¦‚æœå½“å‰è¿›ç¨‹ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ³¨å†Œè¡¨è¯»å–ç”¨æˆ·çº§ç¯å¢ƒå˜é‡ï¼ˆWindowsï¼‰
            try:
                import winreg
                key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, r'Environment')
                try:
                    milvus_host, _ = winreg.QueryValueEx(key, 'MILVUS_HOST')
                    winreg.CloseKey(key)
                except FileNotFoundError:
                    winreg.CloseKey(key)
                    milvus_host = None
            except:
                milvus_host = None
        
        if milvus_host:
            connection_configs.append({'host': milvus_host, 'port': '19530'})
            print(f"[MilvusVectorDB] âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„Milvusä¸»æœº: {milvus_host}")
        
        # æ ¹æ®è¿è¡Œç¯å¢ƒé€‰æ‹©è¿æ¥æ–¹å¼
        try:
            import subprocess
            import re
            
            if is_wsl:
                # åœ¨WSLä¸­è¿è¡Œï¼šç›´æ¥ä½¿ç”¨localhostï¼ˆæœ€ç®€å•çš„æ–¹å¼ï¼‰
                print(f"[MilvusVectorDB] ğŸ” æ£€æµ‹åˆ°WSLç¯å¢ƒï¼Œä½¿ç”¨localhostè¿æ¥")
                connection_configs = [
                    {'host': 'localhost', 'port': '19530'},
                    {'host': '127.0.0.1', 'port': '19530'},
                ]
            elif is_windows:
                # åœ¨Windowsä¸­è¿è¡Œï¼šéœ€è¦å°è¯•WSLç½‘å…³IPæˆ–é…ç½®ç«¯å£è½¬å‘
                wsl_gateway_ip = None
                # æ–¹æ³•1: è·å–WSL2çš„é»˜è®¤ç½‘å…³IPï¼ˆWindowsä¸»æœºåœ¨WSLç½‘ç»œä¸­çš„IPï¼‰- è¿™æ˜¯ä»Windowsè®¿é—®WSLæœåŠ¡çš„æ­£ç¡®IP
                try:
                    # å°è¯•å¤šç§æ–¹å¼æå–IP
                    result = subprocess.run(
                        ['wsl', 'bash', '-c', "ip route show default | head -1"],
                        capture_output=True,
                        text=True,
                        timeout=5
                    )
                    if result.returncode == 0:
                        route_output = result.stdout.strip()
                        print(f"[MilvusVectorDB] ğŸ” WSLè·¯ç”±å‘½ä»¤è¾“å‡º: '{route_output}'")
                        
                        # ä»è·¯ç”±è¾“å‡ºä¸­æå–IPï¼ˆæ ¼å¼ï¼šdefault via 172.28.48.1 dev eth0...ï¼‰
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–IP
                        ip_match = re.search(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})', route_output)
                        if ip_match:
                            wsl_gateway_ip = ip_match.group(1)
                            if wsl_gateway_ip not in [c['host'] for c in connection_configs]:
                                connection_configs.append({'host': wsl_gateway_ip, 'port': '19530'})
                                print(f"[MilvusVectorDB] âœ… æ£€æµ‹åˆ°WSLç½‘å…³IPï¼ˆWindowsåœ¨WSLä¸­çš„IPï¼‰: {wsl_gateway_ip} â­ è¿™æ˜¯ä»Windowsè®¿é—®WSLçš„æ­£ç¡®IP")
                        else:
                            print(f"[MilvusVectorDB] âš ï¸ æ— æ³•ä»è·¯ç”±è¾“å‡ºä¸­æå–IP: '{route_output}'")
                    else:
                        print(f"[MilvusVectorDB] âš ï¸ WSLè·¯ç”±å‘½ä»¤å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                        if result.stderr:
                            print(f"[MilvusVectorDB] âš ï¸ é”™è¯¯è¾“å‡º: {result.stderr[:200]}")
                except Exception as e:
                    print(f"[MilvusVectorDB] âš ï¸ æ— æ³•è·å–WSLç½‘å…³IPï¼Œå¼‚å¸¸: {type(e).__name__}: {e}")
                
                # å¦‚æœç½‘å…³IPæ£€æµ‹å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                if not wsl_gateway_ip:
                    # æ–¹æ³•2: ä»WSL hostname -Iè·å–ç¬¬ä¸€ä¸ªIPï¼ˆå¯èƒ½æ˜¯ç½‘å…³IPï¼‰
                    try:
                        result = subprocess.run(
                            ['wsl', 'hostname', '-I'],
                            capture_output=True,
                            text=True,
                            timeout=3
                        )
                        if result.returncode == 0:
                            wsl_ips = result.stdout.strip().split()
                            # é€šå¸¸ç¬¬ä¸€ä¸ªIPæ˜¯ä¸»IPï¼Œå¯èƒ½æ˜¯172.x.x.xæ ¼å¼ï¼ˆWSL2å¸¸ç”¨ï¼‰
                            for wsl_ip in wsl_ips[:2]:  # åªå–å‰2ä¸ªIPå°è¯•
                                if wsl_ip and re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', wsl_ip):
                                    # WSL2é€šå¸¸ä½¿ç”¨172.x.x.xç½‘æ®µ
                                    if wsl_ip.startswith('172.') or wsl_ip.startswith('192.168.'):
                                        if wsl_ip not in [c['host'] for c in connection_configs]:
                                            connection_configs.append({'host': wsl_ip, 'port': '19530'})
                                            print(f"[MilvusVectorDB] ğŸ” æ£€æµ‹åˆ°WSL IP: {wsl_ip}")
                                        break
                    except Exception as e:
                        print(f"[MilvusVectorDB] âš ï¸ æ— æ³•è·å–WSL IP: {e}")
                
                # æœ€åå°è¯•localhostï¼ˆéœ€è¦WSLç«¯å£è½¬å‘é…ç½®ï¼‰
                connection_configs.append({'host': 'localhost', 'port': '19530'})
                connection_configs.append({'host': '127.0.0.1', 'port': '19530'})
            else:
                # çº¯Linux/Macç¯å¢ƒï¼ˆéWSLï¼‰ï¼Œç›´æ¥ä½¿ç”¨localhost
                print(f"[MilvusVectorDB] ğŸ” æ£€æµ‹åˆ°Linux/Macç¯å¢ƒï¼Œä½¿ç”¨localhostè¿æ¥")
                if not connection_configs:
                    connection_configs = [
                        {'host': 'localhost', 'port': '19530'},
                        {'host': '127.0.0.1', 'port': '19530'},
                    ]
        except Exception as e:
            print(f"[MilvusVectorDB] âš ï¸ ç¯å¢ƒæ£€æµ‹å‡ºé”™: {e}")
            # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            if not connection_configs:
                connection_configs = [
                    {'host': 'localhost', 'port': '19530'},
                    {'host': '127.0.0.1', 'port': '19530'},
                ]
        
        # å°è¯•è¿æ¥
        connected = False
        last_error = None
        print(f"[MilvusVectorDB] ğŸ” å°è¯•è¿æ¥Milvusï¼Œå…± {len(connection_configs)} ä¸ªé…ç½®...")
        for i, config in enumerate(connection_configs):
            try:
                print(f"[MilvusVectorDB]   å°è¯• {i+1}/{len(connection_configs)}: {config['host']}:{config['port']}")
                connections.connect(
                    alias='default',
                    host=config['host'],
                    port=config['port'],
                    timeout=5  # 5ç§’è¶…æ—¶
                )
                print(f"[MilvusVectorDB] âœ… å·²è¿æ¥åˆ°Milvus ({config['host']}:{config['port']})")
                connected = True
                break
            except Exception as e:
                print(f"[MilvusVectorDB]   âŒ è¿æ¥å¤±è´¥: {str(e)[:100]}")
                last_error = e
                continue
        
        if not connected:
            print(f"\n[MilvusVectorDB] âŒ æ‰€æœ‰è¿æ¥å°è¯•å‡å¤±è´¥ï¼")
            print(f"[MilvusVectorDB] æœ€åé”™è¯¯: {last_error}")
            
            if is_wsl:
                print(f"\n[MilvusVectorDB] ğŸ’¡ æ’æŸ¥æ­¥éª¤ï¼ˆWSLç¯å¢ƒä¸­ï¼‰ï¼š")
                print(f"   1. æ£€æŸ¥Milvuså®¹å™¨æ˜¯å¦è¿è¡Œ:")
                print(f"      docker ps | grep milvus")
                print(f"      docker logs milvus-standalone")
                print(f"   2. æ£€æŸ¥ç«¯å£æ˜¯å¦ç›‘å¬:")
                print(f"      ss -tuln | grep 19530")
                print(f"   3. æµ‹è¯•è¿æ¥:")
                print(f"      python3 -c \"from pymilvus import connections; connections.connect('default', host='localhost', port='19530')\"")
            elif is_windows:
                print(f"\n[MilvusVectorDB] ğŸ’¡ æ’æŸ¥æ­¥éª¤ï¼ˆWindowsè®¿é—®WSLä¸­çš„Milvusï¼‰ï¼š")
                print(f"\n   âš ï¸ é‡è¦ï¼šWSL2ç½‘ç»œéš”ç¦»ï¼ŒWindowsæ— æ³•ç›´æ¥è®¿é—®WSLæœåŠ¡")
                print(f"\n   æ–¹æ¡ˆ1ï¼šé…ç½®ç«¯å£è½¬å‘ï¼ˆæ¨èï¼Œä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡ŒPowerShellï¼‰:")
                print("      $wslIP = (wsl bash -c \"ip route show default | awk '{print \\$3}'\").Trim()")
                print("      netsh interface portproxy add v4tov4 listenport=19530 listenaddress=0.0.0.0 connectport=19530 connectaddress=$wslIP")
                print("      ç„¶åè®¾ç½®ç¯å¢ƒå˜é‡: $env:MILVUS_HOST = 'localhost'")
                print(f"\n   æ–¹æ¡ˆ2ï¼šåœ¨WSLä¸­è¿è¡Œï¼ˆæœ€ç®€å•ï¼‰:")
                print("      wsl")
                print("      cd /mnt/c/Users/qq100/Desktop/å›½é‡‘è¯åˆ¸/Agent")
                print("      conda activate quant")
                print("      python run_full_pipeline.py")
                print(f"\n   æ–¹æ¡ˆ3ï¼šæ£€æŸ¥Milvusæ˜¯å¦è¿è¡Œ:")
                print(f"      wsl docker ps | grep milvus")
            else:
                print(f"\n[MilvusVectorDB] ğŸ’¡ æ’æŸ¥æ­¥éª¤:")
                print(f"   1. æ£€æŸ¥MilvusæœåŠ¡æ˜¯å¦è¿è¡Œ")
                print(f"   2. æ£€æŸ¥ç«¯å£19530æ˜¯å¦ç›‘å¬")
            
            raise last_error
        
        # 2. åŠ è½½GPUåµŒå…¥æ¨¡å‹
        print(f"[MilvusVectorDB] åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model}")
        self.model = SentenceTransformer(embedding_model)
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print(f"[MilvusVectorDB] âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU: {torch.cuda.get_device_name(0)}")
        else:
            print(f"[MilvusVectorDB] âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        
        # 3. åˆå§‹åŒ–chunkerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_chunking:
            # â­ chunké…ç½®ï¼šä¸Milvusç°æœ‰æ•°æ®ä¸€è‡´ï¼ˆçº¦450å­—ï¼‰
            # æ–°æ”¿ç­–æ£€ç´¢æ—¶LLMä¹Ÿä¼šç”Ÿæˆ400-450å­—çš„ç‰‡æ®µï¼Œç¡®ä¿ç²’åº¦åŒ¹é…
            self.chunker = PolicyDocumentChunker(
                chunk_size_target=400,
                chunk_size_max=450,
                overlap=50,
                absolute_max=450
            )
        
        # 4. åˆ›å»ºæˆ–è·å–é›†åˆï¼ˆåªä½¿ç”¨chunkçº§åˆ«ï¼‰
        self._init_chunk_collection()
        
        chunk_count = self.chunk_collection.num_entities
        print(f"[MilvusVectorDB] âœ… åˆå§‹åŒ–å®Œæˆï¼ˆç®€åŒ–ç‰ˆï¼šåªä½¿ç”¨chunkçº§åˆ«ï¼‰")
        print(f"[MilvusVectorDB]   - Chunkçº§: {chunk_count} ä¸ªchunks")
    
    # åˆ é™¤æ–‡æ¡£çº§é›†åˆåˆå§‹åŒ–æ–¹æ³•ï¼Œåªä½¿ç”¨chunkçº§åˆ«
    
    def _init_chunk_collection(self):
        """åˆå§‹åŒ–æˆ–è·å–Chunkçº§Milvusé›†åˆ"""
        # å®šä¹‰Chunk Schema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=150),
            FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=self.embedding_dim),
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=5000),  # è¶…ä¿å®ˆè®¾ç½®ï¼Œè§„é¿pymilvus bug
            FieldSchema(name="chunk_index", dtype=DataType.INT64),
            FieldSchema(name="chunk_type", dtype=DataType.VARCHAR, max_length=20),
            FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=500),  # æ–‡æ¡£æ ‡é¢˜
            FieldSchema(name="timestamp", dtype=DataType.VARCHAR, max_length=150),  # å‘å¸ƒæ—¶é—´
            FieldSchema(name="industries", dtype=DataType.VARCHAR, max_length=500),  # â­ ä¸­ä¿¡ä¸€çº§è¡Œä¸šï¼ˆé€—å·åˆ†éš”ï¼Œç»è¿‡DS32Bè¿‡æ»¤ï¼‰
            FieldSchema(name="investment_relevance", dtype=DataType.VARCHAR, max_length=10),  # â­ æŠ•èµ„ç›¸å…³æ€§ï¼šé«˜/ä½
            FieldSchema(name="report_series", dtype=DataType.VARCHAR, max_length=50),  # â­ æŠ¥å‘Šç³»åˆ—ï¼šæ™¨ä¼šçºªè¦/æ™šé—´é€Ÿé€’/ç­–ç•¥ç ”ç©¶ç­‰
            FieldSchema(name="industry_policy_segments", dtype=DataType.VARCHAR, max_length=20000),  # â­ è¡Œä¸šåŠå¯¹åº”æ”¿ç­–ç‰‡æ®µï¼ˆJSONæ ¼å¼ï¼Œå¢åŠ åˆ°20000ä»¥æ”¯æŒå¤§å‹JSONï¼‰
        ]
        
        schema = CollectionSchema(fields, description="æ”¿ç­–æ–‡æ¡£Chunkå‘é‡åº“")
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        if utility.has_collection(self.chunk_collection_name):
            print(f"[MilvusVectorDB] Chunké›†åˆå·²å­˜åœ¨ï¼Œæ£€æŸ¥schema...")
            existing_collection = Collection(self.chunk_collection_name)
            existing_fields = [field.name for field in existing_collection.schema.fields]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«titleå­—æ®µ
            if "title" not in existing_fields:
                print(f"[MilvusVectorDB] âš ï¸ ç°æœ‰collectionç¼ºå°‘titleå­—æ®µï¼Œé‡æ–°åˆ›å»º...")
                utility.drop_collection(self.chunk_collection_name)
                print(f"[MilvusVectorDB] åˆ›å»ºæ–°çš„Chunké›†åˆ...")
                self.chunk_collection = Collection(self.chunk_collection_name, schema)
                
                # åˆ›å»ºç´¢å¼•
                index_params = {
                    "metric_type": "L2",
                    "index_type": "HNSW",
                    "params": {"M": 16, "efConstruction": 200}
                }
                self.chunk_collection.create_index("embedding", index_params)
                print(f"[MilvusVectorDB] âœ… Chunkç´¢å¼•åˆ›å»ºå®Œæˆ (HNSW)")
            else:
                print(f"[MilvusVectorDB] âœ… Chunké›†åˆschemaæ­£ç¡®ï¼ŒåŠ è½½ä¸­...")
                self.chunk_collection = existing_collection
        else:
            print(f"[MilvusVectorDB] åˆ›å»ºæ–°Chunké›†åˆ...")
            self.chunk_collection = Collection(self.chunk_collection_name, schema)
            
            # åˆ›å»ºç´¢å¼•
            index_params = {
                "metric_type": "L2",
                "index_type": "HNSW",
                "params": {"M": 16, "efConstruction": 200}
            }
            self.chunk_collection.create_index("embedding", index_params)
            print(f"[MilvusVectorDB] âœ… Chunkç´¢å¼•åˆ›å»ºå®Œæˆ (HNSW)")
        
        # åŠ è½½é›†åˆåˆ°å†…å­˜ï¼ˆä¼˜åŒ–ï¼šæ£€æŸ¥çŠ¶æ€åå†åŠ è½½ï¼‰
        print(f"[MilvusVectorDB] ğŸ”„ æ­£åœ¨åŠ è½½Chunké›†åˆåˆ°å†…å­˜...")
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦ä¸ºç©ºï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
            import threading
            
            entity_count = [None]
            count_error = [None]
            
            def get_count():
                try:
                    entity_count[0] = self.chunk_collection.num_entities
                except Exception as e:
                    count_error[0] = e
            
            count_thread = threading.Thread(target=get_count, daemon=True)
            count_thread.start()
            count_thread.join(timeout=10)  # 10ç§’è¶…æ—¶
            
            if count_thread.is_alive():
                print(f"[MilvusVectorDB] âš ï¸ æ£€æŸ¥é›†åˆå¤§å°è¶…æ—¶ï¼Œå‡è®¾ä¸ºç©ºé›†åˆï¼ˆæ–°é›†åˆï¼‰")
                print(f"[MilvusVectorDB] âœ… è·³è¿‡loadæ“ä½œï¼ˆæ–°é›†åˆä¸éœ€è¦loadï¼Œæ’å…¥æ•°æ®æ—¶ä¼šè‡ªåŠ¨åŠ è½½ï¼‰")
                return  # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œåç»­load
            
            if count_error[0]:
                print(f"[MilvusVectorDB] âš ï¸ æ£€æŸ¥é›†åˆå¤§å°å¤±è´¥: {count_error[0]}ï¼Œå‡è®¾ä¸ºç©ºé›†åˆ")
                print(f"[MilvusVectorDB] âœ… è·³è¿‡loadæ“ä½œï¼ˆæ–°é›†åˆä¸éœ€è¦loadï¼‰")
                return
            
            print(f"[MilvusVectorDB] ğŸ” å½“å‰é›†åˆå®ä½“æ•°: {entity_count[0]}")
            
            if entity_count[0] == 0:
                # ç©ºé›†åˆï¼Œè·³è¿‡loadæ“ä½œï¼ˆç©ºé›†åˆä¸éœ€è¦loadï¼‰
                print(f"[MilvusVectorDB] âœ… ç©ºé›†åˆï¼Œè·³è¿‡loadæ“ä½œï¼ˆæ’å…¥æ•°æ®æ—¶ä¼šè‡ªåŠ¨åŠ è½½ï¼‰")
            else:
                # éç©ºé›†åˆï¼Œå¿…é¡»åŠ è½½
                # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½
                try:
                    # å°è¯•æŸ¥è¯¢ï¼Œå¦‚æœèƒ½æŸ¥è¯¢è¯´æ˜å·²åŠ è½½
                    self.chunk_collection.query(expr="id >= 0", limit=1, output_fields=["id"])
                    print(f"[MilvusVectorDB] âœ… é›†åˆå·²åœ¨å†…å­˜ä¸­")
                except:
                    # æœªåŠ è½½ï¼Œæ‰§è¡ŒåŠ è½½ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                    print(f"[MilvusVectorDB] ğŸ”„ æ­£åœ¨åŠ è½½ {entity_count[0]} ä¸ªå®ä½“åˆ°å†…å­˜...")
                    print(f"[MilvusVectorDB] âš ï¸ å¦‚æœé•¿æ—¶é—´å¡åœ¨æ­¤å¤„ï¼Œå¯èƒ½æ˜¯MinIOæœªè¿è¡Œ")
                    
                    # ä½¿ç”¨çº¿ç¨‹+è¶…æ—¶æœºåˆ¶ï¼Œé¿å…æ— é™ç­‰å¾…
                    import threading
                    import time
                    
                    load_success = [False]
                    load_error = [None]
                    
                    def load_in_thread():
                        try:
                            self.chunk_collection.load()
                            load_success[0] = True
                        except Exception as e:
                            load_error[0] = e
                    
                    load_thread = threading.Thread(target=load_in_thread, daemon=True)
                    load_thread.start()
                    load_thread.join(timeout=30)  # 30ç§’è¶…æ—¶
                    
                    if load_thread.is_alive():
                        print(f"[MilvusVectorDB] âŒ åŠ è½½è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
                        print(f"[MilvusVectorDB] ğŸ’¡ å¯èƒ½åŸå› ï¼šMinIOæœåŠ¡æœªæ­£å¸¸è¿è¡Œ")
                        print(f"[MilvusVectorDB] ğŸ’¡ æ£€æŸ¥å‘½ä»¤: docker ps | grep minio")
                        print(f"[MilvusVectorDB] ğŸ’¡ ä¿®å¤MinIO:")
                        print(f"   cd ~/milvus && docker compose down")
                        print(f"   sudo rm -rf volumes/minio/.minio.sys")
                        print(f"   docker compose up -d")
                        print(f"[MilvusVectorDB] âš ï¸ è·³è¿‡loadæ“ä½œï¼Œç»§ç»­åˆå§‹åŒ–ï¼ˆå¦‚æœæ˜¯æ–°é›†åˆå¯èƒ½ä¸éœ€è¦loadï¼‰")
                        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç»§ç»­ï¼ˆå¯¹äºæ–°é›†åˆå¯ä»¥è·³è¿‡loadï¼‰
                    elif load_error[0]:
                        print(f"[MilvusVectorDB] âŒ åŠ è½½å¤±è´¥: {load_error[0]}")
                        print(f"[MilvusVectorDB] ğŸ’¡ å¯èƒ½åŸå› ï¼šMinIOæœåŠ¡æœªæ­£å¸¸è¿è¡Œ")
                        print(f"[MilvusVectorDB] ğŸ’¡ æ£€æŸ¥å‘½ä»¤: docker ps | grep minio")
                        print(f"[MilvusVectorDB] ğŸ’¡ ä¿®å¤æ­¥éª¤:")
                        print(f"   1. cd ~/milvus")
                        print(f"   2. docker compose down")
                        print(f"   3. sudo rm -rf volumes/minio/.minio.sys")
                        print(f"   4. docker compose up -d")
                        print(f"   5. ç­‰å¾…30ç§’åé‡è¯•")
                        print(f"[MilvusVectorDB] âš ï¸ è·³è¿‡loadæ“ä½œï¼Œç»§ç»­åˆå§‹åŒ–ï¼ˆæ–°é›†åˆå¯èƒ½ä¸éœ€è¦loadï¼‰")
                        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç»§ç»­
                    elif load_success[0]:
                        print(f"[MilvusVectorDB] âœ… Chunké›†åˆå·²åŠ è½½åˆ°å†…å­˜")
                
        except Exception as e:
            print(f"[MilvusVectorDB] âŒ åŠ è½½é›†åˆæ—¶å‡ºé”™: {e}")
            raise
    
    def get_max_doc_id_number(self) -> int:
        """
        è·å–æœ€å¤§çš„doc_idç¼–å·ï¼ˆç”¨äºç»§ç»­ç¼–å·ï¼‰
        
        ä»Milvuså’Œç¼“å­˜æ–‡ä»¶ä¸­æŸ¥æ‰¾æœ€å¤§ç¼–å·ï¼Œå–ä¸¤è€…ä¸­çš„æœ€å¤§å€¼
        
        Returns:
            æœ€å¤§çš„doc_idç¼–å·ï¼ˆä¾‹å¦‚ï¼šå¦‚æœæœ€å¤§æ˜¯doc_1144ï¼Œè¿”å›1144ï¼‰ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›0
        """
        import re
        import json
        from pathlib import Path
        
        max_milvus = 0
        max_cache = 0
        
        # æ–¹æ³•1ï¼šä»MilvusæŸ¥è¯¢
        try:
            if hasattr(self, 'chunk_collection') and self.chunk_collection is not None:
                existing_doc_ids = self._get_existing_doc_ids()
                for doc_id in existing_doc_ids:
                    match = re.match(r'^doc_(\d+)$', str(doc_id))
                    if match:
                        number = int(match.group(1))
                        if number > max_milvus:
                            max_milvus = number
        except Exception as e:
            print(f"[MilvusVectorDB] âš ï¸ ä»MilvusæŸ¥è¯¢æœ€å¤§ç¼–å·å¤±è´¥: {e}")
        
        # æ–¹æ³•2ï¼šä»ç¼“å­˜æ–‡ä»¶æŸ¥è¯¢
        try:
            cache_file = Path("cache/industry_agent_cache.json")
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                for doc_id in cache_data.keys():
                    match = re.match(r'^doc_(\d+)$', str(doc_id))
                    if match:
                        number = int(match.group(1))
                        if number > max_cache:
                            max_cache = number
        except Exception as e:
            print(f"[MilvusVectorDB] âš ï¸ ä»ç¼“å­˜æ–‡ä»¶æŸ¥è¯¢æœ€å¤§ç¼–å·å¤±è´¥: {e}")
        
        # å–ä¸¤è€…ä¸­çš„æœ€å¤§å€¼
        max_number = max(max_milvus, max_cache)
        
        if max_number > 0:
            print(f"[MilvusVectorDB] âœ… æœ€å¤§doc_idç¼–å·: {max_number} (Milvus: {max_milvus}, ç¼“å­˜: {max_cache})")
        else:
            print(f"[MilvusVectorDB] âš ï¸ æœªæ‰¾åˆ°doc_æ ¼å¼çš„doc_idï¼Œè¿”å›0")
        
        return max_number
    
    def _get_existing_doc_ids(self) -> set:
        """
        è·å–Milvusä¸­å·²å­˜åœ¨çš„æ‰€æœ‰doc_idï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Returns:
            å·²å­˜åœ¨çš„doc_idé›†åˆ
        """
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•°æ®
            if not utility.has_collection(self.chunk_collection_name):
                print(f"[MilvusVectorDB] âš ï¸ é›†åˆ {self.chunk_collection_name} ä¸å­˜åœ¨")
                return set()
            
            # å°è¯•åŠ è½½é›†åˆï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
            try:
                self.chunk_collection.load()
                print(f"[MilvusVectorDB] âœ… é›†åˆå·²åŠ è½½")
            except Exception as load_error:
                print(f"[MilvusVectorDB] âš ï¸ é›†åˆåŠ è½½å¤±è´¥: {load_error}ï¼Œå°è¯•ç»§ç»­æŸ¥è¯¢")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•ç»§ç»­æŸ¥è¯¢ï¼ˆæ–°æ’å…¥çš„æ•°æ®å¯èƒ½åœ¨å†…å­˜ä¸­ï¼‰
            
            entity_count = self.chunk_collection.num_entities
            print(f"[MilvusVectorDB] ğŸ“Š é›†åˆ {self.chunk_collection_name} å…±æœ‰ {entity_count} ä¸ªentities")
            if entity_count == 0:
                print(f"[MilvusVectorDB] âš ï¸ é›†åˆä¸ºç©ºï¼Œæ— å·²å­˜åœ¨çš„doc_id")
                return set()
            
            # æŸ¥è¯¢æ‰€æœ‰å”¯ä¸€çš„doc_id
            existing_doc_ids = set()
            
            try:
                total = self.chunk_collection.num_entities
                print(f"[MilvusVectorDB] ğŸ” æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£ï¼ˆå…± {total} ä¸ªchunksï¼‰...")
                
                # ä½¿ç”¨è¿­ä»£å™¨æŸ¥è¯¢æ‰€æœ‰doc_idï¼ˆåˆ†æ‰¹æŸ¥è¯¢é¿å…å†…å­˜æº¢å‡ºï¼‰
                batch_size = 10000
                for offset in range(0, total, batch_size):
                    limit = min(batch_size, total - offset)
                    try:
                        results = self.chunk_collection.query(
                            expr=f"id >= {offset} && id < {offset + limit}",
                            output_fields=["doc_id"],
                            limit=limit
                        )
                        for result in results:
                            doc_id = result.get('doc_id')
                            if doc_id:
                                existing_doc_ids.add(doc_id)
                    except Exception as batch_error:
                        # å¦‚æœæ‰¹é‡æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ç®€å•æŸ¥è¯¢
                        print(f"[MilvusVectorDB] âš ï¸ æ‰¹é‡æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ç®€å•æŸ¥è¯¢: {batch_error}")
                        break
                    
                    if (offset + batch_size) % 50000 == 0:
                        print(f"[MilvusVectorDB]   å·²æ£€æŸ¥ {min(offset + batch_size, total)}/{total} ä¸ªchunks...")
                
                # å¦‚æœæ‰¹é‡æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ç®€å•æŸ¥è¯¢æ‰€æœ‰æ•°æ®
                if len(existing_doc_ids) == 0 and total > 0:
                    try:
                        results = self.chunk_collection.query(
                            expr="id >= 0",
                            output_fields=["doc_id"],
                            limit=min(100000, total)  # æœ€å¤šæŸ¥è¯¢10ä¸‡æ¡
                        )
                        for result in results:
                            doc_id = result.get('doc_id')
                            if doc_id:
                                existing_doc_ids.add(doc_id)
                    except Exception as e2:
                        print(f"[MilvusVectorDB] âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼Œå‡è®¾æ— å·²å­˜åœ¨æ–‡æ¡£: {e2}")
                        return set()
                
                print(f"[MilvusVectorDB] âœ… æ£€æŸ¥å®Œæˆï¼Œå‘ç° {len(existing_doc_ids)} ä¸ªå”¯ä¸€çš„doc_id")
            except Exception as e:
                print(f"[MilvusVectorDB] âš ï¸ æ£€æŸ¥å·²å­˜åœ¨æ–‡æ¡£å¤±è´¥: {e}ï¼Œå‡è®¾æ— å·²å­˜åœ¨æ–‡æ¡£")
                return set()
            
            return existing_doc_ids
        except Exception as e:
            print(f"[MilvusVectorDB] âš ï¸ æ£€æŸ¥å·²å­˜åœ¨æ–‡æ¡£å¤±è´¥: {e}ï¼Œå‡è®¾æ— å·²å­˜åœ¨æ–‡æ¡£")
            return set()
    
    def get_existing_title_timestamp_pairs(self) -> set:
        """
        è·å–Milvusä¸­å·²å­˜åœ¨çš„æ‰€æœ‰ (æ ‡é¢˜, å‘å¸ƒæ—¶é—´) ç»„åˆï¼ˆç”¨äºå…¥åº“å‰å»é‡ï¼‰
        
        å»é‡é€»è¾‘ï¼šæ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´åŒæ—¶ä¸€æ ·æ‰ç®—é‡å¤
        
        Returns:
            å·²å­˜åœ¨çš„ (title, timestamp) å…ƒç»„é›†åˆ
        """
        try:
            if not utility.has_collection(self.chunk_collection_name):
                return set()
            
            # å°è¯•åŠ è½½é›†åˆ
            try:
                self.chunk_collection.load()
            except:
                pass
            
            entity_count = self.chunk_collection.num_entities
            if entity_count == 0:
                return set()
            
            existing_pairs = set()
            
            # åˆ†æ‰¹æŸ¥è¯¢æ‰€æœ‰ (title, timestamp) ç»„åˆ
            # â­ Milvusé™åˆ¶ï¼šoffset + limit ä¸èƒ½è¶…è¿‡ 16384
            print(f"[MilvusVectorDB] ğŸ” è·å–å·²å­˜åœ¨çš„ (æ ‡é¢˜, æ—¶é—´) ç»„åˆï¼ˆå…± {entity_count} æ¡è®°å½•ï¼‰...")
            
            batch_size = 10000  # æ¯æ‰¹æŸ¥è¯¢10000æ¡
            offset = 0
            
            while offset < entity_count:
                try:
                    # è®¡ç®—æœ¬æ‰¹æ¬¡æŸ¥è¯¢æ•°é‡ï¼ˆä¸è¶…è¿‡16384é™åˆ¶ï¼‰
                    current_limit = min(batch_size, entity_count - offset, 16384 - offset % 16384)
                    if current_limit <= 0:
                        break
                    
                    results = self.chunk_collection.query(
                        expr="id >= 0",
                        output_fields=["title", "timestamp"],
                        offset=offset,
                        limit=current_limit
                    )
                    
                    if not results:
                        break
                    
                    for result in results:
                        title = result.get('title', '')
                        timestamp = result.get('timestamp', '')
                        if title:
                            # ä½¿ç”¨ (title, timestamp) å…ƒç»„ä½œä¸ºå”¯ä¸€æ ‡è¯†
                            existing_pairs.add((title, timestamp))
                    
                    offset += len(results)
                    
                    # å¦‚æœè¿”å›æ•°é‡å°‘äºè¯·æ±‚æ•°é‡ï¼Œè¯´æ˜å·²åˆ°æœ«å°¾
                    if len(results) < current_limit:
                        break
                        
                except Exception as e:
                    print(f"[MilvusVectorDB] âš ï¸ åˆ†æ‰¹æŸ¥è¯¢å¤±è´¥ (offset={offset}): {e}")
                    break
            
            print(f"[MilvusVectorDB] âœ… å·²å­˜åœ¨ {len(existing_pairs)} ä¸ªå”¯ä¸€çš„ (æ ‡é¢˜, æ—¶é—´) ç»„åˆ")
            return existing_pairs
            
        except Exception as e:
            print(f"[MilvusVectorDB] âš ï¸ è·å–å·²å­˜åœ¨ç»„åˆå¤±è´¥: {e}")
            return set()
    
    def add_documents(self, segments: List[PolicySegment], batch_size: int = 100, skip_existing: bool = True):
        """
        æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°Milvuså‘é‡åº“ï¼ˆåªä½¿ç”¨chunkçº§åˆ«ï¼‰
        
        Args:
            segments: PolicySegmentåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£ï¼ˆæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼Œé»˜è®¤Trueï¼‰
        """
        print(f"[MilvusVectorDB] å¼€å§‹æ·»åŠ  {len(segments)} ä¸ªæ–‡æ¡£...")
        print(f"[MilvusVectorDB] ç®€åŒ–ç‰ˆæ¨¡å¼ï¼šåªæ·»åŠ chunkçº§å‘é‡")
        
        # â­ æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£
        if skip_existing:
            existing_doc_ids = self._get_existing_doc_ids()
            if existing_doc_ids:
                original_count = len(segments)
                segments = [seg for seg in segments if seg.doc_id not in existing_doc_ids]
                skipped_count = original_count - len(segments)
                if skipped_count > 0:
                    print(f"[MilvusVectorDB] âœ… è·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„æ–‡æ¡£ï¼Œå‰©ä½™ {len(segments)} ä¸ªå¾…å¤„ç†")
                else:
                    print(f"[MilvusVectorDB] âœ… æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯æ–°çš„ï¼Œæ— éœ€è·³è¿‡")
            else:
                print(f"[MilvusVectorDB] âœ… æœªå‘ç°å·²å­˜åœ¨çš„æ–‡æ¡£ï¼Œå°†å¤„ç†æ‰€æœ‰ {len(segments)} ä¸ªæ–‡æ¡£")
        
        if not segments:
            print(f"[MilvusVectorDB] âœ… æ‰€æœ‰æ–‡æ¡£éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€æ’å…¥")
            return
        
        # æ‰“å°å…¥åº“å‰çš„æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
        print("\n" + "="*80)
        print("ã€å…¥åº“å‰æ•°æ®æ£€æŸ¥ã€‘")
        print("="*80)
        for i, seg in enumerate(segments, 1):
            print(f"\næ–‡æ¡£ {i}/{len(segments)}:")
            print(f"  doc_id: {seg.doc_id}")
            print(f"  title: {seg.title[:80]}..." if len(seg.title) > 80 else f"  title: {seg.title}")
            print(f"  timestamp: {seg.timestamp}")
            print(f"  industries: {seg.industries}")  # â­ ç»è¿‡DS32Bè¿‡æ»¤åçš„è¡Œä¸š
            print(f"  investment_relevance: {seg.metadata.get('investment_relevance', 'N/A')}")
            print(f"  report_series: {seg.metadata.get('report_series', 'N/A')}")  # â­ æŠ¥å‘Šç³»åˆ—
            print(f"  content_length: {len(seg.content)} å­—ç¬¦")
            print(f"  content_preview: {seg.content[:200]}..." if len(seg.content) > 200 else f"  content: {seg.content}")
            
            # æ˜¾ç¤ºè¡Œä¸šæ”¿ç­–ç‰‡æ®µ
            industry_segments = seg.metadata.get('industry_policy_segments', {})
            if industry_segments:
                print(f"  industry_policy_segments:")
                for industry, segments_list in industry_segments.items():
                    print(f"    - {industry}: {len(segments_list)} ä¸ªç‰‡æ®µ")
                    for j, segment_text in enumerate(segments_list[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ª
                        preview = segment_text[:100] + "..." if len(segment_text) > 100 else segment_text
                        print(f"      ç‰‡æ®µ{j}: {preview}")
        print("="*80 + "\n")
        
        try:
            # åªæ·»åŠ chunkçº§å‘é‡
            self._add_chunk_level(segments, batch_size)
            
            print(f"[MilvusVectorDB] âœ… å…¨éƒ¨æ’å…¥å®Œæˆ")
            print(f"[MilvusVectorDB]   - Chunkçº§: {self.chunk_collection.num_entities} ä¸ª")
        except Exception as e:
            print(f"[MilvusVectorDB] âŒ æ•°æ®æ’å…¥å¤±è´¥: {e}")
            print(f"[MilvusVectorDB] é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            print(f"[MilvusVectorDB] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            print(f"[MilvusVectorDB] ğŸ’¡ æç¤º: å·²æ’å…¥çš„æ•°æ®å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œç¨‹åºç»§ç»­æ’å…¥å‰©ä½™æ•°æ®ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰")
            raise e
    
    # åˆ é™¤æ–‡æ¡£çº§å‘é‡æ·»åŠ æ–¹æ³•ï¼Œåªä½¿ç”¨chunkçº§åˆ«
    
    def _add_chunk_level(self, segments: List[PolicySegment], batch_size: int = 100):
        """æ·»åŠ Chunkçº§åˆ«æ•°æ®åˆ°Milvus"""
        if not segments:
            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ æ²¡æœ‰æ–‡æ¡£éœ€è¦åˆ‡åˆ†")
            return
        
        # æ£€æŸ¥é›†åˆæ˜¯å¦éœ€è¦åŠ è½½ï¼ˆç©ºé›†åˆä¸éœ€è¦loadï¼Œæ’å…¥æ—¶ä¼šè‡ªåŠ¨åŠ è½½ï¼‰
        try:
            # å…ˆæ£€æŸ¥é›†åˆæ˜¯å¦ä¸ºç©º
            entity_count = self.chunk_collection.num_entities
            print(f"[MilvusVectorDB] [Chunkçº§] ğŸ” å½“å‰é›†åˆå®ä½“æ•°: {entity_count}")
            
            if entity_count == 0:
                # ç©ºé›†åˆï¼Œä¸éœ€è¦loadï¼Œæ’å…¥æ•°æ®æ—¶ä¼šè‡ªåŠ¨åŠ è½½
                print(f"[MilvusVectorDB] [Chunkçº§] âœ… ç©ºé›†åˆï¼Œè·³è¿‡loadï¼ˆæ’å…¥æ•°æ®æ—¶ä¼šè‡ªåŠ¨åŠ è½½ï¼‰")
            else:
                # éç©ºé›†åˆï¼Œéœ€è¦å…ˆåŠ è½½æ‰èƒ½æ’å…¥æ–°æ•°æ®
                try:
                    # å°è¯•æŸ¥è¯¢ï¼Œå¦‚æœèƒ½æŸ¥è¯¢è¯´æ˜å·²åŠ è½½
                    self.chunk_collection.query(expr="id >= 0", limit=1, output_fields=["id"])
                    print(f"[MilvusVectorDB] [Chunkçº§] âœ… é›†åˆå·²åœ¨å†…å­˜ä¸­")
                except:
                    # æœªåŠ è½½ï¼Œæ‰§è¡ŒåŠ è½½ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                    print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ é›†åˆæœ‰æ•°æ®ä½†æœªåŠ è½½ï¼Œæ­£åœ¨åŠ è½½...")
                    
                    import threading
                    
                    load_success = [False]
                    load_error = [None]
                    
                    def load_in_thread():
                        try:
                            self.chunk_collection.load()
                            load_success[0] = True
                        except Exception as e:
                            load_error[0] = e
                    
                    load_thread = threading.Thread(target=load_in_thread, daemon=True)
                    load_thread.start()
                    load_thread.join(timeout=30)  # 30ç§’è¶…æ—¶
                    
                    if load_thread.is_alive():
                        print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œå°è¯•ç»§ç»­æ’å…¥")
                        print(f"[MilvusVectorDB] [Chunkçº§] ğŸ’¡ å¦‚æœåç»­æ’å…¥å¤±è´¥ï¼Œæ£€æŸ¥MinIO: docker ps | grep minio")
                    elif load_error[0]:
                        error_msg = str(load_error[0])
                        if "collection not loaded" in error_msg.lower():
                            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ é›†åˆæœªåŠ è½½ï¼Œå°è¯•ç»§ç»­æ’å…¥ï¼ˆMilvuså¯èƒ½ä¼šè‡ªåŠ¨åŠ è½½ï¼‰")
                        else:
                            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¤±è´¥ï¼Œä½†å°è¯•ç»§ç»­æ’å…¥: {load_error[0]}")
                    elif load_success[0]:
                        print(f"[MilvusVectorDB] [Chunkçº§] âœ… é›†åˆå·²åŠ è½½")
        except Exception as check_error:
            # æ£€æŸ¥å¤±è´¥ï¼Œå‡è®¾æ˜¯ç©ºé›†åˆï¼Œç»§ç»­æ’å…¥
            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ æ£€æŸ¥é›†åˆçŠ¶æ€å¤±è´¥: {check_error}ï¼Œå‡è®¾ä¸ºç©ºé›†åˆï¼Œç»§ç»­æ’å…¥")
        
        print(f"[MilvusVectorDB] [Chunkçº§] å¼€å§‹åˆ‡åˆ†å’Œå‘é‡åŒ–...")
        
        all_chunks: List[DocumentChunk] = []
        chunk_texts = []
        
        # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œåˆ‡åˆ†ï¼ˆä¼ å…¥å®Œæ•´å…ƒæ•°æ®ï¼‰
        for seg in segments:
            # â­ ç»Ÿä¸€timestampå¤„ç†ï¼Œä¸generate_insights.pyä¿æŒä¸€è‡´
            timestamp_str = seg.timestamp.isoformat() if seg.timestamp else ""
            # ä»metadataä¸­æå–æŠ•èµ„ç›¸å…³æ€§æ ‡ç­¾ã€æŠ¥å‘Šç³»åˆ—å’Œè¡Œä¸šæ”¿ç­–ç‰‡æ®µ
            investment_relevance = seg.metadata.get('investment_relevance', '')
            report_series = seg.metadata.get('report_series', 'N/A')  # â­ æŠ¥å‘Šç³»åˆ—
            industry_policy_segments_dict = seg.metadata.get('industry_policy_segments', {})
            # åºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸²
            import json
            industry_policy_segments_json = json.dumps(industry_policy_segments_dict, ensure_ascii=False) if industry_policy_segments_dict else ""
            chunks = self.chunker.chunk_document(
                doc_id=seg.doc_id,
                title=seg.title,
                content=seg.content,
                timestamp=timestamp_str,
                industries=','.join(seg.industries) if seg.industries else '',
                investment_relevance=investment_relevance,
                report_series=report_series,
                industry_policy_segments=industry_policy_segments_json
            )
            
            # æ”¶é›†chunkå’Œæ–‡æœ¬ï¼ˆå…ƒæ•°æ®å·²åœ¨chunkå¯¹è±¡ä¸­ï¼‰
            for chunk in chunks:
                all_chunks.append(chunk)
                chunk_texts.append(chunk.content)
        
        print(f"[MilvusVectorDB] [Chunkçº§] å…±åˆ‡åˆ†ä¸º {len(all_chunks)} ä¸ªchunks")
        
        # æ‰“å°åˆ‡åˆ†åçš„chunkè¯¦ç»†ä¿¡æ¯
        print("\n" + "="*80)
        print("ã€Chunkåˆ‡åˆ†åæ•°æ®æ£€æŸ¥ã€‘")
        print("="*80)
        for i, chunk in enumerate(all_chunks[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªchunk
            print(f"\nChunk {i}/{min(5, len(all_chunks))}:")
            print(f"  chunk_id: {chunk.chunk_id}")
            print(f"  doc_id: {chunk.doc_id}")
            print(f"  chunk_index: {chunk.chunk_index}")
            print(f"  chunk_type: {chunk.chunk_type}")
            print(f"  title: {chunk.title[:50]}..." if len(chunk.title) > 50 else f"  title: {chunk.title}")
            print(f"  timestamp: {chunk.timestamp}")
            print(f"  industries: {chunk.industries}")  # â­ è¿‡æ»¤åçš„è¡Œä¸š
            print(f"  investment_relevance: {chunk.investment_relevance}")
            print(f"  report_series: {chunk.report_series}")  # â­ æŠ¥å‘Šç³»åˆ—
            print(f"  content_length: {len(chunk.content)} å­—ç¬¦")
            print(f"  content: {chunk.content[:150]}..." if len(chunk.content) > 150 else f"  content: {chunk.content}")
            
            # æ˜¾ç¤ºè¡Œä¸šæ”¿ç­–ç‰‡æ®µï¼ˆJSONæ ¼å¼ï¼‰
            if chunk.industry_policy_segments:
                import json
                try:
                    segments_dict = json.loads(chunk.industry_policy_segments)
                    print(f"  industry_policy_segments: {list(segments_dict.keys())}")
                except:
                    print(f"  industry_policy_segments: (JSONè§£æå¤±è´¥)")
        
        if len(all_chunks) > 5:
            print(f"\n... è¿˜æœ‰ {len(all_chunks) - 5} ä¸ªchunksæœªæ˜¾ç¤º")
        print("="*80 + "\n")
        
        # ğŸ” æ­¥éª¤1ï¼šæ£€æŸ¥åŸå§‹all_chunksæ•°æ®ï¼ˆæŠ€æœ¯è°ƒè¯•ç”¨ï¼‰
        print(f"\n[è°ƒè¯•1] æ£€æŸ¥åŸå§‹all_chunkså¯¹è±¡:")
        for i, chunk in enumerate(all_chunks[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
            print(f"  Chunk {i}:")
            print(f"    chunk_idç±»å‹={type(chunk.chunk_id)}, é•¿åº¦={len(chunk.chunk_id)}")
            print(f"    doc_idç±»å‹={type(chunk.doc_id)}, é•¿åº¦={len(chunk.doc_id)}")
            print(f"    contentç±»å‹={type(chunk.content)}, é•¿åº¦={len(chunk.content)}")
            print(f"    chunk_typeç±»å‹={type(chunk.chunk_type)}, é•¿åº¦={len(chunk.chunk_type)}")
        
        # â­ ä¼˜åŒ–ï¼šé™åˆ¶500å­—ç¬¦ï¼ˆembedding max_tokens=512ï¼Œä¸­æ–‡çº¦500å­—ç¬¦å®‰å…¨ï¼‰
        MAX_CHUNK_LEN = 450  # ä¸Milvusç°æœ‰æ•°æ®ä¸€è‡´
        before_max = max(len(chunk.content) for chunk in all_chunks) if all_chunks else 0
        
        truncated_count = 0
        for i, chunk in enumerate(all_chunks):
            # æ£€æŸ¥å¹¶åŒæ­¥chunk_texts
            if len(chunk.content) > MAX_CHUNK_LEN:
                print(f"âš ï¸ chunk {chunk.chunk_id} è¶…é•¿({len(chunk.content)})ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿï¼")
                chunk.content = chunk.content[:MAX_CHUNK_LEN]
                truncated_count += 1
            chunk_texts[i] = chunk.content[:MAX_CHUNK_LEN]  # åŒæ­¥
        
        after_max = max(len(chunk.content) for chunk in all_chunks) if all_chunks else 0
        
        print(f"[MilvusVectorDB] [Chunkçº§] é•¿åº¦æ£€æŸ¥ (é™åˆ¶{MAX_CHUNK_LEN}å­—ç¬¦ï¼Œembedding max_tokens=512):")
        print(f"  - æœ€å¤§é•¿åº¦: {before_max} å­—ç¬¦")
        if truncated_count > 0:
            print(f"  - âš ï¸ å‘ç°{truncated_count}ä¸ªè¶…é•¿chunkï¼ˆå·²æˆªæ–­ï¼‰")
        
        # ä½¿ç”¨GPUæ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        embeddings = self.model.encode(
            chunk_texts,
            device=device,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True  # â­ å½’ä¸€åŒ–ï¼šæ–¹ä¾¿è®¡ç®—ç›¸ä¼¼åº¦
        )
        
        print(f"[MilvusVectorDB] [Chunkçº§] âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œshape: {embeddings.shape}")
        
        # â­â­â­ æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ä¸è¶…è¿‡450
        final_contents = [str(txt)[:450] for txt in chunk_texts]
        final_max = max(len(c) for c in final_contents) if final_contents else 0
        print(f"[MilvusVectorDB] [Chunkçº§] âœ… æœ€ç»ˆcontenté•¿åº¦: max={final_max} (é™åˆ¶:450)")
        
        # ğŸ” æ­¥éª¤2ï¼šæ£€æŸ¥chunk_textsåˆ—è¡¨
        print(f"\n[è°ƒè¯•2] æ£€æŸ¥è¾…åŠ©åˆ—è¡¨:")
        print(f"  chunk_textsé•¿åº¦: {len(chunk_texts)}")
        if chunk_texts:
            print(f"  chunk_texts[0]ç±»å‹={type(chunk_texts[0])}, é•¿åº¦={len(chunk_texts[0])}")
        if all_chunks:
            print(f"  all_chunks[0]å…ƒæ•°æ®:")
            print(f"    - title: {all_chunks[0].title[:50]}...")
            print(f"    - timestamp: {all_chunks[0].timestamp}")
            print(f"    - industries: {all_chunks[0].industries[:50]}")
        
        # å‡†å¤‡æ’å…¥æ•°æ®ï¼ˆç›´æ¥ä»all_chunksæå–ï¼ŒåŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼‰
        print(f"\n[è°ƒè¯•3] æ„å»ºentitiesæ•°ç»„...")
        
        # ğŸ”¥ å…³é”®ï¼šæ£€æŸ¥embeddingsç»“æ„
        print(f"  embeddingsç±»å‹: {type(embeddings)}")
        print(f"  embeddings shape: {embeddings.shape}")
        print(f"  embeddings[0]ç±»å‹: {type(embeddings[0])}")
        print(f"  embeddings[0] shape/len: {embeddings[0].shape if hasattr(embeddings[0], 'shape') else len(embeddings[0])}")
        
        # è½¬æ¢embeddings
        embeddings_list = embeddings.tolist()
        print(f"  embeddings_listç±»å‹: {type(embeddings_list)}")
        print(f"  embeddings_list[0]ç±»å‹: {type(embeddings_list[0])}")
        print(f"  embeddings_list[0]é•¿åº¦: {len(embeddings_list[0])}")
        
        # â­ Milvus çš„ VARCHAR(max_length=...) æ˜¯æŒ‰â€œå­—èŠ‚é•¿åº¦â€é™åˆ¶ï¼ˆUTF-8ï¼‰ï¼Œä¸æ˜¯æŒ‰å­—ç¬¦æ•°
        # ä¸­æ–‡å­—ç¬¦é€šå¸¸3å­—èŠ‚ï¼šå³ä½¿len(title)=185ï¼Œä¹Ÿå¯èƒ½>500å­—èŠ‚è€Œæ’å…¥å¤±è´¥ã€‚
        def _truncate_utf8(value: Any, max_bytes: int) -> str:
            s = "" if value is None else str(value)
            b = s.encode("utf-8")
            if len(b) <= max_bytes:
                return s
            # é€æ­¥ç¼©çŸ­ç›´åˆ°å­—èŠ‚é•¿åº¦<=max_bytesï¼ˆä¿è¯ä¸æˆªæ–­åœ¨UTF-8ä¸­é—´ï¼‰
            cut = max_bytes
            while cut > 0:
                try:
                    return b[:cut].decode("utf-8")
                except UnicodeDecodeError:
                    cut -= 1
            return ""  # æç«¯æƒ…å†µ

        entities = [
            [_truncate_utf8(chunk.chunk_id, 150) for chunk in all_chunks],  # chunk_id
            [_truncate_utf8(chunk.doc_id, 100) for chunk in all_chunks],  # doc_id
            embeddings_list,  # embedding
            [_truncate_utf8(chunk.content, 450) for chunk in all_chunks],  # contentï¼ˆé™åˆ¶450å­—ç¬¦/å­—èŠ‚ï¼›è¿™é‡ŒæŒ‰å­—èŠ‚æˆªæ–­æ›´å®‰å…¨ï¼‰
            [chunk.chunk_index for chunk in all_chunks],  # chunk_index
            [_truncate_utf8(chunk.chunk_type, 20) for chunk in all_chunks],  # chunk_type
            [_truncate_utf8(chunk.title, 500) for chunk in all_chunks],  # titleï¼ˆæŒ‰å­—èŠ‚ï¼‰
            [_truncate_utf8(chunk.timestamp, 150) for chunk in all_chunks],  # timestamp
            [_truncate_utf8(chunk.industries, 500) for chunk in all_chunks],  # industriesï¼ˆæŒ‰å­—èŠ‚ï¼‰
            [_truncate_utf8(chunk.investment_relevance, 10) for chunk in all_chunks],  # investment_relevance
            [_truncate_utf8(chunk.report_series, 50) for chunk in all_chunks],  # report_series
            [_truncate_utf8(chunk.industry_policy_segments, 20000) for chunk in all_chunks],  # industry_policy_segments
        ]
        
        # ğŸ” æ­¥éª¤3ï¼šç«‹å³æ£€æŸ¥æ„å»ºåçš„entities
        print(f"[è°ƒè¯•3] entitiesæ„å»ºå®Œæˆï¼Œæ£€æŸ¥å‰3ä¸ªå…ƒç´ :")
        for idx in range(10):
            if idx < len(entities) and entities[idx]:
                item = entities[idx][0] if entities[idx] else None
                if isinstance(item, str):
                    print(f"  entities[{idx}][0]: type=str, len={len(item)}")
                elif isinstance(item, list):
                    print(f"  entities[{idx}][0]: type=list, len={len(item)}")
                else:
                    print(f"  entities[{idx}][0]: type={type(item)}")
        
        # ğŸ” ç»ˆæä¿®å¤ï¼šç›´æ¥åœ¨entitiesæ•°ç»„ä¸­å¼ºåˆ¶æˆªæ–­æ‰€æœ‰å­—æ®µ
        print(f"[MilvusVectorDB] [Chunkçº§] ===== å¼€å§‹ç»ˆæå­—æ®µé•¿åº¦æ£€æŸ¥ =====")
        
        # å­—æ®µé…ç½®ï¼ˆç´¢å¼•ï¼Œåç§°ï¼Œæœ€å¤§é•¿åº¦ï¼‰
        # entitiesæ•°ç»„é¡ºåº: 0=chunk_id, 1=doc_id, 2=embedding, 3=content, 4=chunk_index, 
        #                   5=chunk_type, 6=title, 7=timestamp, 8=industries, 
        #                   9=investment_relevance, 10=report_series, 11=industry_policy_segments
        string_fields_config = [
            (0, 'chunk_id', 150),
            (1, 'doc_id', 100),
            (3, 'content', 450),  # â­ å…³é”®é™åˆ¶ï¼šembedding max_tokens=512ï¼Œä¸­æ–‡çº¦450å­—ç¬¦
            (5, 'chunk_type', 20),
            (6, 'title', 500),  # â­ ä¿®å¤ï¼štitleæ˜¯ç´¢å¼•6ï¼Œä¸æ˜¯7
            (7, 'timestamp', 150),
            (8, 'industries', 500),  # ä¸­ä¿¡ä¸€çº§è¡Œä¸š
            (9, 'investment_relevance', 10),  # æŠ•èµ„ç›¸å…³æ€§
            (10, 'report_series', 50),  # æŠ¥å‘Šç³»åˆ—
            (11, 'industry_policy_segments', 20000),  # è¡Œä¸šåŠå¯¹åº”æ”¿ç­–ç‰‡æ®µ
        ]
        
        total_truncated = 0
        for idx, name, max_len in string_fields_config:
            # æ£€æŸ¥å½“å‰æœ€å¤§é•¿åº¦
            field = entities[idx]
            current_max = max(len(str(field[i])) for i in range(len(field))) if field else 0
            print(f"  [{idx}] {name:15s}: æœ€å¤§ {current_max:6d} / é™åˆ¶ {max_len:6d}", end="")
            
            # å¦‚æœè¶…é•¿ï¼Œç«‹å³æˆªæ–­
            if current_max > max_len:
                print(f"  âš ï¸ è¶…é•¿ï¼")
                truncated_count = 0
                for i in range(len(field)):
                    item_len = len(str(field[i]))
                    if item_len > max_len:
                        old_val = str(field[i])
                        # æ‰“å°å‰50å­—ç¬¦çš„æ ·æœ¬ï¼Œæ‰¾å‡ºçœŸå‡¶
                        if truncated_count == 0:
                            print(f"      ğŸ” ç¬¬1ä¸ªè¶…é•¿å…ƒç´ [{i}]:")
                            print(f"         ç±»å‹: {type(field[i])}")
                            print(f"         é•¿åº¦: {item_len}")
                            print(f"         å‰100å­—ç¬¦: {old_val[:100]}")
                        
                        field[i] = old_val[:max_len]  # ç›´æ¥ä¿®æ”¹
                        truncated_count += 1
                
                print(f"      âœ… æˆªæ–­äº† {truncated_count} ä¸ªå…ƒç´ ")
                total_truncated += truncated_count
                
                # éªŒè¯æˆªæ–­ç»“æœ
                new_max = max(len(str(field[i])) for i in range(len(field)))
                print(f"      âœ… æ–°æœ€å¤§é•¿åº¦: {new_max}")
            else:
                print(f"  âœ…")
        
        print(f"[MilvusVectorDB] [Chunkçº§] ===== æ£€æŸ¥å®Œæˆï¼Œå…±æˆªæ–­ {total_truncated} ä¸ªå…ƒç´  =====")
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ æœ€ç»ˆéªŒè¯ï¼šæ’å…¥å‰1ç§’å†æ£€æŸ¥ä¸€æ¬¡
        print(f"\n[MilvusVectorDB] [Chunkçº§] ğŸ”¥ æ’å…¥å‰æœ€ç»ˆéªŒè¯ ğŸ”¥")
        print(f"  entitiesæ•°ç»„é•¿åº¦: {len(entities)}")
        print(f"  entities[2] (embedding) ç±»å‹: {type(entities[2])}")
        print(f"  entities[2] å…ƒç´ æ•°: {len(entities[2])}")
        if entities[2]:
            print(f"  entities[2][0] ç±»å‹: {type(entities[2][0])}")
            print(f"  entities[2][0] é•¿åº¦: {len(entities[2][0]) if isinstance(entities[2][0], list) else 'N/A'}")
        
        for idx in [0, 1, 3, 5, 6, 7, 8, 9, 10, 11]:  # æ‰€æœ‰VARCHARå­—æ®µ
            if idx < len(entities):
                max_len_now = max(len(str(entities[idx][i])) for i in range(len(entities[idx])))
                print(f"  entities[{idx}] å½“å‰æœ€å¤§é•¿åº¦: {max_len_now}")
        
        print(f"[MilvusVectorDB] [Chunkçº§] âœ… æœ€ç»ˆéªŒè¯å®Œæˆ\n")
        
        # ğŸ” æ‰“å°Milvus collectionçš„å®é™…schema
        print(f"\n[è°ƒè¯•5] æ£€æŸ¥Milvus schema:")
        schema = self.chunk_collection.schema
        for field in schema.fields:
            if field.dtype == DataType.VARCHAR:
                max_len = getattr(field, 'max_length', 'N/A')
                print(f"  {field.name}: VARCHAR(max_length={max_len})")
        
        # ğŸ” æ­¥éª¤6ï¼šæ’å…¥å‰ç»ˆææ£€æŸ¥ - æ‰«ææ‰€æœ‰å…ƒç´ æ‰¾è¶…é•¿é¡¹
        print(f"\n[è°ƒè¯•6] æ’å…¥å‰æ‰«ææ‰€æœ‰å…ƒç´ ï¼ˆæŸ¥æ‰¾è¶…é•¿é¡¹ï¼‰:")
        print(f"  âš ï¸ æ³¨æ„ï¼šMilvusæŠ¥é”™'1th string'æ˜¯ç¬¬2ä¸ªVARCHARå­—æ®µ = doc_id")
        print(f"  âš ï¸ å¦‚æœdoc_idæ”¶åˆ°contentçš„å€¼ï¼Œè¯´æ˜entitiesé¡ºåºé”™äº†ï¼")
        
        field_names_for_debug = ['chunk_id', 'doc_id', 'embedding', 'content', 'chunk_index', 
                                  'chunk_type', 'title', 'timestamp', 'industries', 'investment_relevance', 'report_series', 'industry_policy_segments']
        
        # å­—æ®µé•¿åº¦é™åˆ¶æ˜ å°„ï¼ˆMilvusæŒ‰UTF-8å­—èŠ‚è®¡æ•°ï¼‰
        field_max_lengths = {
            0: 150,   # chunk_id
            1: 100,   # doc_id
            3: 450,   # content
            5: 20,    # chunk_type
            6: 500,   # title â­ é‡è¦
            7: 150,   # timestamp
            8: 500,   # industries
            9: 10,    # investment_relevance
            10: 50,   # report_series
            11: 20000 # industry_policy_segments
        }
        
        # å®Œæ•´æ£€æŸ¥å¹¶æˆªæ–­æ‰€æœ‰VARCHARå­—æ®µ
        print(f"\n[è°ƒè¯•6] æ£€æŸ¥å¹¶æŒ‰UTF-8å­—èŠ‚æˆªæ–­æ‰€æœ‰VARCHARå­—æ®µ:")
        for idx in [0, 1, 3, 5, 6, 7, 8, 9, 10, 11]:
            if idx >= len(entities):
                continue
            field_name = field_names_for_debug[idx] if idx < len(field_names_for_debug) else f'field_{idx}'
            field_data = entities[idx]
            max_allowed = field_max_lengths.get(idx, 500)
            
            # æ‰¾å‡ºæ‰€æœ‰é•¿åº¦ï¼ˆæŒ‰UTF-8å­—èŠ‚ï¼‰
            lengths = [len(str(item).encode("utf-8")) for item in field_data]
            current_max = max(lengths) if lengths else 0
            
            # æ‰¾å‡ºå¹¶æˆªæ–­è¶…é•¿é¡¹
            truncated = 0
            for i, length in enumerate(lengths):
                if length > max_allowed:
                    field_data[i] = _truncate_utf8(field_data[i], max_allowed)
                    truncated += 1
            
            if truncated > 0:
                print(f"  [{idx}] {field_name}: æˆªæ–­äº† {truncated} ä¸ªå…ƒç´  (é™åˆ¶{max_allowed})")
            else:
                print(f"  [{idx}] {field_name}: âœ… æ— è¶…é•¿ (max={current_max}, é™åˆ¶{max_allowed})")
        
        # æœ€ç»ˆéªŒè¯
        print(f"\n[è°ƒè¯•7] æˆªæ–­åéªŒè¯:")
        for idx in [0, 1, 3, 5, 6, 7, 8, 9, 10, 11]:
            if idx >= len(entities):
                continue
            max_len = max(len(str(item).encode("utf-8")) for item in entities[idx])
            max_allowed = field_max_lengths.get(idx, 500)
            status = "âœ…" if max_len <= max_allowed else "âŒ"
            print(f"  entities[{idx}]: max={max_len} / é™åˆ¶{max_allowed} {status}")
        
        # â­ åˆ†æ‰¹æ’å…¥ï¼ˆé¿å…gRPCæ¶ˆæ¯å¤§å°é™åˆ¶ï¼š64MBï¼‰
        print(f"\n[MilvusVectorDB] [Chunkçº§] ğŸš€ å¼€å§‹åˆ†æ‰¹æ’å…¥æ•°æ®...")
        print(f"  æ€»å…± {len(entities[0])} ä¸ªchunks")
        print(f"  entitiesæ•°ç»„ç»“æ„: {len(entities)} ä¸ªå­—æ®µ")
        
        CHUNK_INSERT_BATCH = 500  # æ¯æ‰¹500ä¸ªchunksï¼ˆçº¦4MBï¼‰
        total_chunks = len(entities[0])
        total_inserted = 0
        
        # â­ æ£€æŸ¥å¹¶æˆªæ–­ industry_policy_segments å­—æ®µï¼ˆç´¢å¼•11ï¼‰
        print(f"\n[MilvusVectorDB] [Chunkçº§] æ£€æŸ¥ industry_policy_segments å­—æ®µé•¿åº¦...")
        max_segments_length = 20000  # ä¸Schemaä¸­çš„max_lengthä¸€è‡´
        truncated_count = 0
        for i in range(len(entities[11])):
            seg_str = str(entities[11][i])
            if len(seg_str) > max_segments_length:
                # å°è¯•æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™JSONç»“æ„
                try:
                    import json
                    seg_dict = json.loads(seg_str)
                    # å¦‚æœJSONå¤ªå¤§ï¼Œæˆªæ–­æ¯ä¸ªè¡Œä¸šçš„æ”¿ç­–ç‰‡æ®µåˆ—è¡¨
                    for industry, segments_list in seg_dict.items():
                        if isinstance(segments_list, list):
                            # é™åˆ¶æ¯ä¸ªè¡Œä¸šçš„ç‰‡æ®µæ•°é‡ï¼Œå¹¶æˆªæ–­æ¯ä¸ªç‰‡æ®µé•¿åº¦
                            max_segments_per_industry = 20
                            max_segment_length = 200
                            seg_dict[industry] = [
                                seg[:max_segment_length] if len(seg) > max_segment_length else seg
                                for seg in segments_list[:max_segments_per_industry]
                            ]
                    # é‡æ–°åºåˆ—åŒ–
                    new_seg_str = json.dumps(seg_dict, ensure_ascii=False)
                    if len(new_seg_str) > max_segments_length:
                        # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œç›´æ¥æˆªæ–­
                        new_seg_str = new_seg_str[:max_segments_length-3] + "..."
                    entities[11][i] = new_seg_str
                    truncated_count += 1
                except:
                    # JSONè§£æå¤±è´¥ï¼Œç›´æ¥æˆªæ–­
                    entities[11][i] = seg_str[:max_segments_length-3] + "..."
                    truncated_count += 1
        
        if truncated_count > 0:
            print(f"  âš ï¸ å·²æˆªæ–­ {truncated_count} ä¸ªè¶…é•¿çš„ industry_policy_segments å­—æ®µ")
        else:
            print(f"  âœ… æ‰€æœ‰ industry_policy_segments å­—æ®µé•¿åº¦æ­£å¸¸")
        
        for i in range(0, total_chunks, CHUNK_INSERT_BATCH):
            end_idx = min(i + CHUNK_INSERT_BATCH, total_chunks)
            
            # å‡†å¤‡æ‰¹æ¬¡æ•°æ® - ä¿®å¤å­—æ®µé¡ºåºåŒ¹é…Schema
            batch_entities = [
                entities[0][i:end_idx],  # chunk_id
                entities[1][i:end_idx],  # doc_id
                entities[2][i:end_idx],  # embedding
                entities[3][i:end_idx],  # content
                entities[4][i:end_idx],  # chunk_index
                entities[5][i:end_idx],  # chunk_type
                entities[6][i:end_idx],  # title
                entities[7][i:end_idx],  # timestamp
                entities[8][i:end_idx],  # industries
                entities[9][i:end_idx],  # investment_relevance
                entities[10][i:end_idx],  # report_series
                entities[11][i:end_idx],  # industry_policy_segments
            ]
            
            # æ’å…¥æ‰¹æ¬¡ï¼ˆæ·»åŠ é”™è¯¯å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ æç¤ºï¼‰
            batch_num = i//CHUNK_INSERT_BATCH + 1
            total_batches = (total_chunks-1)//CHUNK_INSERT_BATCH + 1
            print(f"[MilvusVectorDB] [Chunkçº§] æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} ({end_idx-i} chunks)...")
            try:
                self.chunk_collection.insert(batch_entities)
                total_inserted += (end_idx - i)
            except Exception as e:
                print(f"[MilvusVectorDB] âŒ æ‰¹æ¬¡ {batch_num} æ’å…¥å¤±è´¥: {e}")
                print(f"[MilvusVectorDB] ğŸ’¡ æç¤º: å·²æˆåŠŸæ’å…¥å‰ {total_inserted} ä¸ªchunks")
                print(f"[MilvusVectorDB] ğŸ’¡ æç¤º: å¯ä»¥é‡æ–°è¿è¡Œç¨‹åºï¼Œå·²æ’å…¥çš„æ•°æ®ä¸ä¼šé‡å¤ï¼ˆMilvusä¼šè‡ªåŠ¨å»é‡ï¼‰")
                raise e
        
        # æ‰§è¡Œflushï¼ˆæ•°æ®æŒä¹…åŒ–ï¼‰- æ·»åŠ è¶…æ—¶ä¿æŠ¤
        # æ³¨æ„ï¼šæ’å…¥æ•°æ®åä¸éœ€è¦loadé›†åˆï¼Œç›´æ¥flushå³å¯
        print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ æ­£åœ¨flushæ•°æ®åˆ°å­˜å‚¨...")
        print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ å¦‚æœé•¿æ—¶é—´å¡åœ¨æ­¤å¤„ï¼Œå¯èƒ½æ˜¯Milvus rootcoordæœåŠ¡å¼‚å¸¸")
        
        import threading
        
        flush_success = [False]
        flush_error = [None]
        
        def flush_in_thread():
            try:
                # â­ flushæ“ä½œï¼šåˆ·æ–°æ•°æ®åˆ°ç£ç›˜ï¼ˆå¯èƒ½å¤±è´¥ï¼Œä½†ä¸å½±å“æ•°æ®æ’å…¥ï¼‰
                try:
                    self.chunk_collection.flush()
                    print(f"[MilvusVectorDB] âœ… æ•°æ®å·²åˆ·æ–°åˆ°ç£ç›˜")
                except Exception as flush_error:
                    # flushå¤±è´¥ä¸å½±å“æ•°æ®æ’å…¥ï¼Œæ•°æ®å·²ç»åœ¨Milvusä¸­
                    error_msg = str(flush_error)
                    if "channel not found" in error_msg or "rootcoord" in error_msg:
                        print(f"[MilvusVectorDB] âš ï¸ flushæ“ä½œå¤±è´¥ï¼ˆMilvusæœåŠ¡å†…éƒ¨é”™è¯¯ï¼‰ï¼Œä½†æ•°æ®å·²æˆåŠŸæ’å…¥")
                        print(f"[MilvusVectorDB] ğŸ’¡ å»ºè®®ï¼šå¦‚æœé¢‘ç¹å‡ºç°æ­¤é”™è¯¯ï¼Œè¯·é‡å¯MilvusæœåŠ¡")
                    else:
                        print(f"[MilvusVectorDB] âš ï¸ flushæ“ä½œå¤±è´¥: {flush_error}")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæ•°æ®å·²ç»æ’å…¥æˆåŠŸ
                flush_success[0] = True
            except Exception as e:
                flush_error[0] = e
        
        flush_thread = threading.Thread(target=flush_in_thread, daemon=True)
        flush_thread.start()
        flush_thread.join(timeout=60)  # 60ç§’è¶…æ—¶
        
        if flush_thread.is_alive():
            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ Flushè¶…æ—¶ï¼ˆ60ç§’ï¼‰ï¼Œè·³è¿‡flushç»§ç»­æ‰§è¡Œ")
            print(f"[MilvusVectorDB] [Chunkçº§] ğŸ’¡ å¯èƒ½åŸå› ï¼šMilvus rootcoordæœåŠ¡å¼‚å¸¸")
            print(f"[MilvusVectorDB] [Chunkçº§] ğŸ’¡ æ’æŸ¥æ­¥éª¤ï¼š")
            print(f"   1. æ£€æŸ¥Milvuså®¹å™¨çŠ¶æ€: docker ps | grep milvus")
            print(f"   2. æŸ¥çœ‹Milvusæ—¥å¿—: docker logs milvus-standalone --tail 50")
            print(f"   3. é‡å¯MilvusæœåŠ¡: docker restart milvus-standalone")
            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ æ•°æ®å·²æ’å…¥ä½†æœªflushï¼ŒMilvusä¼šåœ¨åå°è‡ªåŠ¨flush")
            # ç¡®ä¿é›†åˆè¢«åŠ è½½åˆ°å†…å­˜ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ å°è¯•åŠ è½½é›†åˆåˆ°å†…å­˜...")
            max_retries = 3
            for retry in range(max_retries):
                try:
                    # ä½¿ç”¨çº¿ç¨‹+è¶…æ—¶é¿å…å¡æ­»
                    load_done = [False]
                    load_err = [None]
                    
                    def do_load():
                        try:
                            self.chunk_collection.load()
                            load_done[0] = True
                        except Exception as e:
                            load_err[0] = e
                    
                    load_t = threading.Thread(target=do_load, daemon=True)
                    load_t.start()
                    load_t.join(timeout=30)  # 30ç§’è¶…æ—¶
                    
                    if load_done[0]:
                        print(f"[MilvusVectorDB] [Chunkçº§] âœ… é›†åˆå·²åŠ è½½åˆ°å†…å­˜")
                        break
                    elif load_t.is_alive():
                        print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½è¶…æ—¶ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                        if retry < max_retries - 1:
                            print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ ç­‰å¾…5ç§’åé‡è¯•...")
                            import time
                            time.sleep(5)
                    else:
                        print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¤±è´¥: {load_err[0]} ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                        if retry < max_retries - 1:
                            print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ ç­‰å¾…5ç§’åé‡è¯•...")
                            import time
                            time.sleep(5)
                except Exception as e:
                    print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¼‚å¸¸: {e} ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç¨‹åºç»§ç»­
        elif flush_error[0]:
            error_msg = str(flush_error[0])
            if "channel not found" in error_msg.lower():
                print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ Flushå¤±è´¥ï¼šMilvus rootcoordæœåŠ¡å¼‚å¸¸")
                print(f"[MilvusVectorDB] [Chunkçº§] ğŸ’¡ å¯èƒ½åŸå› ï¼š")
                print(f"   1. Milvus rootcoordæœåŠ¡æœªæ­£å¸¸è¿è¡Œ")
                print(f"   2. MilvusæœåŠ¡çŠ¶æ€å¼‚å¸¸")
                print(f"[MilvusVectorDB] [Chunkçº§] ğŸ’¡ æ’æŸ¥æ­¥éª¤ï¼š")
                print(f"   1. æ£€æŸ¥Milvuså®¹å™¨çŠ¶æ€: docker ps | grep milvus")
                print(f"   2. æŸ¥çœ‹Milvusæ—¥å¿—: docker logs milvus-standalone --tail 50")
                print(f"   3. é‡å¯MilvusæœåŠ¡: docker restart milvus-standalone")
                print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ æ•°æ®å·²æ’å…¥ä½†æœªflushï¼ŒMilvusä¼šåœ¨åå°è‡ªåŠ¨flush")
                # ç¡®ä¿é›†åˆè¢«åŠ è½½åˆ°å†…å­˜ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ å°è¯•åŠ è½½é›†åˆåˆ°å†…å­˜...")
                max_retries = 3
                for retry in range(max_retries):
                    try:
                        load_done = [False]
                        load_err = [None]
                        def do_load():
                            try:
                                self.chunk_collection.load()
                                load_done[0] = True
                            except Exception as e:
                                load_err[0] = e
                        load_t = threading.Thread(target=do_load, daemon=True)
                        load_t.start()
                        load_t.join(timeout=30)
                        if load_done[0]:
                            print(f"[MilvusVectorDB] [Chunkçº§] âœ… é›†åˆå·²åŠ è½½åˆ°å†…å­˜")
                            break
                        elif load_t.is_alive():
                            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½è¶…æ—¶ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                            if retry < max_retries - 1:
                                import time
                                time.sleep(5)
                        else:
                            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¤±è´¥: {load_err[0]} ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                            if retry < max_retries - 1:
                                import time
                                time.sleep(5)
                    except Exception as e:
                        print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¼‚å¸¸: {e} ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç¨‹åºç»§ç»­ï¼ˆæ•°æ®å¯èƒ½å·²ç»éƒ¨åˆ†æŒä¹…åŒ–ï¼‰
            else:
                print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ Flushå¤±è´¥: {flush_error[0]}")
                print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ æ•°æ®å·²æ’å…¥ä½†æœªflushï¼Œç»§ç»­æ‰§è¡Œ")
                # ç¡®ä¿é›†åˆè¢«åŠ è½½åˆ°å†…å­˜ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                print(f"[MilvusVectorDB] [Chunkçº§] ğŸ”„ å°è¯•åŠ è½½é›†åˆåˆ°å†…å­˜...")
                max_retries = 3
                for retry in range(max_retries):
                    try:
                        load_done = [False]
                        load_err = [None]
                        def do_load():
                            try:
                                self.chunk_collection.load()
                                load_done[0] = True
                            except Exception as e:
                                load_err[0] = e
                        load_t = threading.Thread(target=do_load, daemon=True)
                        load_t.start()
                        load_t.join(timeout=30)
                        if load_done[0]:
                            print(f"[MilvusVectorDB] [Chunkçº§] âœ… é›†åˆå·²åŠ è½½åˆ°å†…å­˜")
                            break
                        elif load_t.is_alive():
                            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½è¶…æ—¶ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                            if retry < max_retries - 1:
                                import time
                                time.sleep(5)
                        else:
                            print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¤±è´¥: {load_err[0]} ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                            if retry < max_retries - 1:
                                import time
                                time.sleep(5)
                    except Exception as e:
                        print(f"[MilvusVectorDB] [Chunkçº§] âš ï¸ åŠ è½½å¼‚å¸¸: {e} ï¼ˆå°è¯• {retry+1}/{max_retries}ï¼‰")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç¨‹åºç»§ç»­
        elif flush_success[0]:
            print(f"[MilvusVectorDB] [Chunkçº§] âœ… æ•°æ®å·²flushåˆ°å­˜å‚¨")
        
        print(f"[MilvusVectorDB] [Chunkçº§] âœ… æ’å…¥å®Œæˆï¼Œå…± {total_inserted} ä¸ªchunks")
    
    def search_similar(self, query_text: str = None, query_segment: PolicySegment = None,
                      top_k: int = 20, where_filter: Dict = None) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢ (GPUåŠ é€Ÿ)
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_segment: æŸ¥è¯¢æ–‡æ¡£
            top_k: è¿”å›ç»“æœæ•°é‡
            where_filter: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬ï¼ˆä¼˜åŒ–ï¼šè·³è¿‡æ ¼å¼å†…å®¹ï¼Œæå–å®è´¨éƒ¨åˆ†ï¼‰
        if query_segment:
            # æ ‡é¢˜æƒé‡ï¼šé‡å¤3æ¬¡
            title_part = f"{query_segment.title}\n{query_segment.title}\n{query_segment.title}"
            
            # æå–å®è´¨å†…å®¹ï¼ˆè·³è¿‡å‰é¢çš„æ ¼å¼éƒ¨åˆ†ï¼‰
            content = query_segment.content
            start_pos = 200  # é»˜è®¤è·³è¿‡å‰200å­—ï¼ˆå‘æ–‡å•ä½ã€æ–‡å·ç­‰ï¼‰
            for marker in ['ç¬¬ä¸€æ¡', 'ä¸€ã€', 'ï¼ˆä¸€ï¼‰', '1.', 'æ€»åˆ™', 'ç¬¬ä¸€ç« ']:
                pos = content.find(marker)
                if pos > 0 and pos < 500:
                    start_pos = pos
                    break
            
            content_part = content[start_pos:start_pos+1000]
            query_text = f"{title_part}\n\n{content_part}"
        
        if not query_text:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        query_embedding = self.model.encode(
            [query_text],
            device=device,
            convert_to_numpy=True,
            normalize_embeddings=True  # â­ å½’ä¸€åŒ–
        )[0]
        
        # æœç´¢å‚æ•°ï¼ˆefå¿…é¡»>=top_kï¼Œè®¾ç½®ä¸º512ä¿è¯è¶³å¤Ÿå¤§ï¼‰
        search_params = {"metric_type": "L2", "params": {"ef": 512}}
        
        # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼ï¼ˆåªç”¨æ—¶é—´è¿‡æ»¤ï¼Œé¿å…æ¼æ£€ï¼‰
        expr = None
        if where_filter:
            filter_parts = []
            
            # â­ æ ¸å¿ƒï¼šæ—¶é—´èŒƒå›´è¿‡æ»¤
            if 'timestamp_after' in where_filter:
                filter_parts.append(f'timestamp >= "{where_filter["timestamp_after"]}"')
            
            if 'timestamp_before' in where_filter:
                filter_parts.append(f'timestamp <= "{where_filter["timestamp_before"]}"')
            
            if filter_parts:
                expr = ' && '.join(filter_parts)
        
        # æ³¨æ„ï¼šä¸ä½¿ç”¨è¡Œä¸šç­‰å­—æ®µè¿›è¡Œè¿‡æ»¤
        # åŸå› ï¼šé¿å…å› åˆ†ç±»é”™è¯¯å¯¼è‡´æ¼æ£€ï¼Œæé«˜å¬å›ç‡
        
        # æ‰§è¡Œæœç´¢
        results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            expr=expr,  # æ—¶é—´è¿‡æ»¤è¡¨è¾¾å¼
            output_fields=["doc_id", "title", "timestamp", "industries"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
        similar_docs = []
        for hit in results[0]:
            # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆå½’ä¸€åŒ–å‘é‡çš„L2è·ç¦»è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            # å¯¹äºå½’ä¸€åŒ–å‘é‡: cosine_similarity = 1 - (L2_distance^2 / 2)
            # L2è·ç¦»èŒƒå›´: [0, 2]ï¼Œå…¶ä¸­0=å®Œå…¨ç›¸åŒï¼Œ2=å®Œå…¨ç›¸å
            l2_distance = hit.distance
            cosine_similarity = 1.0 - (l2_distance ** 2) / 2.0
            # ç¡®ä¿åœ¨[0, 1]èŒƒå›´å†…
            similarity = max(0.0, min(1.0, cosine_similarity))
            
            similar_docs.append({
                'doc_id': hit.entity.get('doc_id'),
                'title': hit.entity.get('title'),
                'timestamp': hit.entity.get('timestamp'),         # â­ æ ¸å¿ƒï¼šç”¨äºæ—¶é—´è·¨åº¦è®¡ç®—
                'industries': hit.entity.get('industries'),       # â­ æ ¸å¿ƒï¼šç”¨äºè¡Œä¸šè¿‡æ»¤
                'distance': l2_distance,                          # L2è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼ŒèŒƒå›´0-2ï¼‰
                'similarity': similarity,                         # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
                'score': similarity,                              # â­ ç»Ÿä¸€å­—æ®µåï¼šscore
                    })
            
            return similar_docs
    
    def search_by_doc(self, doc_id: str, top_k: int = 5, exclude_self: bool = True) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ–‡æ¡£IDæœç´¢ç›¸ä¼¼æ–‡æ¡£
        
        Args:
            doc_id: æ–‡æ¡£ID
            top_k: è¿”å›ç»“æœæ•°é‡
            exclude_self: æ˜¯å¦æ’é™¤è‡ªå·±
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        # æŸ¥è¯¢æ–‡æ¡£
        query_result = self.collection.query(
            expr=f'doc_id == "{doc_id}"',
            output_fields=["embedding"]
        )
        
        if not query_result:
                return []
            
        # è·å–å‘é‡
        query_embedding = query_result[0]['embedding']
        
        # æœç´¢ï¼ˆefè®¾ç½®ä¸º512ä¿è¯è¶³å¤Ÿå¤§ï¼‰
        search_params = {"metric_type": "L2", "params": {"ef": 512}}
        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=top_k + (1 if exclude_self else 0),
            output_fields=["doc_id", "title", "timestamp", "industries"]
        )
            
            # æ ¼å¼åŒ–ç»“æœ
        similar_docs = []
        for hit in results[0]:
                    # æ’é™¤è‡ªå·±
            if exclude_self and hit.entity.get('doc_id') == doc_id:
                        continue
                    
            similar_docs.append({
                'doc_id': hit.entity.get('doc_id'),
                'title': hit.entity.get('title'),
                'timestamp': hit.entity.get('timestamp'),         # â­ æ ¸å¿ƒ
                'industries': hit.entity.get('industries'),       # â­ æ ¸å¿ƒ
                'distance': hit.distance,
                'similarity': 1 / (1 + hit.distance),
                    })
            
            return similar_docs[:top_k]
            
    def search_with_dual_layer(
        self,
        query_text: str = None,
        query_segment: PolicySegment = None,
        query_timestamp: str = None,
        top_k_docs: int = 200,  # â­ å¢åŠ åˆ°200ï¼Œæé«˜å¬å›ç‡
        top_k_chunks: int = 50,  # â­ å¢åŠ åˆ°50ä¸ªchunk
        enable_time_filter: bool = True,
        enable_time_weighting: bool = True,
        enable_industry_boost: bool = True  # â­ å¯ç”¨è¡Œä¸šåŒ¹é…åŠ æƒ
    ) -> Dict[str, Any]:
        """
        åŒå±‚æ£€ç´¢ï¼šæ–‡æ¡£çº§ç²—æ’ + Chunkçº§ç²¾æ’
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_segment: æŸ¥è¯¢æ–‡æ¡£
            query_timestamp: æŸ¥è¯¢æ—¶é—´æˆ³ï¼ˆç”¨äºæ—¶é—´è¿‡æ»¤ï¼‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DD
            top_k_docs: æ–‡æ¡£çº§è¿”å›æ•°é‡
            top_k_chunks: Chunkçº§è¿”å›æ•°é‡
            enable_time_filter: æ˜¯å¦å¯ç”¨æ—¶é—´è¿‡æ»¤ï¼ˆåªæ£€ç´¢å†å²æ–‡æ¡£ï¼‰
            enable_time_weighting: æ˜¯å¦å¯ç”¨æ—¶é—´åŠ æƒ
            
        Returns:
            {'documents': [...], 'chunks': [...], 'query_info': {...}}
        """
        if not self.enable_chunking:
            # å¦‚æœæœªå¯ç”¨chunkingï¼Œåªè¿”å›æ–‡æ¡£çº§æ£€ç´¢
            docs = self.search_similar(
                query_text=query_text,
                query_segment=query_segment,
                top_k=top_k_docs
            )
            return {'documents': docs, 'chunks': [], 'query_info': {'chunking_enabled': False}}
        
        # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬ï¼ˆä¼˜åŒ–ï¼šæå–å®è´¨å†…å®¹ï¼‰
        if query_segment:
            # æ ‡é¢˜æƒé‡ï¼šé‡å¤3æ¬¡
            title_part = f"{query_segment.title}\n{query_segment.title}\n{query_segment.title}"
            
            # æå–å®è´¨å†…å®¹ï¼ˆè·³è¿‡æ ¼å¼éƒ¨åˆ†ï¼‰
            content = query_segment.content
            start_pos = 200
            for marker in ['ç¬¬ä¸€æ¡', 'ä¸€ã€', 'ï¼ˆä¸€ï¼‰', '1.', 'æ€»åˆ™', 'ç¬¬ä¸€ç« ']:
                pos = content.find(marker)
                if pos > 0 and pos < 500:
                    start_pos = pos
                    break
            
            content_part = content[start_pos:start_pos+1000]
            query_text = f"{title_part}\n\n{content_part}"
            
            if not query_timestamp:
                query_timestamp = query_segment.timestamp.isoformat()
        
        if not query_text:
            return {'documents': [], 'chunks': [], 'query_info': {'error': 'No query text'}}
        
        print(f"[MilvusVectorDB] [åŒå±‚æ£€ç´¢] å¼€å§‹...")
        print(f"[MilvusVectorDB]   - æŸ¥è¯¢: {query_text[:50]}...")
        if query_timestamp:
            print(f"[MilvusVectorDB]   - æ—¶é—´: {query_timestamp}")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        query_embedding = self.model.encode(
            [query_text],
            device=device,
            convert_to_numpy=True,
            normalize_embeddings=True  # â­ å½’ä¸€åŒ–
        )[0]
        
        # === æ­¥éª¤1: æ–‡æ¡£çº§æ£€ç´¢ï¼ˆç²—æ’ï¼‰ ===
        print(f"[MilvusVectorDB] [æ­¥éª¤1] æ–‡æ¡£çº§æ£€ç´¢ï¼ˆç²—æ’ï¼‰...")
        
        # æ„å»ºæ—¶é—´è¿‡æ»¤è¡¨è¾¾å¼
        expr = None
        if enable_time_filter and query_timestamp:
            expr = f'timestamp < "{query_timestamp}"'
            print(f"[MilvusVectorDB]   - æ—¶é—´è¿‡æ»¤: {expr}")
        
        # â­ efå¿…é¡»>=top_k_docsï¼ˆ200ï¼‰ï¼Œè®¾ç½®ä¸º512ä¿è¯è¶³å¤Ÿ
        search_params = {"metric_type": "L2", "params": {"ef": 512}}
        doc_results = self.collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param=search_params,
            limit=top_k_docs,
            expr=expr,  # æ—¶é—´è¿‡æ»¤è¡¨è¾¾å¼
            output_fields=["doc_id", "title", "timestamp", "industries"]
        )
        
        # æ ¼å¼åŒ–æ–‡æ¡£çº§ç»“æœ
        documents = []
        doc_ids = []
        for hit in doc_results[0]:
            doc_id = hit.entity.get('doc_id')
            doc_ids.append(doc_id)
            doc_timestamp = hit.entity.get('timestamp')
            
            # è®¡ç®—æ—¶é—´æƒé‡
            time_weight = 1.0
            time_diff_days = None
            if enable_time_weighting and query_timestamp and doc_timestamp:
                try:
                    query_date = datetime.fromisoformat(query_timestamp)
                    doc_date = datetime.fromisoformat(doc_timestamp)
                    time_diff_days = (query_date - doc_date).days
                    
                    if time_diff_days <= 90:
                        time_weight = 1.2
                    elif time_diff_days <= 180:
                        time_weight = 1.0
                    elif time_diff_days <= 365:
                        time_weight = 0.8
                    else:
                        time_weight = 0.6
                except:
                    pass
            
            similarity = 1 / (1 + hit.distance)
            weighted_score = similarity * time_weight
            
            documents.append({
                'doc_id': doc_id,
                'title': hit.entity.get('title'),
                'timestamp': doc_timestamp,                       # â­ æ ¸å¿ƒï¼šæ—¶é—´è·¨åº¦è®¡ç®—
                'industries': hit.entity.get('industries'),       # â­ æ ¸å¿ƒï¼šè¡Œä¸šè¿‡æ»¤
                'distance': hit.distance,
                'similarity': similarity,
                'time_weight': time_weight,
                'weighted_score': weighted_score,
                'time_diff_days': time_diff_days,
            })
        
        # æŒ‰åŠ æƒå¾—åˆ†é‡æ–°æ’åº
        if enable_time_weighting:
            documents.sort(key=lambda x: x['weighted_score'], reverse=True)
        
        print(f"[MilvusVectorDB] [æ­¥éª¤1] âœ… æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
        
        # === æ­¥éª¤2: Chunkçº§æ£€ç´¢ï¼ˆç²¾æ’ï¼‰ ===
        print(f"[MilvusVectorDB] [æ­¥éª¤2] Chunkçº§æ£€ç´¢ï¼ˆç²¾æ’ï¼‰...")
        
        if not doc_ids:
            return {
                'documents': documents,
                'chunks': [],
                'query_info': {
                    'query_text': query_text[:100],
                    'query_timestamp': query_timestamp,
                    'time_filter_enabled': enable_time_filter,
                    'time_weighting_enabled': enable_time_weighting,
                }
            }
        
        # æ„å»ºChunkè¿‡æ»¤è¡¨è¾¾å¼
        doc_ids_str = '", "'.join(doc_ids)
        chunk_expr = f'doc_id in ["{doc_ids_str}"]'
        
        if enable_time_filter and query_timestamp:
            chunk_expr += f' && timestamp < "{query_timestamp}"'
        
        print(f"[MilvusVectorDB]   - è¿‡æ»¤æ¡ä»¶: doc_id in top-{len(doc_ids)}")
        
        # æ‰§è¡ŒChunkçº§æ£€ç´¢
        chunk_results = self.chunk_collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param=search_params,
            limit=top_k_chunks,
            expr=chunk_expr,
            output_fields=["chunk_id", "doc_id", "content", "chunk_index", "chunk_type", "timestamp", "industries", "investment_relevance", "report_series", "industry_policy_segments"]
        )
        
        # æ ¼å¼åŒ–Chunkçº§ç»“æœ
        chunks = []
        for hit in chunk_results[0]:
            chunks.append({
                'chunk_id': hit.entity.get('chunk_id'),
                'doc_id': hit.entity.get('doc_id'),
                'content': hit.entity.get('content'),
                'chunk_index': hit.entity.get('chunk_index'),
                'chunk_type': hit.entity.get('chunk_type'),
                'timestamp': hit.entity.get('timestamp'),         # â­ æ ¸å¿ƒ
                'industries': hit.entity.get('industries'),       # â­ æ ¸å¿ƒ
                'investment_relevance': hit.entity.get('investment_relevance'),
                'report_series': hit.entity.get('report_series'),  # â­ æŠ¥å‘Šç³»åˆ—
                'industry_policy_segments': hit.entity.get('industry_policy_segments'),
                'distance': hit.distance,
                'similarity': 1 / (1 + hit.distance),
            })
        
        print(f"[MilvusVectorDB] [æ­¥éª¤2] âœ… æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³æ®µè½")
        print(f"[MilvusVectorDB] [åŒå±‚æ£€ç´¢] âœ… å®Œæˆ")
        
        return {
            'documents': documents[:top_k_docs],
            'chunks': chunks[:top_k_chunks],
            'query_info': {
                'query_text': query_text[:100],
                'query_timestamp': query_timestamp,
                'time_filter_enabled': enable_time_filter,
                'time_weighting_enabled': enable_time_weighting,
                'total_docs_found': len(documents),
                'total_chunks_found': len(chunks),
            }
        }
    
    def clear(self):
        """æ¸…ç©ºMilvuså‘é‡åº“ï¼ˆåŒ…æ‹¬æ–‡æ¡£çº§å’ŒChunkçº§ï¼‰"""
        try:
            # æ¸…ç©ºæ–‡æ¡£çº§é›†åˆ
            utility.drop_collection(self.collection_name)
            print(f"[MilvusVectorDB] âœ… æ–‡æ¡£çº§é›†åˆ {self.collection_name} å·²åˆ é™¤")
            
            # æ¸…ç©ºChunkçº§é›†åˆ
            if self.enable_chunking and utility.has_collection(self.chunk_collection_name):
                utility.drop_collection(self.chunk_collection_name)
                print(f"[MilvusVectorDB] âœ… Chunkçº§é›†åˆ {self.chunk_collection_name} å·²åˆ é™¤")
            
            # é‡æ–°åˆ›å»º
            self._init_collection()
            if self.enable_chunking:
                self._init_chunk_collection()
                
            print(f"[MilvusVectorDB] âœ… é›†åˆå·²é‡æ–°åˆ›å»º")
        except Exception as e:
            print(f"[MilvusVectorDB] âŒ æ¸…ç©ºå¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªä½¿ç”¨chunkçº§åˆ«ï¼‰"""
        stats = {
            'total_documents': 0,  # ç®€åŒ–ç‰ˆï¼šæ²¡æœ‰æ–‡æ¡£çº§æ•°æ®
            'collection_name': self.chunk_collection_name,
            'embedding_dim': self.embedding_dim,
            'gpu_enabled': torch.cuda.is_available(),
            'gpu_device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'chunking_enabled': self.enable_chunking,
            'status': 'è¿è¡Œä¸­ï¼ˆç®€åŒ–ç‰ˆï¼šåªä½¿ç”¨chunkçº§åˆ«ï¼‰'
        }
        
        if self.enable_chunking:
            stats['total_chunks'] = self.chunk_collection.num_entities
            stats['chunk_collection_name'] = self.chunk_collection_name
            stats['avg_chunks_per_doc'] = 0  # ç®€åŒ–ç‰ˆï¼šæ— æ³•è®¡ç®—å¹³å‡å€¼
        
        return stats
    
    def search_chunks(self, query_text: str, top_k: int = 500, rerank_top_k: int = None, exclude_doc_id: str = None, exclude_title: str = None, exclude_timestamp = None, before_timestamp = None, after_timestamp = None, allow_same_day: bool = False, use_reranker: bool = True) -> List[Dict[str, Any]]:
        """
        æœç´¢chunkçº§åˆ«å‘é‡ï¼ˆç®€åŒ–ç‰ˆRAGï¼‰+ å¯é€‰Rerankingç²¾æ’
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: ç²—æ’å¬å›æ•°é‡ï¼ˆé»˜è®¤500ï¼‰
            rerank_top_k: ç²¾æ’è¿”å›æ•°é‡ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨top_kï¼‰
            exclude_doc_id: è¦æ’é™¤çš„doc_idï¼ˆé¿å…åŒ¹é…åˆ°è‡ªå·±ï¼‰
            exclude_title: è¦æ’é™¤çš„æ ‡é¢˜ï¼ˆä¸exclude_timestampé…åˆä½¿ç”¨ï¼Œè¿‡æ»¤æ ‡é¢˜+æ—¶é—´éƒ½ç›¸åŒçš„æ–‡æ¡£ï¼‰
            exclude_timestamp: è¦æ’é™¤çš„å‘æ–‡æ—¶é—´ï¼ˆä¸exclude_titleé…åˆä½¿ç”¨ï¼‰
            before_timestamp: æ—¶é—´çº¦æŸï¼Œåªæ£€ç´¢æ—©äºæ­¤æ—¶é—´çš„æ–‡æ¡£ï¼ˆç²—æ’é˜¶æ®µè¿‡æ»¤ï¼‰
            after_timestamp: æ—¶é—´çº¦æŸï¼Œåªæ£€ç´¢æ™šäºæ­¤æ—¶é—´çš„æ–‡æ¡£ï¼ˆç”¨äºé™åˆ¶æ—¶é—´çª—å£ï¼Œå¦‚åªæ£€ç´¢2å¹´å†…çš„æ”¿ç­–ï¼‰
            allow_same_day: æ˜¯å¦å…è®¸åŒä¸€å¤©çš„æ–‡æ¡£ï¼ˆé»˜è®¤Falseï¼Œä¸¥æ ¼æ—©äºï¼‰
            use_reranker: æ˜¯å¦ä½¿ç”¨RerankeräºŒé˜¶æ®µç²¾æ’ï¼ˆé»˜è®¤Trueï¼Œæå‡ç²¾åº¦ï¼‰
            
        Returns:
            chunkæœç´¢ç»“æœåˆ—è¡¨ï¼ˆå¦‚æœå¯ç”¨rerankerï¼Œä¼šæ·»åŠ 'rerank_score'å­—æ®µï¼‰
        """
        if not self.enable_chunking:
            print(f"[MilvusVectorDB] âš ï¸ Chunkæœç´¢æœªå¯ç”¨")
            return []
        
        # â­ ç²—æ’å’Œç²¾æ’æ•°é‡è®¾ç½®
        if rerank_top_k is None:
            rerank_top_k = top_k
        
        retrieval_top_k = top_k  # ç²—æ’å¬å›æ•°é‡
        final_top_k = rerank_top_k if use_reranker else top_k  # ç²¾æ’åè¿”å›æ•°é‡
        
        print(f"[MilvusVectorDB] æœç´¢chunks: '{query_text[:50]}...'")
        if use_reranker:
            print(f"[MilvusVectorDB]   - ç²—æ’: å¬å›top-{retrieval_top_k} å€™é€‰")
            print(f"[MilvusVectorDB]   - ç²¾æ’: Rerankerç­›é€‰top-{final_top_k}")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        query_embedding = self.model.encode(
            [query_text],
            device=device,
            convert_to_numpy=True,
            normalize_embeddings=True
        )[0]
        
        # ç¡®ä¿é›†åˆå·²åŠ è½½ï¼ˆæœç´¢å‰å¿…é¡»åŠ è½½ï¼‰
        try:
            # å°è¯•æŸ¥è¯¢ï¼Œå¦‚æœèƒ½æŸ¥è¯¢è¯´æ˜å·²åŠ è½½
            self.chunk_collection.query(expr="id >= 0", limit=1, output_fields=["id"])
        except:
            # æœªåŠ è½½ï¼Œæ‰§è¡ŒåŠ è½½
            print(f"[MilvusVectorDB] [æœç´¢] ğŸ”„ é›†åˆæœªåŠ è½½ï¼Œæ­£åœ¨åŠ è½½...")
            try:
                import threading
                
                load_success = [False]
                load_error = [None]
                
                def load_in_thread():
                    try:
                        self.chunk_collection.load()
                        load_success[0] = True
                    except Exception as e:
                        load_error[0] = e
                
                load_thread = threading.Thread(target=load_in_thread, daemon=True)
                load_thread.start()
                load_thread.join(timeout=30)  # 30ç§’è¶…æ—¶
                
                if load_thread.is_alive():
                    print(f"[MilvusVectorDB] [æœç´¢] âš ï¸ åŠ è½½è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œå°è¯•ç»§ç»­æœç´¢ï¼ˆæ–°æ’å…¥çš„æ•°æ®å¯èƒ½åœ¨å†…å­˜ä¸­ï¼‰")
                    print(f"[MilvusVectorDB] [æœç´¢] ğŸ’¡ å¦‚æœæœç´¢å¤±è´¥ï¼Œå¯èƒ½æ˜¯MilvusæœåŠ¡å¼‚å¸¸")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå°è¯•ç»§ç»­æœç´¢ï¼ˆæ–°æ’å…¥çš„æ•°æ®å¯èƒ½åœ¨å†…å­˜ä¸­ï¼‰
                elif load_error[0]:
                    error_msg = str(load_error[0])
                    if "not loaded" in error_msg.lower() or "collection not loaded" in error_msg.lower():
                        print(f"[MilvusVectorDB] [æœç´¢] âš ï¸ é›†åˆåŠ è½½å¤±è´¥ï¼Œå°è¯•ç»§ç»­æœç´¢ï¼ˆæ–°æ’å…¥çš„æ•°æ®å¯èƒ½åœ¨å†…å­˜ä¸­ï¼‰")
                        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå°è¯•ç»§ç»­æœç´¢
                    else:
                        print(f"[MilvusVectorDB] [æœç´¢] âš ï¸ é›†åˆåŠ è½½å¤±è´¥: {load_error[0]}ï¼Œå°è¯•ç»§ç»­æœç´¢")
                        # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå°è¯•ç»§ç»­æœç´¢
                elif load_success[0]:
                    print(f"[MilvusVectorDB] [æœç´¢] âœ… é›†åˆå·²åŠ è½½")
            except Exception as load_error:
                print(f"[MilvusVectorDB] [æœç´¢] âš ï¸ é›†åˆåŠ è½½è¿‡ç¨‹å‡ºé”™: {load_error}")
                print(f"[MilvusVectorDB] [æœç´¢] ğŸ’¡ å°è¯•ç»§ç»­æœç´¢ï¼ˆæ–°æ’å…¥çš„æ•°æ®å¯èƒ½åœ¨å†…å­˜ä¸­ï¼‰")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå°è¯•ç»§ç»­æœç´¢
        
        # æœç´¢å‚æ•° - ä¼˜åŒ–ï¼šå¢åŠ nprobeæé«˜æœç´¢ç²¾åº¦
        search_params = {
            "metric_type": "L2",
            "params": {"nprobe": 20}  # ä»10å¢åŠ åˆ°20ï¼Œæé«˜æœç´¢ç²¾åº¦
        }
        
        # â­ æ„å»ºMilvusè¿‡æ»¤è¡¨è¾¾å¼ï¼ˆæ—¶é—´è¿‡æ»¤åœ¨æœ€å‰é¢ï¼ï¼‰
        # è¿™æ ·Milvusåªè¿”å›ç¬¦åˆæ—¶é—´æ¡ä»¶çš„ç»“æœï¼Œé¿å…æµªè´¹è®¡ç®—
        filter_expr_parts = []
        
        # before_timestamp: åªæ£€ç´¢æ—©äºæ­¤æ—¶é—´çš„æ–‡æ¡£
        if before_timestamp:
            try:
                from datetime import datetime
                if hasattr(before_timestamp, 'isoformat'):
                    ts_str = before_timestamp.isoformat()
                elif isinstance(before_timestamp, str):
                    ts_str = before_timestamp
                else:
                    ts_str = str(before_timestamp)
                
                # Milvuså­—ç¬¦ä¸²æ¯”è¾ƒï¼štimestamp < "2025-01-01"
                if allow_same_day:
                    # å…è®¸åŒä¸€å¤©ï¼štimestamp <= "2025-01-01"
                    filter_expr_parts.append(f'timestamp <= "{ts_str[:10]}"')
                else:
                    # ä¸¥æ ¼æ—©äºï¼štimestamp < "2025-01-01"
                    filter_expr_parts.append(f'timestamp < "{ts_str[:10]}"')
            except Exception as e:
                print(f"[MilvusVectorDB] âš ï¸ æ„å»ºbefore_timestampè¿‡æ»¤è¡¨è¾¾å¼å¤±è´¥: {e}")
        
        # after_timestamp: åªæ£€ç´¢æ™šäºæ­¤æ—¶é—´çš„æ–‡æ¡£ï¼ˆç”¨äºé™åˆ¶æ—¶é—´çª—å£ï¼‰
        if after_timestamp:
            try:
                from datetime import datetime
                if hasattr(after_timestamp, 'isoformat'):
                    after_ts_str = after_timestamp.isoformat()
                elif isinstance(after_timestamp, str):
                    after_ts_str = after_timestamp
                else:
                    after_ts_str = str(after_timestamp)
                
                # Milvuså­—ç¬¦ä¸²æ¯”è¾ƒï¼štimestamp >= "2023-01-01"
                filter_expr_parts.append(f'timestamp >= "{after_ts_str[:10]}"')
            except Exception as e:
                print(f"[MilvusVectorDB] âš ï¸ æ„å»ºafter_timestampè¿‡æ»¤è¡¨è¾¾å¼å¤±è´¥: {e}")
        
        # åˆå¹¶è¿‡æ»¤è¡¨è¾¾å¼
        filter_expr = ' && '.join(filter_expr_parts) if filter_expr_parts else None
        
        if filter_expr:
            print(f"[MilvusVectorDB] â­ Milvuså±‚é¢æ—¶é—´è¿‡æ»¤: {filter_expr}")
        
        # æ‰§è¡Œæœç´¢ï¼ˆå¸¦æ—¶é—´è¿‡æ»¤ï¼‰
        results = self.chunk_collection.search(
            data=[query_embedding.tolist()],
            anns_field="embedding",
            param=search_params,
            limit=retrieval_top_k,  # â­ ä½¿ç”¨è°ƒæ•´åçš„å¬å›æ•°é‡
            expr=filter_expr,  # â­ æ—¶é—´è¿‡æ»¤åœ¨Milvuså±‚é¢è¿›è¡Œï¼
            output_fields=["chunk_id", "doc_id", "content", "chunk_index", "chunk_type", "title", "timestamp", "industries", "investment_relevance", "report_series", "industry_policy_segments"]
        )
        
        # â­ é¢„å¤„ç†æ’é™¤æ¡ä»¶ï¼šæ ‡é¢˜+æ—¶é—´ï¼ˆç”¨äºæ’é™¤æ–°æ”¿ç­–è‡ªèº«ï¼‰
        exclude_timestamp_date = None
        if exclude_title and exclude_timestamp:
            try:
                from datetime import datetime
                if hasattr(exclude_timestamp, 'date'):
                    exclude_timestamp_date = exclude_timestamp.date()
                elif isinstance(exclude_timestamp, str):
                    if 'T' in exclude_timestamp:
                        exclude_timestamp_date = datetime.fromisoformat(exclude_timestamp.replace('Z', '+00:00')).date()
                    else:
                        exclude_timestamp_date = datetime.fromisoformat(exclude_timestamp).date()
            except:
                pass
        
        # æ ¼å¼åŒ–ç»“æœ
        # â­ æ³¨æ„ï¼šæ—¶é—´è¿‡æ»¤å·²åœ¨Milvuså±‚é¢å®Œæˆï¼ˆfilter_exprï¼‰ï¼Œè¿™é‡Œåªéœ€è¦æ’é™¤è‡ªèº«
        formatted_results = []
        excluded_by_title_time = 0
        for hits in results:
            for hit in hits:
                doc_id = hit.entity.get('doc_id')
                hit_timestamp = hit.entity.get('timestamp', '')
                
                # è¿‡æ»¤æ‰exclude_doc_id
                if exclude_doc_id and doc_id == exclude_doc_id:
                    continue
                
                # â­ è¿‡æ»¤æ‰æ ‡é¢˜+å‘æ–‡æ—¶é—´éƒ½ç›¸åŒçš„æ–‡æ¡£ï¼ˆæ’é™¤æ–°æ”¿ç­–è‡ªèº«ï¼‰
                if exclude_title and exclude_timestamp_date:
                    hit_title = hit.entity.get('title', '')
                    if hit_title == exclude_title:
                        try:
                            from datetime import datetime
                            if hit_timestamp:
                                if 'T' in str(hit_timestamp):
                                    hit_date = datetime.fromisoformat(str(hit_timestamp).replace('Z', '+00:00')).date()
                                else:
                                    hit_date = datetime.fromisoformat(str(hit_timestamp)).date()
                                if hit_date == exclude_timestamp_date:
                                    excluded_by_title_time += 1
                                    continue  # æ ‡é¢˜+æ—¶é—´éƒ½ç›¸åŒï¼Œè·³è¿‡ï¼ˆæ˜¯æ–°æ”¿ç­–è‡ªèº«ï¼‰
                        except:
                            pass
                
                # â­ å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼šL2è·ç¦»èŒƒå›´[0, 2]ï¼Œè½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦[0, 1]
                l2_distance = hit.distance
                cosine_similarity = 1.0 - (l2_distance ** 2) / 2.0
                similarity = max(0.0, min(1.0, cosine_similarity))  # ç¡®ä¿åœ¨[0, 1]èŒƒå›´å†…
                
                formatted_results.append({
                    'chunk_id': hit.entity.get('chunk_id'),
                    'doc_id': doc_id,
                    'content': hit.entity.get('content'),
                    'similarity': similarity,  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼ï¼‰
                    'chunk_index': hit.entity.get('chunk_index'),
                    'chunk_type': hit.entity.get('chunk_type'),
                    'title': hit.entity.get('title'),
                    'timestamp': hit_timestamp,
                    'industries': hit.entity.get('industries')
                })
        
        exclude_msg = f"ï¼ˆå·²æ’é™¤doc_id={exclude_doc_id}ï¼‰" if exclude_doc_id else ""
        if excluded_by_title_time > 0:
            exclude_msg += f"ï¼ˆå·²æ’é™¤{excluded_by_title_time}ä¸ªæ ‡é¢˜+æ—¶é—´ç›¸åŒçš„chunksï¼‰"
        time_filter_msg = f"ï¼ˆæ—¶é—´è¿‡æ»¤å·²åœ¨Milvuså±‚é¢å®Œæˆï¼‰" if filter_expr else ""
        print(f"[MilvusVectorDB] âœ… ç²—æ’å®Œæˆ: {len(formatted_results)} ä¸ªç›¸å…³chunks{exclude_msg}{time_filter_msg}")
        
        # â­ äºŒé˜¶æ®µç²¾æ’ï¼ˆRerankingï¼‰
        if use_reranker and len(formatted_results) > final_top_k:
            try:
                from utils.reranker import get_reranker
                reranker = get_reranker()
                formatted_results = reranker.rerank(query_text, formatted_results, top_k=final_top_k)
            except Exception as e:
                print(f"[MilvusVectorDB] âš ï¸ Rerankingå¤±è´¥: {e}ï¼Œè¿”å›å‘é‡æ£€ç´¢ç»“æœ")
                formatted_results = formatted_results[:final_top_k]
        else:
            formatted_results = formatted_results[:final_top_k]
        
        return formatted_results
    
    def search_chunks_multi_query(self, query_chunks: List[str], top_k_per_query: int = 10, exclude_doc_id: str = None, use_reranker: bool = True, final_top_k: int = None) -> List[Dict[str, Any]]:
        """
        ç²¾ç»†åŒ–æœç´¢ï¼šå¯¹å¤šä¸ªquery chunksåˆ†åˆ«è¿›è¡Œå‘é‡æœç´¢ï¼Œç„¶ååˆå¹¶ç»“æœ + å¯é€‰å…¨å±€Reranking
        
        è¿™ç§æ–¹æ³•ç¡®ä¿ï¼š
        1. æ¯ä¸ªquery chunkéƒ½èƒ½æ‰¾åˆ°æœ€åŒ¹é…çš„æ•°æ®åº“chunks
        2. åŒ¹é…æ›´ç²¾ç¡®ï¼Œå› ä¸ºç²’åº¦ä¸€è‡´ï¼ˆchunk vs chunkï¼‰
        3. é¿å…äº†é•¿æ–‡æ¡£æŸ¥è¯¢æ—¶çš„è¯­ä¹‰ç¨€é‡Šé—®é¢˜
        4. å…¨å±€Rerankingï¼šåˆå¹¶åå¯¹æ‰€æœ‰å€™é€‰ç»Ÿä¸€ç²¾æ’
        
        Args:
            query_chunks: æŸ¥è¯¢chunksåˆ—è¡¨ï¼ˆå·²åˆ‡åˆ†çš„æŸ¥è¯¢æ–‡æœ¬ï¼‰
            top_k_per_query: æ¯ä¸ªquery chunkè¿”å›çš„æœ€ç›¸ä¼¼chunkæ•°é‡
            exclude_doc_id: è¦æ’é™¤çš„doc_idï¼ˆé¿å…åŒ¹é…åˆ°è‡ªå·±ï¼‰
            use_reranker: æ˜¯å¦å¯¹åˆå¹¶åçš„ç»“æœè¿›è¡Œå…¨å±€Reranking
            final_top_k: æœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è¿”å›æ‰€æœ‰åˆå¹¶åçš„ç»“æœï¼‰
            
        Returns:
            åˆå¹¶åçš„chunkæœç´¢ç»“æœåˆ—è¡¨ï¼ˆå·²å»é‡ï¼ŒæŒ‰ç›¸ä¼¼åº¦æˆ–rerank_scoreæ’åºï¼‰
        """
        if not self.enable_chunking:
            print(f"[MilvusVectorDB] âš ï¸ Chunkæœç´¢æœªå¯ç”¨")
            return []
        
        if not query_chunks:
            return []
        
        print(f"[MilvusVectorDB] ç²¾ç»†åŒ–æœç´¢: {len(query_chunks)} ä¸ªquery chunksï¼Œæ¯ä¸ªè¿”å›top_{top_k_per_query}")
        
        # æ‰¹é‡ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆGPUåŠ é€Ÿï¼‰
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        query_embeddings = self.model.encode(
            query_chunks,
            device=device,
            convert_to_numpy=True,
            normalize_embeddings=True,
            batch_size=32,
            show_progress_bar=False
        )
        
        # æœç´¢å‚æ•°
        search_params = {
            "metric_type": "L2",
            "params": {"nprobe": 20}
        }
        
        # å¯¹æ¯ä¸ªquery chunkæ‰§è¡Œæœç´¢
        all_results = []
        chunk_id_set = set()  # ç”¨äºå»é‡
        
        for i, query_embedding in enumerate(query_embeddings):
            results = self.chunk_collection.search(
                data=[query_embedding.tolist()],
                anns_field="embedding",
                param=search_params,
                limit=top_k_per_query,
                output_fields=["chunk_id", "doc_id", "content", "chunk_index", "chunk_type", "title", "timestamp", "industries", "investment_relevance", "report_series", "industry_policy_segments"]
            )
            
            for hits in results:
                for hit in hits:
                    chunk_id = hit.entity.get('chunk_id')
                    doc_id = hit.entity.get('doc_id')
                    
                    # è¿‡æ»¤æ‰exclude_doc_id
                    if exclude_doc_id and doc_id == exclude_doc_id:
                        continue
                    
                    # å»é‡ï¼šå¦‚æœåŒä¸€ä¸ªchunkè¢«å¤šä¸ªquery chunkåŒ¹é…åˆ°ï¼Œä¿ç•™ç›¸ä¼¼åº¦æ›´é«˜çš„
                    # â­ å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼šL2è·ç¦»èŒƒå›´[0, 2]ï¼Œè½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦[0, 1]
                    l2_distance = hit.distance
                    cosine_similarity = 1.0 - (l2_distance ** 2) / 2.0
                    similarity = max(0.0, min(1.0, cosine_similarity))  # ç¡®ä¿åœ¨[0, 1]èŒƒå›´å†…
                    
                    if chunk_id not in chunk_id_set:
                        chunk_id_set.add(chunk_id)
                        all_results.append({
                            'chunk_id': chunk_id,
                            'doc_id': doc_id,
                            'content': hit.entity.get('content'),
                            'similarity': similarity,
                            'chunk_index': hit.entity.get('chunk_index'),
                            'chunk_type': hit.entity.get('chunk_type'),
                            'title': hit.entity.get('title'),
                            'timestamp': hit.entity.get('timestamp'),
                            'industries': hit.entity.get('industries'),
                            'investment_relevance': hit.entity.get('investment_relevance'),
                            'report_series': hit.entity.get('report_series'),  # â­ æŠ¥å‘Šç³»åˆ—
                            'industry_policy_segments': hit.entity.get('industry_policy_segments'),
                            'matched_by_query_chunk': i  # è®°å½•æ˜¯å“ªä¸ªquery chunkåŒ¹é…åˆ°çš„
                        })
                    else:
                        # å¦‚æœå·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç›¸ä¼¼åº¦ï¼ˆä¿ç•™æ›´é«˜çš„ï¼‰
                        for existing in all_results:
                            if existing['chunk_id'] == chunk_id and similarity > existing['similarity']:
                                existing['similarity'] = similarity
                                existing['matched_by_query_chunk'] = i
                                break
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_results.sort(key=lambda x: x['similarity'], reverse=True)
        
        exclude_msg = f"ï¼ˆå·²æ’é™¤doc_id={exclude_doc_id}ï¼‰" if exclude_doc_id else ""
        print(f"[MilvusVectorDB] âœ… ç²¾ç»†åŒ–æœç´¢å®Œæˆ: åˆå¹¶åæ‰¾åˆ° {len(all_results)} ä¸ªç›¸å…³chunksï¼ˆå·²å»é‡ï¼‰{exclude_msg}")
        
        # â­ å…¨å±€Rerankingï¼šç”¨å®Œæ•´queryå¯¹åˆå¹¶åçš„ç»“æœè¿›è¡Œç»Ÿä¸€ç²¾æ’
        if use_reranker and final_top_k and len(all_results) > final_top_k:
            try:
                # å°†å¤šä¸ªquery chunksåˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´query
                full_query = "\n".join(query_chunks)
                
                from utils.reranker import get_reranker
                reranker = get_reranker()
                print(f"[MilvusVectorDB] ğŸ”„ å¯¹åˆå¹¶åçš„ {len(all_results)} ä¸ªå€™é€‰è¿›è¡Œå…¨å±€Reranking...")
                all_results = reranker.rerank(full_query, all_results, top_k=final_top_k)
            except Exception as e:
                print(f"[MilvusVectorDB] âš ï¸ å…¨å±€Rerankingå¤±è´¥: {e}ï¼Œè¿”å›å‘é‡æ£€ç´¢ç»“æœ")
                all_results = all_results[:final_top_k] if final_top_k else all_results
        elif final_top_k:
            all_results = all_results[:final_top_k]
        
        return all_results
    
    def query_by_report_series(self, report_series: str, exclude_doc_id: str = None, limit: int = 10) -> List[Dict[str, Any]]:
        """
        æŒ‰æŠ¥å‘Šç³»åˆ—æŸ¥è¯¢å†å²æ”¿ç­–ï¼ˆä¸ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼Œç›´æ¥æŒ‰æ ‡ç­¾æŸ¥è¯¢ï¼‰
        
        Args:
            report_series: æŠ¥å‘Šç³»åˆ—æ ‡ç­¾ï¼ˆå¦‚"äº”å¹´è§„åˆ’-å»ºè®®"ã€"ä¸­å¤®ç»æµå·¥ä½œä¼šè®®"ç­‰ï¼‰
            exclude_doc_id: è¦æ’é™¤çš„doc_idï¼ˆé¿å…åŒ¹é…åˆ°è‡ªå·±ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            åŒç³»åˆ—å†å²æ”¿ç­–åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        """
        if not report_series or report_series == 'N/A':
            print(f"[MilvusVectorDB] âš ï¸ æŠ¥å‘Šç³»åˆ—ä¸ºç©ºæˆ–ä¸º'N/A'ï¼Œè·³è¿‡æŸ¥è¯¢")
            return []
        
        print(f"[MilvusVectorDB] ğŸ” æŒ‰æŠ¥å‘Šç³»åˆ—æŸ¥è¯¢: {report_series}")
        
        # æ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼
        expr = f'report_series == "{report_series}"'
        if exclude_doc_id:
            expr += f' && doc_id != "{exclude_doc_id}"'
        
        # æŸ¥è¯¢å­—æ®µ
        output_fields = [
            "chunk_id", "doc_id", "content", "chunk_index", "chunk_type",
            "title", "timestamp", "industries", "investment_relevance", 
            "report_series", "industry_policy_segments"
        ]
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨å‘é‡æœç´¢ï¼Œç›´æ¥æŒ‰å­—æ®µæŸ¥è¯¢ï¼‰
            results = self.chunk_collection.query(
                expr=expr,
                output_fields=output_fields,
                limit=limit * 10  # æŸ¥è¯¢æ›´å¤šchunksï¼Œç„¶åæŒ‰doc_idåˆå¹¶
            )
            
            if not results:
                print(f"[MilvusVectorDB] âš ï¸ æœªæ‰¾åˆ°æŠ¥å‘Šç³»åˆ—ä¸º'{report_series}'çš„å†å²æ”¿ç­–")
                return []
            
            print(f"[MilvusVectorDB] âœ… æ‰¾åˆ° {len(results)} ä¸ªchunksï¼ˆæŠ¥å‘Šç³»åˆ—: {report_series}ï¼‰")
            
            # æŒ‰doc_idåˆå¹¶chunks
            docs_by_id = {}
            for chunk in results:
                doc_id = chunk.get('doc_id')
                if not doc_id:
                    continue
                
                if doc_id not in docs_by_id:
                    docs_by_id[doc_id] = {
                        'doc_id': doc_id,
                        'title': chunk.get('title', ''),
                        'timestamp': chunk.get('timestamp', ''),
                        'industries': chunk.get('industries', ''),
                        'investment_relevance': chunk.get('investment_relevance', ''),
                        'report_series': chunk.get('report_series', report_series),
                        'chunks': [],
                        'content': ''
                    }
                
                # æ·»åŠ chunkå†…å®¹
                content = chunk.get('content', '')
                if content:
                    docs_by_id[doc_id]['chunks'].append({
                        'chunk_id': chunk.get('chunk_id'),
                        'content': content,
                        'chunk_index': chunk.get('chunk_index', 0)
                    })
                    docs_by_id[doc_id]['content'] += f"{content}\n\n"
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æ—¶é—´æ’åº
            doc_list = []
            for doc_id, doc_data in docs_by_id.items():
                # æ¸…ç†å†…å®¹
                doc_data['content'] = doc_data['content'].strip()
                # æŒ‰chunk_indexæ’åºchunks
                doc_data['chunks'].sort(key=lambda x: x.get('chunk_index', 0))
                doc_list.append(doc_data)
            
            # æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼Œæ–¹ä¾¿æ—¶é—´å¯¹æ¯”åˆ†æï¼‰
            from datetime import datetime as dt_class
            def parse_timestamp(ts):
                if not ts:
                    return None
                try:
                    # å°è¯•è§£æISOæ ¼å¼
                    if 'T' in str(ts):
                        return dt_class.fromisoformat(str(ts).replace('Z', '+00:00'))
                    else:
                        return dt_class.strptime(str(ts), '%Y-%m-%d')
                except:
                    return None
            
            doc_list.sort(key=lambda x: parse_timestamp(x.get('timestamp')) or dt_class.min, reverse=False)  # ä»æ—§åˆ°æ–°
            
            # é™åˆ¶è¿”å›æ•°é‡
            result_list = doc_list[:limit]
            
            print(f"[MilvusVectorDB] âœ… æŒ‰æŠ¥å‘Šç³»åˆ—æŸ¥è¯¢å®Œæˆ: æ‰¾åˆ° {len(result_list)} ä¸ªåŒç³»åˆ—å†å²æ”¿ç­–ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰")
            
            return result_list
            
        except Exception as e:
            print(f"[MilvusVectorDB] âŒ æŒ‰æŠ¥å‘Šç³»åˆ—æŸ¥è¯¢å¤±è´¥: {e}")
            return []


    def get_full_document_content(self, doc_id: str = None, title: str = None, timestamp: str = None) -> str:
        """
        æ ¹æ®doc_idæˆ–(title, timestamp)è·å–è¯¥æ–‡æ¡£çš„å®Œæ•´å†…å®¹ï¼ˆåˆå¹¶æ‰€æœ‰chunksï¼‰
        
        â­ ç”¨é€”ï¼šRAGæ£€ç´¢åï¼Œå›åº“æ‹‰å–å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œè§£å†³"æ£€ç´¢åˆ°çš„å†…å®¹å¤ªçŸ­"é—®é¢˜
        
        Args:
            doc_id: æ–‡æ¡£IDï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            title: æ–‡æ¡£æ ‡é¢˜ï¼ˆä¸timestampé…åˆä½¿ç”¨ï¼‰
            timestamp: æ–‡æ¡£æ—¶é—´æˆ³ï¼ˆä¸titleé…åˆä½¿ç”¨ï¼‰
            
        Returns:
            åˆå¹¶åçš„å®Œæ•´æ–‡æ¡£å†…å®¹
        """
        if not doc_id and not (title and timestamp):
            return ""
        
        try:
            # æ„å»ºæŸ¥è¯¢è¡¨è¾¾å¼
            if doc_id:
                expr = f'doc_id == "{doc_id}"'
            else:
                # ä½¿ç”¨ title + timestamp æŸ¥è¯¢
                expr = f'title == "{title}" && timestamp == "{timestamp}"'
            
            # æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰chunks
            results = self.chunk_collection.query(
                expr=expr,
                output_fields=["chunk_id", "content", "chunk_index"],
                limit=100  # ä¸€ä¸ªæ–‡æ¡£æœ€å¤š100ä¸ªchunks
            )
            
            if not results:
                return ""
            
            # æŒ‰chunk_indexæ’åº
            results.sort(key=lambda x: x.get('chunk_index', 0))
            
            # åˆå¹¶æ‰€æœ‰chunksçš„å†…å®¹
            contents = [r.get('content', '') for r in results if r.get('content')]
            full_content = '\n\n'.join(contents)
            
            return full_content
            
        except Exception as e:
            print(f"[MilvusVectorDB] âš ï¸ è·å–å®Œæ•´æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return ""
    
    def get_documents_full_content(self, doc_ids: list = None, title_timestamp_pairs: list = None) -> Dict[str, str]:
        """
        æ‰¹é‡è·å–å¤šä¸ªæ–‡æ¡£çš„å®Œæ•´å†…å®¹
        
        Args:
            doc_ids: æ–‡æ¡£IDåˆ—è¡¨
            title_timestamp_pairs: (title, timestamp) å…ƒç»„åˆ—è¡¨
            
        Returns:
            {doc_idæˆ–"title|timestamp": å®Œæ•´å†…å®¹} å­—å…¸
        """
        result = {}
        
        if doc_ids:
            for doc_id in doc_ids:
                content = self.get_full_document_content(doc_id=doc_id)
                if content:
                    result[doc_id] = content
        
        if title_timestamp_pairs:
            for title, timestamp in title_timestamp_pairs:
                content = self.get_full_document_content(title=title, timestamp=timestamp)
                if content:
                    result[f"{title}|{timestamp}"] = content
        
        return result


def get_vector_db() -> MilvusVectorDatabase:
    """è·å–Milvuså‘é‡æ•°æ®åº“å®ä¾‹"""
    return MilvusVectorDatabase(chunk_only=True)  # ç¡®ä¿åªä½¿ç”¨chunkçº§åˆ«



