"""
Novelty Agent - å¢é‡åˆ†æï¼ˆæ”¿ç­–æ–°æ—§å¯¹æ¯”ï¼‰
åˆ†ä¸»é¢˜RAGæ£€ç´¢ + LLMå¯¹æ¯”åˆ†æ
"""
from typing import List, Dict, Any
from datetime import datetime, timedelta
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from .base import BaseAgent
from models import PolicySegment
from core.clients.volcengine_client import get_volcengine_client


class NoveltyAgent(BaseAgent):
    """å¢é‡åˆ†æAgent - åˆ†ä¸»é¢˜RAG + LLMç›¸å…³æ€§è¯„åˆ† + Rerankerç²¾æ’"""
    
    MAX_DOCS = 50  # æœ€å¤§è¿”å›æ–‡æ¡£æ•°
    
    def __init__(self, vector_db=None):
        super().__init__("NoveltyAgent")
        self.vector_db = vector_db
        self.llm_client = get_volcengine_client()
        self.log("âœ… NoveltyAgentåˆå§‹åŒ–å®Œæˆ")
    
    def process(self, input_data: Any) -> Dict[str, Any]:
        """
        å¤„ç†æ•°æ®çš„ä¸»æ–¹æ³•
        
        Args:
            input_data: PolicySegmentå¯¹è±¡
            
        Returns:
            åŒ…å«analysisã€topicsã€topic_rag_resultsçš„å­—å…¸
        """
        if isinstance(input_data, PolicySegment):
            return self.analyze_with_topics(input_data)
        return {
            'analysis': '## æ”¿ç­–å¢é‡åˆ†æ\n\nï¼ˆç”Ÿæˆå¤±è´¥ï¼‰',
            'topics': [],
            'topic_rag_results': {}
        }
    
    def analyze_with_topics(self, segment: PolicySegment, topics: List[str] = None,
                             use_dimension_analysis: bool = True) -> Dict[str, Any]:
        """
        ä½¿ç”¨å¤–éƒ¨æä¾›çš„ä¸»é¢˜è¯è¿›è¡Œåˆ†ä¸»é¢˜RAGå¢é‡åˆ†æ
        
        æ–°æµç¨‹ï¼ˆuse_dimension_analysis=Trueï¼‰ï¼š
        1. ä½¿ç”¨meetingå¯¹æ¯”æå–çš„ä¸»é¢˜è¯ï¼ˆæˆ–è‡ªåŠ¨æå–ï¼‰
        2. æ¯ä¸ªä¸»é¢˜æ‹†åˆ†ä¸º3ä¸ªç»†åˆ†ç»´åº¦
        3. æ¯ä¸ªç»´åº¦ç‹¬ç«‹RAGæ£€ç´¢ + LLMå¯¹æ¯”åˆ†æ
        4. æ±‡æ€»å„ç»´åº¦åˆ†æç”Ÿæˆä¸»é¢˜æŠ¥å‘Š
        
        æ—§æµç¨‹ï¼ˆuse_dimension_analysis=Falseï¼‰ï¼š
        1. æ¯ä¸ªä¸»é¢˜ç‹¬ç«‹RAGæ£€ç´¢å†å²æ”¿ç­–
        2. ç›´æ¥ç”Ÿæˆä¸€å¯¹å¤šå¯¹æ¯”åˆ†æ
        
        Args:
            segment: å½“å‰æ”¿ç­–æ–‡æ¡£
            topics: å¤–éƒ¨æä¾›çš„ä¸»é¢˜è¯åˆ—è¡¨ï¼ˆæ¥è‡ªmeetingå¯¹æ¯”ï¼‰
            use_dimension_analysis: æ˜¯å¦ä½¿ç”¨åˆ†ç»´åº¦ç²¾ç»†åŒ–åˆ†æ
            
        Returns:
            {
                'analysis': markdownæ–‡æœ¬,
                'topic_rag_results': {topic: [å†å²æ”¿ç­–åˆ—è¡¨], ...},
                'topics': ä½¿ç”¨çš„ä¸»é¢˜è¯åˆ—è¡¨
            }
        """
        # è®¡ç®—2å¹´æ—¶é—´çª—å£
        if segment.timestamp:
            after_timestamp = segment.timestamp - timedelta(days=730)
            self.log(f"â° æ—¶é—´çª—å£: {after_timestamp.strftime('%Y-%m-%d')} ~ {segment.timestamp.strftime('%Y-%m-%d')} (2å¹´å†…)")
        else:
            after_timestamp = None
        
        # å¦‚æœæ²¡æœ‰æä¾›ä¸»é¢˜è¯ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•æå–
        if not topics:
            self.log("æœªæä¾›ä¸»é¢˜è¯ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•æå–...")
            investment_summary = self._extract_investment_content(segment)
            topics = self._extract_investment_topics(investment_summary)
        
        self.log(f"ä½¿ç”¨ {len(topics)} ä¸ªä¸»é¢˜è¯è¿›è¡Œåˆ†ä¸»é¢˜åˆ†æ: {topics}")
        
        if use_dimension_analysis:
            # æ–°æµç¨‹ï¼šåˆ†ç»´åº¦ç²¾ç»†åŒ–RAG
            self.log("ğŸš€ ä½¿ç”¨åˆ†ç»´åº¦ç²¾ç»†åŒ–RAGæµç¨‹...")
            
            # ç›´æ¥åœ¨_generate_topic_analysis_reportä¸­å¤„ç†åˆ†ç»´åº¦é€»è¾‘
            analysis = self._generate_topic_analysis_report(
                segment=segment,
                topics=topics,
                topic_rag_results={},  # æ–°æµç¨‹ä¸éœ€è¦é¢„å…ˆRAG
                use_dimension_analysis=True
            )
            
            return {
                'analysis': analysis,
                'topic_rag_results': {},  # æ–°æµç¨‹çš„RAGç»“æœåœ¨å†…éƒ¨å¤„ç†
                'topics': topics
            }
        else:
            # æ—§æµç¨‹ï¼šæ¯ä¸ªä¸»é¢˜ç‹¬ç«‹RAG
            self.log("ğŸ“š ä½¿ç”¨ä¼ ç»ŸRAGæµç¨‹...")
            topic_rag_results = {}
            
            if self.vector_db and topics:
                for topic in topics:
                    self.log(f"  æ£€ç´¢ä¸»é¢˜: {topic}")
                    results, query_text = self._search_by_topic_with_time(
                        segment, topic, top_k=50, after_timestamp=after_timestamp
                    )
                    if results:
                        # LLMç›¸å…³æ€§è¿‡æ»¤
                        results = self._llm_relevance_rerank(
                            new_policy_title=f"{segment.title} - {topic}ä¸»é¢˜",
                            new_policy_content=query_text,
                            candidates=results,
                            top_k=30
                        )
                        if results:
                            topic_rag_results[topic] = results
                            self.log(f"    ä¸»é¢˜ '{topic}': æœ€ç»ˆä¿ç•™ {len(results)} ç¯‡")
            
            # ç”Ÿæˆåˆ†ä¸»é¢˜å¢é‡åˆ†ææŠ¥å‘Š
            self.log("ç”Ÿæˆåˆ†ä¸»é¢˜å¢é‡åˆ†ææŠ¥å‘Š...")
            analysis = self._generate_topic_analysis_report(
                segment=segment,
                topics=topics,
                topic_rag_results=topic_rag_results,
                use_dimension_analysis=False
            )
            
            return {
                'analysis': analysis,
                'topic_rag_results': topic_rag_results,
                'topics': topics
            }
    
    def _generate_topic_analysis_report(self, segment: PolicySegment, 
                                         topics: List[str],
                                         topic_rag_results: Dict[str, List[Dict]],
                                         use_dimension_analysis: bool = True) -> str:
        """
        ç”Ÿæˆåˆ†ä¸»é¢˜å¢é‡åˆ†ææŠ¥å‘Š
        
        Args:
            segment: å½“å‰æ”¿ç­–
            topics: ä¸»é¢˜è¯åˆ—è¡¨
            topic_rag_results: ä¸»é¢˜RAGç»“æœï¼ˆæ—§æµç¨‹ç”¨ï¼‰
            use_dimension_analysis: æ˜¯å¦ä½¿ç”¨åˆ†ç»´åº¦ç²¾ç»†åŒ–åˆ†æï¼ˆæ–°æµç¨‹ï¼‰
        """
        all_parts = []
        
        # æ ‡é¢˜
        all_parts.append("## åˆ†ä¸»é¢˜æ·±åº¦åˆ†æ\n\n")
        
        if not topics:
            all_parts.append("ï¼ˆæœªæå–åˆ°æŠ•èµ„ä¸»é¢˜ï¼‰\n")
            return "".join(all_parts)
        
        # æ¦‚è¿°
        all_parts.append(f"æœ¬æ¬¡åˆ†æèšç„¦ä»¥ä¸‹ **{len(topics)}** ä¸ªæ ¸å¿ƒæŠ•èµ„ä¸»é¢˜ï¼š{', '.join(topics)}\n\n")
        all_parts.append("---\n\n")
        
        # è®¡ç®—2å¹´æ—¶é—´çª—å£
        if segment.timestamp:
            after_timestamp = segment.timestamp - timedelta(days=730)
        else:
            after_timestamp = None
        
        # æ¯ä¸ªä¸»é¢˜çš„å¯¹æ¯”åˆ†æ
        topic_idx = 1
        total_docs = 0
        
        for topic in topics:
            self.log(f"ğŸ“Š å¤„ç†ä¸»é¢˜ '{topic}'...")
            
            if use_dimension_analysis and self.vector_db:
                # æ–°æµç¨‹ï¼šåˆ†ç»´åº¦ç²¾ç»†åŒ–RAG
                topic_analysis, doc_count = self._analyze_topic_by_dimensions(
                    segment=segment,
                    topic=topic,
                    topic_idx=topic_idx,
                    after_timestamp=after_timestamp
                )
                total_docs += doc_count
            else:
                # æ—§æµç¨‹ï¼šç›´æ¥ä½¿ç”¨å·²æœ‰çš„RAGç»“æœ
                topic_docs = topic_rag_results.get(topic, [])
                if not topic_docs:
                    continue
                topic_analysis = self._generate_topic_comparison(
                    segment=segment,
                    topic=topic,
                    topic_idx=topic_idx,
                    topic_docs=topic_docs
                )
                total_docs += len(topic_docs)
            
            all_parts.append(topic_analysis)
            topic_idx += 1
        
        # æ€»ç»“
        topics_str = 'ã€'.join(topics)
        summary = f"""
---

### ä¸»é¢˜åˆ†æå°ç»“

æœ¬æ¬¡åˆ†ä¸»é¢˜åˆ†æå…±æ¶‰åŠ **{len(topics)}** ä¸ªæ ¸å¿ƒæŠ•èµ„ä¸»é¢˜ï¼Œå¯¹æ¯”äº† **{total_docs}** ç¯‡ç›¸å…³å†å²æ”¿ç­–ã€‚

**æ ¸å¿ƒä¸»é¢˜**ï¼š{topics_str}

**åˆ†æè¦ç‚¹**ï¼š
- é€šè¿‡åˆ†ç»´åº¦ç²¾ç»†åŒ–æ£€ç´¢ï¼Œæé«˜äº†æ”¿ç­–å¯¹æ¯”çš„ç²¾å‡†åº¦
- æ¯ä¸ªä¸»é¢˜æ‹†åˆ†ä¸º3ä¸ªç»†åˆ†ç»´åº¦ï¼Œåˆ†åˆ«è¿›è¡ŒRAGæ£€ç´¢å’Œå¯¹æ¯”åˆ†æ
- é€šè¿‡è¡¨æ ¼å¯¹æ¯”ï¼Œæ¸…æ™°å±•ç¤ºæ–°æ—§æ”¿ç­–çš„è¾¹é™…å˜åŒ–

**åç»­è·Ÿè¸ª**ï¼šå»ºè®®æŒç»­å…³æ³¨ä¸Šè¿°ä¸»é¢˜çš„æ”¿ç­–è½åœ°è¿›å±•ã€äº§ä¸šè®¢å•å’Œäº§èƒ½å˜åŒ–ã€‚
"""
        all_parts.append(summary)
        
        return "".join(all_parts)

    def _analyze_topic_by_dimensions(self, segment: PolicySegment, topic: str,
                                      topic_idx: int, after_timestamp=None) -> tuple:
        """
        åˆ†ç»´åº¦ç²¾ç»†åŒ–åˆ†æå•ä¸ªä¸»é¢˜
        
        æµç¨‹ï¼š
        1. ç”¨LLMå°†ä¸»é¢˜æ‹†åˆ†ä¸º3ä¸ªç»†åˆ†ç»´åº¦
        2. æ¯ä¸ªç»´åº¦ï¼šæå–æ–°æ”¿ç­–å†…å®¹ â†’ RAGæ£€ç´¢ â†’ LLMç”Ÿæˆå¯¹æ¯”åˆ†æ
        3. æ±‡æ€»3ä¸ªç»´åº¦çš„åˆ†æç»“æœ
        
        Args:
            segment: å½“å‰æ”¿ç­–
            topic: ä¸»é¢˜è¯
            topic_idx: ä¸»é¢˜åºå·
            after_timestamp: æ—¶é—´çª—å£ä¸‹é™
            
        Returns:
            (ä¸»é¢˜åˆ†ææ–‡æœ¬, æ£€ç´¢åˆ°çš„å†å²æ”¿ç­–æ•°é‡)
        """
        self.log(f"  ğŸ” æ‹†åˆ†ä¸»é¢˜'{topic}'ä¸ºç»†åˆ†ç»´åº¦...")
        
        # Step 1: æ‹†åˆ†ç»´åº¦
        dimensions = self._split_topic_to_dimensions(segment, topic)
        
        if not dimensions:
            self.log(f"  âš ï¸ ä¸»é¢˜'{topic}'æ‹†åˆ†ç»´åº¦å¤±è´¥ï¼Œä½¿ç”¨æ—§æµç¨‹")
            return f"### 3.{topic_idx} {topic}\n\nï¼ˆç»´åº¦æ‹†åˆ†å¤±è´¥ï¼‰\n\n", 0
        
        # Step 2: æ¯ä¸ªç»´åº¦ç‹¬ç«‹RAG + åˆ†æ
        dimension_analyses = []
        all_history_docs = []
        
        for dim in dimensions:
            dim_name = dim.get('dimension', '')
            self.log(f"  ğŸ“Œ å¤„ç†ç»´åº¦'{dim_name}'...")
            
            # RAGæ£€ç´¢
            history_docs = self._search_by_dimension(
                segment=segment,
                topic=topic,
                dimension=dim,
                top_k=15,
                after_timestamp=after_timestamp
            )
            
            # LLMç›¸å…³æ€§è¿‡æ»¤
            if history_docs:
                history_docs = self._llm_relevance_rerank(
                    new_policy_title=f"{segment.title} - {topic}/{dim_name}",
                    new_policy_content=dim.get('content', ''),
                    candidates=history_docs,
                    top_k=10
                )
            
            all_history_docs.extend(history_docs)
            
            # ç”Ÿæˆè¯¥ç»´åº¦çš„å¯¹æ¯”åˆ†æ
            dim_analysis = self._generate_dimension_comparison(
                segment=segment,
                topic=topic,
                dimension=dim,
                history_docs=history_docs
            )
            dimension_analyses.append(dim_analysis)
        
        # Step 3: æ±‡æ€»å„ç»´åº¦åˆ†æ
        # å»é‡ç»Ÿè®¡å†å²æ”¿ç­–æ•°é‡
        unique_docs = {}
        for doc in all_history_docs:
            key = (doc.get('title', ''), doc.get('timestamp', ''))
            if key not in unique_docs:
                unique_docs[key] = doc
        
        topic_analysis = self._generate_topic_comparison(
            segment=segment,
            topic=topic,
            topic_idx=topic_idx,
            topic_docs=list(unique_docs.values()),
            dimension_analyses=dimension_analyses
        )
        
        return topic_analysis, len(unique_docs)
    
    def _extract_investment_content(self, segment: PolicySegment) -> str:
        """
        æå–æ”¿ç­–ä¸­å…·æœ‰æŠ•èµ„ç›¸å…³æ€§çš„æ ¸å¿ƒå†…å®¹
        
        Args:
            segment: æ”¿ç­–æ–‡æ¡£
            
        Returns:
            æŠ•èµ„ç›¸å…³çš„æ ¸å¿ƒå†…å®¹
        """
        prompt = f"""è¯·ä»ä»¥ä¸‹æ”¿ç­–æ–‡æ¡£ä¸­æå–**å…·æœ‰æŠ•èµ„ç›¸å…³æ€§**çš„æ ¸å¿ƒå†…å®¹ã€‚

æ”¿ç­–æ ‡é¢˜ï¼š{segment.title}

æ”¿ç­–åŸæ–‡ï¼š
{segment.content}

---

## ä»»åŠ¡è¯´æ˜

ä½ éœ€è¦æå–å¯¹**æŠ•èµ„åˆ†æ**æœ‰ä»·å€¼çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š

### å¿…é¡»ä¿ç•™çš„å†…å®¹ï¼š
1. **äº§ä¸šæ”¿ç­–**ï¼šæ”¯æŒ/é™åˆ¶å“ªäº›è¡Œä¸šã€äº§ä¸šå‡çº§æ–¹å‘
2. **é‡åŒ–ç›®æ ‡**ï¼šå…·ä½“æ•°å­—ã€ç™¾åˆ†æ¯”ã€é‡‘é¢ã€äº§èƒ½ç›®æ ‡
3. **æ—¶é—´èŠ‚ç‚¹**ï¼š2025å¹´ã€2030å¹´ç­‰å…³é”®æ—¶é—´ç‚¹çš„ç›®æ ‡
4. **è´¢æ”¿/é‡‘èæ”¯æŒ**ï¼šè¡¥è´´ã€ç¨æ”¶ä¼˜æƒ ã€ä¸“é¡¹èµ„é‡‘ã€ä¿¡è´·æ”¯æŒ
5. **é‡ç‚¹é¡¹ç›®**ï¼šåŸºç¡€è®¾æ–½ã€é‡å¤§å·¥ç¨‹ã€è¯•ç‚¹ç¤ºèŒƒ
6. **æŠ€æœ¯æ–¹å‘**ï¼šæ–°èƒ½æºã€äººå·¥æ™ºèƒ½ã€åŠå¯¼ä½“ç­‰å…·ä½“æŠ€æœ¯
7. **åŒºåŸŸå¸ƒå±€**ï¼šå“ªäº›åœ°åŒºé‡ç‚¹å‘å±•ä»€ä¹ˆäº§ä¸š

### å¿…é¡»è¿‡æ»¤æ‰çš„å†…å®¹ï¼š
1. æ”¿æ²»å®£ç¤ºè¯­ã€åŸåˆ™æ€§è¡¨è¿°
2. ç©ºæ´è¡¨æ€ã€é‡å¤å†…å®¹
3. ä¸æŠ•èµ„æ— å…³çš„è¡Œæ”¿ç®¡ç†å†…å®¹

### è¾“å‡ºè¦æ±‚ï¼š
- ç›´æ¥è¾“å‡ºæç‚¼åçš„æ ¸å¿ƒå†…å®¹
- ä¿ç•™åŸæ–‡çš„å…³é”®æ•°æ®å’Œæªæ–½
- å¯ä»¥æ•´ç†è¯­å¥ï¼Œä½†ä¸æ”¹å˜åŸæ„
- ä¸è¦åŠ æ ‡é¢˜æˆ–æ ¼å¼"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,
                max_tokens=32768
            )
            return response.strip()
        except Exception as e:
            self.log(f"æå–æŠ•èµ„å†…å®¹å¤±è´¥: {e}", level="warning")
            return segment.content
    
    def _extract_investment_topics(self, investment_content: str) -> List[str]:
        """
        ä»æŠ•èµ„æ ¸å¿ƒå†…å®¹ä¸­æå–å…³é”®æŠ•èµ„ä¸»é¢˜ï¼ˆä½¿ç”¨ä¸­ä¿¡ä¸€çº§è¡Œä¸šåˆ†ç±»ï¼‰
        
        Args:
            investment_content: æå–åçš„æŠ•èµ„ç›¸å…³æ ¸å¿ƒå†…å®¹
            
        Returns:
            ä¸»é¢˜è¯åˆ—è¡¨ï¼ˆ5-10ä¸ªï¼ŒæŒ‰é‡è¦æ€§æ’åºï¼‰
        """
        prompt = f"""è¯·ä»ä»¥ä¸‹æŠ•èµ„ç›¸å…³æ”¿ç­–å†…å®¹ä¸­æå–**æœ€æ ¸å¿ƒçš„æŠ•èµ„ä¸»é¢˜**ã€‚

æ”¿ç­–å†…å®¹ï¼š
{investment_content}

---

## ä¸­ä¿¡ä¸€çº§è¡Œä¸šåˆ†ç±»ï¼ˆå¿…é¡»ä»ä»¥ä¸‹è¡Œä¸šä¸­é€‰æ‹©ï¼‰ï¼š
é‡‘èã€ç”µå­ã€è®¡ç®—æœºã€é€šä¿¡ã€ä¼ åª’ã€åŒ»è¯ç”Ÿç‰©ã€æœºæ¢°è®¾å¤‡ã€ç”µåŠ›è®¾å¤‡ã€å›½é˜²å†›å·¥ã€æ±½è½¦ã€å®¶ç”¨ç”µå™¨ã€è½»å·¥åˆ¶é€ ã€å•†è´¸é›¶å”®ã€ç¤¾ä¼šæœåŠ¡ã€é£Ÿå“é¥®æ–™ã€å†œæ—ç‰§æ¸”ã€é’¢é“ã€æœ‰è‰²é‡‘å±ã€åŸºç¡€åŒ–å·¥ã€çŸ³æ²¹çŸ³åŒ–ã€ç…¤ç‚­ã€å»ºç­‘ææ–™ã€å»ºç­‘è£…é¥°ã€æˆ¿åœ°äº§ã€äº¤é€šè¿è¾“ã€å…¬ç”¨äº‹ä¸šã€çººç»‡æœé¥°ã€ç¾å®¹æŠ¤ç†ã€ç¯ä¿ã€ç»¼åˆ

## è¦æ±‚
1. å¿…é¡»ä»ä¸Šè¿°ä¸­ä¿¡ä¸€çº§è¡Œä¸šåˆ†ç±»ä¸­é€‰æ‹©
2. æŒ‰æ”¿ç­–ä¸­çš„é‡è¦æ€§æ’åºï¼ˆæ”¿ç­–ä¸­å…ˆæåˆ°çš„ã€ç¯‡å¹…æ›´å¤§çš„æ’åœ¨å‰é¢ï¼‰
3. æå–5-10ä¸ªæœ€ç›¸å…³çš„è¡Œä¸š
4. åªè¾“å‡ºè¡Œä¸šåç§°ï¼Œç”¨é€—å·åˆ†éš”

## è¾“å‡ºæ ¼å¼
è¡Œä¸š1,è¡Œä¸š2,è¡Œä¸š3,..."""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.1,
                max_tokens=200
            )
            topics = [t.strip() for t in response.split(',') if t.strip()]
            return topics[:10]  # æœ€å¤šè¿”å›10ä¸ª
        except Exception as e:
            self.log(f"æå–æŠ•èµ„ä¸»é¢˜å¤±è´¥: {e}", level="warning")
            return []

    def _search_by_topic_with_time(self, segment: PolicySegment, topic: str, 
                                    top_k: int = 10, after_timestamp=None) -> tuple:
        """
        æŒ‰ä¸»é¢˜æ£€ç´¢ï¼ˆå¸¦æ—¶é—´çº¦æŸï¼‰
        
        Args:
            segment: å½“å‰æ”¿ç­–
            topic: ä¸»é¢˜è¯
            top_k: è¿”å›æ•°é‡
            after_timestamp: æ—¶é—´çª—å£ä¸‹é™ï¼ˆåªæ£€ç´¢æ­¤æ—¶é—´ä¹‹åçš„æ”¿ç­–ï¼Œç”¨äº2å¹´é™åˆ¶ï¼‰
            
        Returns:
            (æ£€ç´¢ç»“æœåˆ—è¡¨, æå–çš„ä¸»é¢˜å†…å®¹query_text)
        """
        if not self.vector_db:
            return [], ""
        
        try:
            # ç”¨LLMç”Ÿæˆè¯¥ä¸»é¢˜çš„æ£€ç´¢ç‰‡æ®µ
            query_text = self._extract_topic_content(segment, topic)
            self.log(f"  ä¸»é¢˜ '{topic}' æ£€ç´¢ç‰‡æ®µ: {len(query_text)}å­—")
            
            # æ£€ç´¢ï¼ˆå¸¦2å¹´æ—¶é—´çª—å£ï¼‰
            chunk_results = self.vector_db.search_chunks(
                query_text=query_text,
                top_k=500,  # ç²—æ’500
                rerank_top_k=100,  # Rerankerç²¾æ’100
                exclude_doc_id=segment.doc_id,
                exclude_title=segment.title,
                exclude_timestamp=segment.timestamp,
                before_timestamp=segment.timestamp,
                after_timestamp=after_timestamp,  # 2å¹´æ—¶é—´çª—å£ä¸‹é™
                allow_same_day=True,
                use_reranker=True
            )
            
            self.log(f"  ä¸»é¢˜ '{topic}' RAGå¬å›: {len(chunk_results)} ä¸ªchunks")
            
            # å»é‡ + æ—¶é—´åŠ æƒ
            deduplicated = self._deduplicate_chunks(
                chunk_results=chunk_results,
                policy_timestamp=segment.timestamp,
                top_k=top_k
            )
            
            self.log(f"  ä¸»é¢˜ '{topic}' å»é‡å: {len(deduplicated)} ç¯‡")
            
            return deduplicated, query_text
            
        except Exception as e:
            self.log(f"ä¸»é¢˜æ£€ç´¢å¤±è´¥ '{topic}': {e}", level="warning")
            return [], ""
    
    def _extract_topic_content(self, segment: PolicySegment, topic: str) -> str:
        """ç”¨LLMæå–è¯¥ä¸»é¢˜ç›¸å…³çš„æ”¿ç­–å†…å®¹"""
        prompt = f"""è¯·ä»ä»¥ä¸‹æ”¿ç­–ä¸­æå–ä¸"{topic}"ç›¸å…³çš„å†…å®¹ã€‚

æ”¿ç­–æ ‡é¢˜ï¼š{segment.title}

æ”¿ç­–å†…å®¹ï¼š
{segment.content}

è¦æ±‚ï¼š
1. ç›´æ¥æ‘˜å½•åŸæ–‡ï¼Œä¸è¦æ”¹å†™
2. å®Œæ•´æå–ç›¸å…³å†…å®¹ï¼Œä¸æˆªæ–­
3. åªè¾“å‡ºæ‘˜å½•å†…å®¹"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,
                max_tokens=32768
            )
            return response.strip()
        except Exception as e:
            self.log(f"æå–ä¸»é¢˜å†…å®¹å¤±è´¥: {e}", level="warning")
            return segment.content

    def _split_topic_to_dimensions(self, segment: PolicySegment, topic: str) -> List[Dict[str, str]]:
        """
        å°†ä¸»é¢˜è¯æ‹†åˆ†ä¸º3ä¸ªç»†åˆ†ç»´åº¦
        
        Args:
            segment: å½“å‰æ”¿ç­–æ–‡æ¡£
            topic: ä¸»é¢˜è¯ï¼ˆå¦‚"å›½é˜²å†›å·¥"ï¼‰
            
        Returns:
            [
                {"dimension": "ç»´åº¦åç§°", "description": "ç»´åº¦æè¿°", "content": "æ–°æ”¿ç­–è¯¥ç»´åº¦çš„å†…å®¹"},
                ...
            ]
        """
        prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±çš„{topic}è¡Œä¸šåˆ†æå¸ˆã€‚è¯·åŸºäºæ–°æ”¿ç­–å†…å®¹ï¼Œå°†"{topic}"ä¸»é¢˜æ‹†åˆ†ä¸º3ä¸ªæœ€å…·æŠ•èµ„ä»·å€¼çš„ç»†åˆ†æ¿å—/å­é¢†åŸŸã€‚

=== æ–°æ”¿ç­– ===
ã€Š{segment.title}ã€‹

{segment.content}

=== ä»»åŠ¡ ===

è¯·å°†"{topic}"ä¸»é¢˜æ‹†åˆ†ä¸º3ä¸ªæœ€é‡è¦ä¸”å…·æœ‰æŠ•èµ„ä»·å€¼çš„ç»†åˆ†æ¿å—ï¼Œæ¯ä¸ªæ¿å—éœ€è¦ï¼š
1. æ¿å—åç§°ï¼šå…·ä½“çš„å­è¡Œä¸šæˆ–ç»†åˆ†é¢†åŸŸï¼ˆå¦‚"æ–°èƒ½æºæ±½è½¦"â†’"æ•´è½¦åˆ¶é€ "ã€"åŠ¨åŠ›ç”µæ± "ã€"å……ç”µåŸºç¡€è®¾æ–½"ï¼‰
2. æ¿å—æè¿°ï¼šè¯´æ˜è¿™ä¸ªç»†åˆ†æ¿å—åŒ…å«ä»€ä¹ˆ
3. æ–°æ”¿ç­–å†…å®¹ï¼šç›´æ¥æ‘˜å½•æ–°æ”¿ç­–ä¸­ä¸è¯¥æ¿å—ç›¸å…³çš„åŸæ–‡

=== è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ ===

```json
{{
  "dimensions": [
    {{
      "dimension": "ç»†åˆ†æ¿å—1åç§°",
      "description": "è¿™ä¸ªæ¿å—åŒ…å«ä»€ä¹ˆ",
      "content": "æ–°æ”¿ç­–ä¸­ä¸è¯¥æ¿å—ç›¸å…³çš„åŸæ–‡æ‘˜å½•"
    }},
    {{
      "dimension": "ç»†åˆ†æ¿å—2åç§°", 
      "description": "è¿™ä¸ªæ¿å—åŒ…å«ä»€ä¹ˆ",
      "content": "æ–°æ”¿ç­–ä¸­ä¸è¯¥æ¿å—ç›¸å…³çš„åŸæ–‡æ‘˜å½•"
    }},
    {{
      "dimension": "ç»†åˆ†æ¿å—3åç§°",
      "description": "è¿™ä¸ªæ¿å—åŒ…å«ä»€ä¹ˆ", 
      "content": "æ–°æ”¿ç­–ä¸­ä¸è¯¥æ¿å—ç›¸å…³çš„åŸæ–‡æ‘˜å½•"
    }}
  ]
}}
```

=== è¦æ±‚ ===
1. æ‹†åˆ†ä¸ºå…·ä½“çš„å­è¡Œä¸š/ç»†åˆ†æ¿å—ï¼Œä¸è¦æ‹†åˆ†ä¸ºåˆ†æè§’åº¦ï¼ˆå¦‚"æ”¿ç­–åŠ›åº¦"ã€"æŠ€æœ¯è·¯çº¿"ï¼‰
2. ä¼˜å…ˆé€‰æ‹©æ”¿ç­–ç€å¢¨è¾ƒå¤šã€æŠ•èµ„ä»·å€¼è¾ƒé«˜çš„ç»†åˆ†æ–¹å‘
3. æ¿å—è¦æœ‰åŒºåˆ†åº¦ï¼Œä¸è¦é‡å 
4. contentå¿…é¡»æ˜¯æ–°æ”¿ç­–åŸæ–‡æ‘˜å½•ï¼Œä¸è¦æ”¹å†™
5. åªè¾“å‡ºJSON"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,
                max_tokens=32768
            )
            
            # è§£æJSON
            import json
            import re
            response = response.strip()
            if response.startswith('```'):
                response = re.sub(r'^```\w*\n?', '', response)
                response = re.sub(r'\n?```$', '', response)
            
            result = json.loads(response)
            dimensions = result.get('dimensions', [])
            
            self.log(f"  ä¸»é¢˜'{topic}'æ‹†åˆ†ä¸º{len(dimensions)}ä¸ªç»´åº¦: {[d['dimension'] for d in dimensions]}")
            
            return dimensions
            
        except Exception as e:
            self.log(f"  æ‹†åˆ†ç»´åº¦å¤±è´¥: {e}", level="warning")
            # é™çº§ï¼šè¿”å›å•ä¸€ç»´åº¦
            return [{
                "dimension": topic,
                "description": f"{topic}ç›¸å…³æ”¿ç­–",
                "content": segment.content
            }]

    def _search_by_dimension(self, segment: PolicySegment, topic: str, 
                              dimension: Dict[str, str], top_k: int = 20,
                              after_timestamp=None) -> List[Dict]:
        """
        æŒ‰ç»´åº¦æ£€ç´¢å†å²æ”¿ç­–
        
        Args:
            segment: å½“å‰æ”¿ç­–
            topic: ä¸»é¢˜è¯
            dimension: ç»´åº¦ä¿¡æ¯ {"dimension": "ç»´åº¦å", "description": "æè¿°", "content": "æ–°æ”¿ç­–å†…å®¹"}
            top_k: è¿”å›æ•°é‡
            after_timestamp: æ—¶é—´çª—å£ä¸‹é™
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.vector_db:
            return []
        
        try:
            dim_name = dimension.get('dimension', '')
            dim_content = dimension.get('content', '')
            
            # ç”¨ç»´åº¦å†…å®¹ä½œä¸ºæ£€ç´¢query
            query_text = f"{topic} {dim_name}: {dim_content}"
            self.log(f"    ç»´åº¦'{dim_name}'æ£€ç´¢: {len(query_text)}å­—")
            
            # æ£€ç´¢
            chunk_results = self.vector_db.search_chunks(
                query_text=query_text,
                top_k=300,  # ç²—æ’300
                rerank_top_k=50,  # Rerankerç²¾æ’50
                exclude_doc_id=segment.doc_id,
                exclude_title=segment.title,
                exclude_timestamp=segment.timestamp,
                before_timestamp=segment.timestamp,
                after_timestamp=after_timestamp,
                allow_same_day=True,
                use_reranker=True
            )
            
            self.log(f"    ç»´åº¦'{dim_name}'RAGå¬å›: {len(chunk_results)}ä¸ªchunks")
            
            # å»é‡ + æ—¶é—´åŠ æƒ
            deduplicated = self._deduplicate_chunks(
                chunk_results=chunk_results,
                policy_timestamp=segment.timestamp,
                top_k=top_k
            )
            
            self.log(f"    ç»´åº¦'{dim_name}'å»é‡å: {len(deduplicated)}ç¯‡")
            
            return deduplicated
            
        except Exception as e:
            self.log(f"    ç»´åº¦æ£€ç´¢å¤±è´¥: {e}", level="warning")
            return []

    def _generate_dimension_comparison(self, segment: PolicySegment, topic: str,
                                        dimension: Dict[str, str], 
                                        history_docs: List[Dict]) -> str:
        """
        ç”Ÿæˆå•ä¸ªç»´åº¦çš„å¯¹æ¯”åˆ†æ
        
        Args:
            segment: å½“å‰æ”¿ç­–
            topic: ä¸»é¢˜è¯
            dimension: ç»´åº¦ä¿¡æ¯
            history_docs: è¯¥ç»´åº¦æ£€ç´¢åˆ°çš„å†å²æ”¿ç­–
            
        Returns:
            è¯¥ç»´åº¦çš„å¯¹æ¯”åˆ†ææ–‡æœ¬
        """
        dim_name = dimension.get('dimension', '')
        dim_desc = dimension.get('description', '')
        dim_content = dimension.get('content', '')
        
        if not history_docs:
            return f"""**{dim_name}**

æ–°æ”¿ç­–è¡¨è¿°ï¼š{dim_content}

å†å²å¯¹æ¯”ï¼šæœªæ£€ç´¢åˆ°ç›¸å…³å†å²æ”¿ç­–

"""
        
        # æ„å»ºå†å²æ”¿ç­–åˆ—è¡¨
        history_for_llm = ""
        for i, doc in enumerate(history_docs, 1):
            title = doc.get('title', 'æœªçŸ¥æ ‡é¢˜')
            timestamp = doc.get('timestamp', 'N/A')
            content = doc.get('content', '')
            history_for_llm += f"""
ã€å†å²æ”¿ç­–{i}ã€‘ã€Š{title}ã€‹ï¼ˆ{timestamp}ï¼‰
{content}
"""
        
        prompt = f"""ä½ æ˜¯{topic}è¡Œä¸šåˆ†æå¸ˆï¼Œè¯·é’ˆå¯¹"{dim_name}"è¿™ä¸ªç»´åº¦ï¼Œå¯¹æ¯”æ–°æ”¿ç­–ä¸å†å²æ”¿ç­–çš„è¾¹é™…å˜åŒ–ã€‚

=== ç»´åº¦è¯´æ˜ ===
ç»´åº¦åç§°ï¼š{dim_name}
ç»´åº¦æè¿°ï¼š{dim_desc}

=== æ–°æ”¿ç­–è¯¥ç»´åº¦å†…å®¹ ===
ã€Š{segment.title}ã€‹

{dim_content}

=== å†å²æ”¿ç­–ï¼ˆ{len(history_docs)}ç¯‡ï¼‰ ===
{history_for_llm}

=== è¾“å‡ºè¦æ±‚ ===

è¯·ç”¨è¡¨æ ¼å¯¹æ¯”æ–°æ”¿ç­–ä¸å†å²æ”¿ç­–åœ¨"{dim_name}"ç»´åº¦çš„è¾¹é™…å˜åŒ–ï¼š

**æ–°æ”¿ç­–è¡¨è¿°**ï¼š
å®Œæ•´å¼•ç”¨æ–°æ”¿ç­–åŸæ–‡

**ä¸å†å²æ”¿ç­–å¯¹æ¯”**ï¼š

| å†å²æ”¿ç­–è¡¨è¿° | è¾¹é™…å˜åŒ– |
|-------------|---------|
| ã€Šæ”¿ç­–åã€‹ï¼ˆYYYYå¹´MMæœˆï¼‰å®Œæ•´å¼•ç”¨å†å²æ”¿ç­–åŸæ–‡ | å…·ä½“è¯´æ˜å˜åŒ–å†…å®¹ï¼ˆå¦‚ï¼šæ–°å¢XXè¡¨è¿°/ä»XXå‡çº§ä¸ºXX/åˆ é™¤XXè¦æ±‚ï¼‰ |

è¦æ±‚ï¼š
- åªé€‰å–ä¸è¯¥ç»´åº¦é«˜åº¦ç›¸å…³çš„å†å²æ”¿ç­–è¿›è¡Œå¯¹æ¯”ï¼Œç›¸å…³æ€§å¼±çš„ä¸è¦æ”¾å…¥è¡¨æ ¼
- æ¯è¡Œå¯¹æ¯”ä¸€ä¸ªä¸åŒçš„å†å²æ”¿ç­–è¦ç‚¹ï¼Œä¸è¦é‡å¤
- å†å²æ”¿ç­–è¡¨è¿°è¦å¸¦ä¸Šæ”¿ç­–åç§°å’Œæ—¶é—´ï¼Œæ ¼å¼ï¼šã€Šæ”¿ç­–åã€‹ï¼ˆYYYYå¹´MMæœˆï¼‰åŸæ–‡å†…å®¹
- è¾¹é™…å˜åŒ–è¦å…·ä½“è¯´æ˜ä¸æ–°æ”¿ç­–ç›¸æ¯”çš„å˜åŒ–å†…å®¹ï¼Œä¸è¦åªå†™"å¼ºåŒ–"ã€"å»¶ç»­"ç­‰ç¬¼ç»Ÿè¯æ±‡
- åˆ—å‡º3-5ä¸ªä¸åŒçš„å¯¹æ¯”è¦ç‚¹

ç„¶åç”¨1-2å¥è¯æ€»ç»“è¯¥ç»´åº¦çš„æ ¸å¿ƒå˜åŒ–ã€‚

=== è¦æ±‚ ===
1. æ–°æ”¿ç­–è¡¨è¿°å•ç‹¬åˆ—åœ¨è¡¨æ ¼å‰é¢ï¼Œä¸è¦æ”¾åœ¨è¡¨æ ¼é‡Œ
2. è¡¨æ ¼åªæœ‰ä¸¤åˆ—ï¼šå†å²æ”¿ç­–è¡¨è¿°ã€è¾¹é™…å˜åŒ–
3. åªå±•ç¤ºä¸è¯¥ç»´åº¦é«˜åº¦ç›¸å…³çš„å¯¹æ¯”ï¼Œç›¸å…³æ€§å¼±çš„å†å²æ”¿ç­–ä¸è¦æ”¾å…¥è¡¨æ ¼
4. å†å²æ”¿ç­–è¡¨è¿°å¿…é¡»å¸¦ä¸Šã€Šæ”¿ç­–åã€‹ï¼ˆYYYYå¹´MMæœˆï¼‰ï¼Œå®Œæ•´å¼•ç”¨åŸæ–‡
5. è¾¹é™…å˜åŒ–è¦å…·ä½“æè¿°å˜åŒ–å†…å®¹ï¼Œä¸è¦åªå†™"å¼ºåŒ–/å»¶ç»­"
6. ä¸è¦ç”¨çœç•¥å·(...)çœç•¥å†…å®¹ï¼Œå®Œæ•´è¾“å‡ºæ‰€æœ‰åˆ†æ
7. åªè¾“å‡ºæ–°æ”¿ç­–è¡¨è¿°ã€è¡¨æ ¼å’Œæ€»ç»“ï¼Œä¸è¦å…¶ä»–å†…å®¹"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,
                max_tokens=32768
            )
            
            return f"""**{dim_name}**ï¼ˆ{dim_desc}ï¼‰

{response.strip()}

"""
        except Exception as e:
            self.log(f"    ç»´åº¦'{dim_name}'åˆ†æç”Ÿæˆå¤±è´¥: {e}", level="warning")
            return f"""**{dim_name}**

æ–°æ”¿ç­–è¡¨è¿°ï¼š{dim_content}

ï¼ˆåˆ†æç”Ÿæˆå¤±è´¥ï¼‰

"""
    
    def _deduplicate_chunks(self, chunk_results: List[Dict], 
                            policy_timestamp: datetime,
                            top_k: int) -> List[Dict[str, Any]]:
        """
        å»é‡ + æ—¶é—´åŠ æƒ
        æŒ‰ (title, timestamp) å»é‡ï¼Œä¿ç•™rerank_scoreæœ€é«˜çš„chunk
        """
        # æŒ‰ (title, timestamp) å»é‡ï¼Œä¿ç•™æœ€é«˜åˆ†çš„
        seen = {}
        for chunk in chunk_results:
            title = chunk.get('title', '')
            timestamp = chunk.get('timestamp', '')
            key = (title, timestamp)
            
            rerank_score = chunk.get('rerank_score', 0.0)
            
            if key not in seen or rerank_score > seen[key].get('rerank_score', 0):
                seen[key] = chunk
        
        # è½¬ä¸ºåˆ—è¡¨ï¼Œè®¡ç®—æ—¶é—´åŠ æƒ
        results = []
        for chunk in seen.values():
            # è®¡ç®—æ—¶é—´åŠ æƒ
            time_bonus = 0.0
            timestamp = chunk.get('timestamp', '')
            if timestamp and policy_timestamp:
                try:
                    if isinstance(timestamp, str):
                        if 'T' in timestamp:
                            doc_dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                        else:
                            doc_dt = datetime.fromisoformat(timestamp)
                    elif isinstance(timestamp, datetime):
                        doc_dt = timestamp
                    else:
                        doc_dt = None
                    
                    if doc_dt:
                        days_diff = (policy_timestamp.date() - doc_dt.date()).days
                        if days_diff <= 365:
                            time_bonus = 0.1 * (1 - days_diff / 365)
                        elif days_diff <= 1095:
                            time_bonus = 0.03 * (1 - (days_diff - 365) / 730)
                except:
                    pass
            
            chunk['time_bonus'] = time_bonus
            chunk['final_score'] = chunk.get('rerank_score', 0.0) + time_bonus
            
            results.append(chunk)
        
        # æŒ‰final_scoreæ’åº
        results.sort(key=lambda x: x.get('final_score', 0.0), reverse=True)
        
        return results[:top_k]

    def _llm_relevance_rerank(self, new_policy_title: str, new_policy_content: str, 
                               candidates: List[Dict], top_k: int = 10) -> List[Dict]:
        """
        ä½¿ç”¨LLMå¯¹RAGæ£€ç´¢åˆ°çš„chunkè¿›è¡Œå¤šç»´åº¦ç›¸å…³æ€§è¯„åˆ†
        
        è¯„åˆ†ç»´åº¦ï¼š
        - ä¸»é¢˜ç›¸å…³åº¦ï¼šæ˜¯å¦è®¨è®ºåŒä¸€ç»†åˆ†é¢†åŸŸ
        - æ”¿ç­–å»¶ç»­æ€§ï¼šæ˜¯å¦æ˜¯åŒä¸€æ”¿ç­–çš„è¿­ä»£/ä¿®è®¢
        - å¯¹æ¯”ä»·å€¼ï¼šå¯¹æ¯”èƒ½å¦å¾—å‡ºæœ‰æ„ä¹‰çš„å¢é‡åˆ†æç»“è®º
        
        Args:
            new_policy_title: æ–°æ”¿ç­–æ ‡é¢˜
            new_policy_content: æ–°æ”¿ç­–ä¸­è¯¥ä¸»é¢˜çš„å…·ä½“å†…å®¹ï¼ˆLLMæå–çš„ï¼‰
            candidates: RAGæ£€ç´¢åˆ°çš„chunkåˆ—è¡¨
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æŒ‰æ€»åˆ†æ’åºåçš„chunkåˆ—è¡¨ï¼ˆå¸¦è¯„åˆ†ï¼‰
        """
        if not candidates:
            return []
        
        # æ„å»ºæ‰€æœ‰chunkçš„å†…å®¹åˆ—è¡¨
        chunk_list_text = ""
        for i, chunk in enumerate(candidates, 1):
            chunk_title = chunk.get('title', '')
            chunk_content = chunk.get('content', '')
            if chunk_content:
                chunk_list_text += f"""
---
ã€{i}ã€‘ã€Š{chunk_title}ã€‹
{chunk_content}
"""
        
        # å¤šç»´åº¦è¯„åˆ†prompt
        prompt = f"""å¯¹ä»¥ä¸‹å†å²æ”¿ç­–ç‰‡æ®µä¸æ–°æ”¿ç­–çš„ç›¸å…³æ€§è¿›è¡Œ**å¤šç»´åº¦è¯„åˆ†**ã€‚

ã€æ–°æ”¿ç­–ã€‘{new_policy_title}

ã€æ–°æ”¿ç­–è¯¥ä¸»é¢˜çš„å…·ä½“å†…å®¹ã€‘
{new_policy_content}

ã€å†å²æ”¿ç­–ç‰‡æ®µåˆ—è¡¨ã€‘
{chunk_list_text}

---

## è¯„åˆ†ç»´åº¦ï¼ˆæ¯é¡¹1-5åˆ†ï¼‰

1. **ä¸»é¢˜ç›¸å…³åº¦**ï¼šå†å²æ”¿ç­–æ˜¯å¦è®¨è®ºä¸æ–°æ”¿ç­–ç›¸åŒçš„ç»†åˆ†é¢†åŸŸï¼Ÿ
   - 1åˆ†ï¼šå®Œå…¨æ— å…³ï¼ˆå¦‚æ–°æ”¿ç­–è®²å›½é˜²å†›å·¥ï¼Œå†å²æ”¿ç­–è®²åœŸåœ°ç®¡ç†ï¼‰
   - 3åˆ†ï¼šå¤§æ–¹å‘ç›¸å…³ä½†ç»†åˆ†é¢†åŸŸä¸åŒ
   - 5åˆ†ï¼šé«˜åº¦ç›¸å…³ï¼Œè®¨è®ºåŒä¸€ç»†åˆ†é¢†åŸŸçš„åŒç±»å†…å®¹

2. **æ”¿ç­–å»¶ç»­æ€§**ï¼šæ˜¯å¦æ˜¯åŒä¸€æ”¿ç­–é“¾æ¡ä¸Šçš„æ–‡ä»¶ï¼Ÿ
   - 1åˆ†ï¼šæ— å…³è”ï¼ˆå¦‚ç«‹æ³•è®¡åˆ’ã€èŒ¶è¯ä¼šè®²è¯ç­‰ï¼‰
   - 3åˆ†ï¼šç›¸å…³ä½†éç›´æ¥å»¶ç»­
   - 5åˆ†ï¼šæ˜ç¡®çš„ä¿®è®¢/å®æ–½ç»†åˆ™/é…å¥—æ”¿ç­–

3. **å¯¹æ¯”ä»·å€¼**ï¼šå¯¹æ¯”èƒ½å¦å¾—å‡ºæœ‰æ„ä¹‰çš„å¢é‡åˆ†æï¼Ÿ
   - 1åˆ†ï¼šæ— å¯¹æ¯”ä»·å€¼ï¼ˆå†…å®¹å¤ªæ³›æˆ–ä¸ç›¸å…³ï¼‰
   - 3åˆ†ï¼šæœ‰ä¸€å®šå‚è€ƒä»·å€¼
   - 5åˆ†ï¼šé«˜å¯¹æ¯”ä»·å€¼ï¼Œèƒ½çœ‹å‡ºæ˜ç¡®çš„æ”¿ç­–å˜åŒ–

## è¾“å‡ºè¦æ±‚

åªè¾“å‡ºJSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "scores": [
    {{"id": 1, "topic": 4, "continuity": 3, "value": 5, "total": 12}},
    {{"id": 2, "topic": 2, "continuity": 1, "value": 2, "total": 5}},
    ...
  ]
}}

æ³¨æ„ï¼š
- è¯„åˆ†æ‰€æœ‰ç‰‡æ®µï¼Œtotal >= 9åˆ†çš„éƒ½å€¼å¾—å¯¹æ¯”
- æ˜æ˜¾æ— å…³çš„ï¼ˆå¦‚ç«‹æ³•è®¡åˆ’ã€åœŸåœ°ç®¡ç†æ¡ä¾‹ã€èŒ¶è¯ä¼šè®²è¯ç­‰ï¼‰ç»™ä½åˆ†
- idå¯¹åº”ç‰‡æ®µç¼–å·
- å°½é‡å¤šä¿ç•™ç›¸å…³æ”¿ç­–ç”¨äºä¸€å¯¹å¤šå¯¹æ¯”"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.1,
                max_tokens=32768
            )
            
            # è§£æJSON
            import json
            import re
            response = response.strip()
            if response.startswith('```'):
                response = re.sub(r'^```\w*\n?', '', response)
                response = re.sub(r'\n?```$', '', response)
            
            result = json.loads(response)
            scores_list = result.get('scores', [])
            
            # æ„å»ºid -> è¯„åˆ†çš„æ˜ å°„
            score_map = {}
            for score_item in scores_list:
                idx = score_item.get('id')
                if idx:
                    score_map[idx] = {
                        'topic': score_item.get('topic', 0),
                        'continuity': score_item.get('continuity', 0),
                        'value': score_item.get('value', 0),
                        'total': score_item.get('total', 0)
                    }
            
            # ç»™æ¯ä¸ªcandidateæ·»åŠ è¯„åˆ†ï¼Œå¹¶è¿‡æ»¤ä½åˆ†çš„
            scored_candidates = []
            min_score = 9  # æœ€ä½æ€»åˆ†é˜ˆå€¼ï¼ˆé™ä½åˆ°9åˆ†ï¼Œä¿ç•™æ›´å¤šç›¸å…³æ”¿ç­–ç”¨äºä¸€å¯¹å¤šå¯¹æ¯”ï¼‰
            
            for i, chunk in enumerate(candidates, 1):
                if i in score_map:
                    scores = score_map[i]
                    if scores['total'] >= min_score:
                        chunk['llm_scores'] = scores
                        chunk['llm_total_score'] = scores['total']
                        scored_candidates.append(chunk)
            
            # æŒ‰æ€»åˆ†æ’åº
            scored_candidates.sort(key=lambda x: x.get('llm_total_score', 0), reverse=True)
            
            self.log(f"  LLMå¤šç»´åº¦è¯„åˆ†: {len(candidates)}ä¸ªchunk â†’ {len(scored_candidates)}ä¸ªç›¸å…³(â‰¥{min_score}åˆ†)")
            
            # æ‰“å°top3çš„è¯„åˆ†è¯¦æƒ…
            for i, chunk in enumerate(scored_candidates[:3]):
                scores = chunk.get('llm_scores', {})
                title = chunk.get('title', '')[:30]
                self.log(f"    [{i+1}] {title}... | ä¸»é¢˜:{scores.get('topic',0)} å»¶ç»­:{scores.get('continuity',0)} ä»·å€¼:{scores.get('value',0)} æ€»åˆ†:{scores.get('total',0)}")
            
            return scored_candidates[:top_k]
            
        except Exception as e:
            self.log(f"  LLMå¤šç»´åº¦è¯„åˆ†å¤±è´¥: {e}", level="warning")
            # é™çº§ï¼šè¿”å›åŸå§‹candidatesçš„å‰top_kä¸ª
            return candidates[:top_k]

    def _generate_topic_comparison(self, segment: PolicySegment, 
                                    topic: str,
                                    topic_idx: int,
                                    topic_docs: List[Dict],
                                    dimension_analyses: List[str] = None) -> str:
        """
        ç”Ÿæˆä¸»é¢˜æ·±åº¦åˆ†æï¼ˆæ±‡æ€»å„ç»´åº¦çš„åˆ†æç»“æœï¼‰
        
        Args:
            segment: å½“å‰æ”¿ç­–
            topic: ä¸»é¢˜è¯
            topic_idx: ä¸»é¢˜åºå·
            topic_docs: è¯¥ä¸»é¢˜æ£€ç´¢åˆ°çš„å†å²æ”¿ç­–ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
            dimension_analyses: å„ç»´åº¦çš„åˆ†æç»“æœåˆ—è¡¨
        """
        if dimension_analyses:
            # æ–°æµç¨‹ï¼šæ±‡æ€»å„ç»´åº¦åˆ†æ
            return self._generate_topic_summary_from_dimensions(
                segment, topic, topic_idx, topic_docs, dimension_analyses
            )
        else:
            # æ—§æµç¨‹ï¼šç›´æ¥ä¸€å¯¹å¤šå¯¹æ¯”ï¼ˆå…¼å®¹ï¼‰
            return self._generate_topic_comparison_legacy(
                segment, topic, topic_idx, topic_docs
            )

    def _generate_topic_summary_from_dimensions(self, segment: PolicySegment,
                                                  topic: str, topic_idx: int,
                                                  topic_docs: List[Dict],
                                                  dimension_analyses: List[str]) -> str:
        """
        æ±‡æ€»å„ç»´åº¦åˆ†æï¼Œç”Ÿæˆä¸»é¢˜æ€»ç»“
        """
        # åˆå¹¶å„ç»´åº¦åˆ†æ
        dimensions_content = "\n".join(dimension_analyses)
        
        # ç”¨LLMç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹ï¼ˆä¸å«æŠ•èµ„å»ºè®®ï¼‰
        prompt = f"""ä½ æ˜¯{topic}è¡Œä¸šé¦–å¸­åˆ†æå¸ˆã€‚ä»¥ä¸‹æ˜¯"{topic}"ä¸»é¢˜å„ç»´åº¦çš„æ”¿ç­–å¯¹æ¯”åˆ†æï¼Œè¯·æ’°å†™æ ¸å¿ƒè§‚ç‚¹ã€‚

=== å„ç»´åº¦åˆ†æ ===

{dimensions_content}

=== è¾“å‡ºè¦æ±‚ ===

#### æ ¸å¿ƒè§‚ç‚¹

**æŠ•èµ„è¯„çº§**ï¼šçœ‹å¤š/çœ‹å¹³/çœ‹ç©º
**æ ¸å¿ƒé€»è¾‘**ï¼šç”¨2-3å¥è¯è¯´æ¸…æ¥šæ”¿ç­–ä¿¡å·å’Œæœ€å¤§è¾¹é™…å˜åŒ–

=== è¦æ±‚ ===
1. æ ¸å¿ƒè§‚ç‚¹è¦æœ‰æ˜ç¡®åˆ¤æ–­ï¼Œä¸è¦æ¨¡æ£±ä¸¤å¯
2. åŸºäºä¸Šè¿°ç»´åº¦åˆ†æå¾—å‡ºç»“è®º
3. ä¸è¦ç”¨çœç•¥å·(...)çœç•¥å†…å®¹ï¼Œå®Œæ•´è¾“å‡ºæ‰€æœ‰åˆ†æ
4. æ€»å­—æ•°100-200å­—"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,
                max_tokens=32768
            )
            
            final_output = f"""### 3.{topic_idx} {topic}

æœ¬ä¸»é¢˜æ‹†åˆ†ä¸ºå¤šä¸ªç»´åº¦è¿›è¡Œç²¾ç»†åŒ–åˆ†æï¼Œå…±æ£€ç´¢åˆ° **{len(topic_docs)}** ç¯‡ç›¸å…³å†å²æ”¿ç­–ã€‚

---

{response.strip()}

---

#### åˆ†ç»´åº¦æ”¿ç­–å¯¹æ¯”

{dimensions_content}

"""
            return final_output
        except Exception as e:
            self.log(f"  ä¸»é¢˜'{topic}'æ±‡æ€»ç”Ÿæˆå¤±è´¥: {e}", level="warning")
            # é™çº§ï¼šç›´æ¥è¾“å‡ºå„ç»´åº¦åˆ†æ
            return f"""### 3.{topic_idx} {topic}

æœ¬ä¸»é¢˜å…±æ£€ç´¢åˆ° **{len(topic_docs)}** ç¯‡ç›¸å…³å†å²æ”¿ç­–ã€‚

---

#### åˆ†ç»´åº¦æ”¿ç­–å¯¹æ¯”

{dimensions_content}

"""

    def _generate_topic_comparison_legacy(self, segment: PolicySegment, 
                                           topic: str, topic_idx: int,
                                           topic_docs: List[Dict]) -> str:
        """
        æ—§æµç¨‹ï¼šç›´æ¥ä¸€å¯¹å¤šå¯¹æ¯”ï¼ˆå…¼å®¹ä¿ç•™ï¼‰
        """
        # æ„å»ºç»™LLMçš„å†å²æ”¿ç­–åˆ—è¡¨
        history_for_llm = ""
        for i, doc in enumerate(topic_docs, 1):
            title = doc.get('title', 'æœªçŸ¥æ ‡é¢˜')
            timestamp = doc.get('timestamp', 'N/A')
            content = doc.get('content', '')
            history_for_llm += f"""
ã€å†å²æ”¿ç­–{i}ã€‘ã€Š{title}ã€‹ï¼ˆ{timestamp}ï¼‰
{content}
"""
        
        num_history = len(topic_docs)
        
        prompt = f"""ä½ æ˜¯ä¸€åé¡¶çº§åˆ¸å•†çš„{topic}è¡Œä¸šé¦–å¸­åˆ†æå¸ˆï¼Œè¯·åŸºäºæ–°æ”¿ç­–å’Œ{num_history}ç¯‡å†å²æ”¿ç­–ï¼Œæ’°å†™ä¸€ä»½æ·±åº¦æ”¿ç­–ç‚¹è¯„ã€‚

=== æ–°æ”¿ç­– ===
ã€Š{segment.title}ã€‹ï¼ˆ{segment.timestamp.strftime('%Yå¹´%mæœˆ%dæ—¥') if segment.timestamp else 'N/A'}ï¼‰

{segment.content}

=== å†å²æ”¿ç­–ï¼ˆ{num_history}ç¯‡ï¼‰ ===
{history_for_llm}

=== åˆ†æä»»åŠ¡ ===

è¯·å¯¹æ¯”æ–°æ”¿ç­–ä¸ä¸Šè¿°{num_history}ç¯‡å†å²æ”¿ç­–ï¼Œæ’°å†™"{topic}"ä¸»é¢˜çš„æ·±åº¦åˆ†æã€‚

é‡ç‚¹å›ç­”ï¼š
1. æ–°æ”¿ç­–é‡Šæ”¾äº†ä»€ä¹ˆä¿¡å·ï¼Ÿæ”¿ç­–æ–¹å‘æ˜¯åŠ ç è¿˜æ˜¯æ”¶ç¼©ï¼Ÿ
2. ç›¸æ¯”å†å²æ”¿ç­–ï¼Œæœ‰å“ªäº›è¾¹é™…å˜åŒ–ï¼ˆæ–°å¢/å¼ºåŒ–/å¼±åŒ–ï¼‰ï¼Ÿ

=== è¾“å‡ºæ ¼å¼ ===

#### æ ¸å¿ƒè§‚ç‚¹

**æŠ•èµ„è¯„çº§**ï¼šçœ‹å¤š/çœ‹å¹³/çœ‹ç©º
**æ ¸å¿ƒé€»è¾‘**ï¼šè¯´æ¸…æ¥šæ”¿ç­–ä¿¡å·å’Œæœ€å¤§è¾¹é™…å˜åŒ–

#### æ”¿ç­–å¯¹æ¯”ä¸è¾¹é™…å˜åŒ–

**æ–°æ”¿ç­–æ ¸å¿ƒè¡¨è¿°**ï¼š
ç›´æ¥å¼•ç”¨æ–°æ”¿ç­–ä¸­å…³äº{topic}çš„é‡è¦è¡¨è¿°

**ä¸å†å²æ”¿ç­–å¯¹æ¯”**ï¼š

| å†å²æ”¿ç­–è¡¨è¿° | è¾¹é™…å˜åŒ– |
|-------------|---------|
| ã€Šæ”¿ç­–åã€‹ï¼ˆYYYYå¹´MMæœˆï¼‰å®Œæ•´å¼•ç”¨å†å²æ”¿ç­–åŸæ–‡ | å…·ä½“è¯´æ˜å˜åŒ–å†…å®¹ |

è¦æ±‚ï¼š
- æ–°æ”¿ç­–è¡¨è¿°å•ç‹¬åˆ—åœ¨è¡¨æ ¼å‰é¢ï¼Œä¸è¦æ”¾åœ¨è¡¨æ ¼é‡Œ
- è¡¨æ ¼åªæœ‰ä¸¤åˆ—ï¼šå†å²æ”¿ç­–è¡¨è¿°ã€è¾¹é™…å˜åŒ–
- åªé€‰å–ä¸è¯¥ä¸»é¢˜é«˜åº¦ç›¸å…³çš„å†å²æ”¿ç­–è¿›è¡Œå¯¹æ¯”ï¼Œç›¸å…³æ€§å¼±çš„ä¸è¦æ”¾å…¥è¡¨æ ¼
- å†å²æ”¿ç­–è¡¨è¿°è¦å¸¦ä¸Šæ”¿ç­–åç§°å’Œæ—¶é—´ï¼Œæ ¼å¼ï¼šã€Šæ”¿ç­–åã€‹ï¼ˆYYYYå¹´MMæœˆï¼‰åŸæ–‡å†…å®¹
- è¾¹é™…å˜åŒ–è¦å…·ä½“è¯´æ˜ä¸æ–°æ”¿ç­–ç›¸æ¯”çš„å˜åŒ–å†…å®¹ï¼Œä¸è¦åªå†™"å¼ºåŒ–"ã€"å»¶ç»­"
- åˆ—å‡º3-5ä¸ªä¸åŒçš„å¯¹æ¯”è¦ç‚¹ï¼Œä¸è¦é‡å¤


=== è¦æ±‚ ===

1. æ–°æ”¿ç­–è¡¨è¿°å•ç‹¬åˆ—åœ¨è¡¨æ ¼å‰é¢ï¼Œè¡¨æ ¼åªæœ‰ä¸¤åˆ—ï¼ˆå†å²æ”¿ç­–è¡¨è¿°ã€è¾¹é™…å˜åŒ–ï¼‰
2. åªå±•ç¤ºä¸è¯¥ä¸»é¢˜é«˜åº¦ç›¸å…³çš„å¯¹æ¯”ï¼Œç›¸å…³æ€§å¼±çš„å†å²æ”¿ç­–ä¸è¦æ”¾å…¥è¡¨æ ¼
3. å†å²æ”¿ç­–è¡¨è¿°å¿…é¡»å¸¦ä¸Šã€Šæ”¿ç­–åã€‹ï¼ˆYYYYå¹´MMæœˆï¼‰ï¼Œå®Œæ•´å¼•ç”¨åŸæ–‡
4. å¼•ç”¨åŸæ–‡æ—¶ç›´æ¥å†™å‡ºæ¥ï¼Œä¸è¦ç”¨ç‰¹æ®Šå¼•å·æ ¼å¼
5. è¾¹é™…å˜åŒ–è¦å…·ä½“æè¿°å˜åŒ–å†…å®¹ï¼Œä¸è¦åªå†™"å¼ºåŒ–/å»¶ç»­"ç­‰ç¬¼ç»Ÿè¯æ±‡
6. ä¸è¦ç”¨çœç•¥å·(...)çœç•¥å†…å®¹ï¼Œå®Œæ•´è¾“å‡ºæ‰€æœ‰åˆ†æ
7. æ€»å­—æ•°1500-2500å­—"""

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,
                max_tokens=32768
            )
            
            final_output = f"""### 3.{topic_idx} {topic}

æœ¬ä¸»é¢˜å…±æ£€ç´¢åˆ° **{len(topic_docs)}** ç¯‡ç›¸å…³å†å²æ”¿ç­–ã€‚

---

{response.strip()}

"""
            return final_output
        except Exception as e:
            self.log(f"  ä¸»é¢˜'{topic}'æ·±åº¦åˆ†æç”Ÿæˆå¤±è´¥: {e}", level="warning")
            return f"### 3.{topic_idx} ä¸»é¢˜ï¼š{topic}\n\nï¼ˆç”Ÿæˆå¤±è´¥ï¼‰\n\n"
