"""
ä¸€é”®è¿è¡Œï¼šå»ºç«‹RAGçŸ¥è¯†åº“
"""
import pandas as pd
import sys
import argparse
from datetime import datetime
from vector_db import MilvusVectorDatabase
from agents import IndustryAgent
from models import PolicySegment
from pathlib import Path

# ==========================================
# è§£æå‘½ä»¤è¡Œå‚æ•°
# ==========================================
parser = argparse.ArgumentParser(description='å»ºç«‹RAGçŸ¥è¯†åº“')
parser.add_argument('--data', '-d', type=str, default='./åˆå¹¶æ•°æ®_20251202_161356.parquet',
                    help='æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆparquetæ ¼å¼ï¼‰')
args = parser.parse_args()

print("="*80)
print("æ”¿ç­–åˆ†æRAGç³»ç»Ÿ - å»ºç«‹çŸ¥è¯†åº“")
print("="*80)

# ==========================================
# æ­¥éª¤1: åŠ è½½å†å²æ•°æ®
# ==========================================
print(f"\n[æ­¥éª¤1] åŠ è½½å†å²æ•°æ®...")
data_file = args.data
print(f"æ•°æ®æ–‡ä»¶: {data_file}")

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if not Path(data_file).exists():
    print(f"âŒ é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
    print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --data å‚æ•°æŒ‡å®šæ•°æ®æ–‡ä»¶è·¯å¾„")
    print(f"   ä¾‹å¦‚: python run_full_pipeline.py --data ./è¾“å‡ºæ–‡ä»¶.parquet")
    sys.exit(1)

df = pd.read_parquet(data_file)
print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(df)} ä¸ªæ–‡æ¡£")

# ==========================================
# æ­¥éª¤2: åˆå¹¶é™„ä»¶å†…å®¹åˆ°æ”¿ç­–å†…å®¹
# ==========================================
print(f"\n[æ­¥éª¤2] åˆå¹¶é™„ä»¶å†…å®¹...")

# æŸ¥æ‰¾å„ä¸ªåˆ—ï¼ˆæŒ‰åˆ—ååŒ¹é…ï¼‰
attachment_col = None
content_col = None
report_series_col = None

for col in df.columns:
    col_str = str(col)
    if 'é™„ä»¶' in col_str and 'å†…å®¹' in col_str:
        attachment_col = col
    elif 'æŠ¥å‘Šç³»åˆ—' in col_str:
        report_series_col = col
    elif content_col is None:
        if 'æ”¿ç­–å…¨æ–‡' in col_str:
            content_col = col
        elif 'å†…å®¹' in col_str and 'é™„ä»¶' not in col_str:
            content_col = col

# å¦‚æœé€šè¿‡åˆ—åæ‰¾ä¸åˆ°æ”¿ç­–å…¨æ–‡ï¼Œä½¿ç”¨ç´¢å¼•
if content_col is None and len(df.columns) > 7:
    content_col = df.columns[7]

# â­ æ˜¾ç¤ºè¯†åˆ«åˆ°çš„åˆ—
print(f"   è¯†åˆ«åˆ°çš„åˆ—:")
print(f"   - æ”¿ç­–å…¨æ–‡åˆ—: {content_col}")
print(f"   - é™„ä»¶å†…å®¹åˆ—: {attachment_col or 'æ— '}")
print(f"   - æŠ¥å‘Šç³»åˆ—åˆ—: {report_series_col or 'æ— '}")

# åˆå¹¶é™„ä»¶å†…å®¹
if attachment_col is not None and content_col is not None:
    merged_count = 0
    for idx in df.index:
        policy_content = str(df.at[idx, content_col]) if pd.notna(df.at[idx, content_col]) else ""
        attachment_content = str(df.at[idx, attachment_col]) if pd.notna(df.at[idx, attachment_col]) else ""
        
        if attachment_content and attachment_content.strip() not in ['None', 'nan', 'NaN', '']:
            if policy_content.strip():
                merged_content = f"{policy_content}\n\n---é™„ä»¶å†…å®¹---\n{attachment_content}"
            else:
                merged_content = attachment_content
            df.at[idx, content_col] = merged_content
            merged_count += 1
    
    print(f"âœ… åˆå¹¶å®Œæˆ: {merged_count} ä¸ªæ–‡æ¡£")

# ==========================================
# æ­¥éª¤3: è¿æ¥Milvuså¹¶æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£
# ==========================================
print(f"\n[æ­¥éª¤3] è¿æ¥Milvuså¹¶æ£€æŸ¥å·²å­˜åœ¨çš„æ–‡æ¡£...")

db = MilvusVectorDatabase(
    collection_name="policy_documents",
    embedding_model="./models/xiaobu-embedding-v2",
    dim=1792,
    enable_chunking=True,
    chunk_only=True
)

# â­ è·å–Milvusä¸­æœ€å¤§çš„doc_idç¼–å·ï¼Œç”¨äºç»§ç»­ç¼–å·
max_doc_id_number = db.get_max_doc_id_number()
print(f"âœ… å½“å‰Milvusä¸­æœ€å¤§doc_idç¼–å·: {max_doc_id_number}")

# â­ è·å–Milvusä¸­å·²å­˜åœ¨çš„ (æ ‡é¢˜, æ—¶é—´) ç»„åˆï¼ˆç”¨äºå»é‡ï¼‰
existing_pairs_in_milvus = db.get_existing_title_timestamp_pairs()
print(f"âœ… Milvusä¸­å·²å­˜åœ¨ {len(existing_pairs_in_milvus)} ä¸ªå”¯ä¸€æ–‡æ¡£")

# ==========================================
# æ­¥éª¤4: è½¬æ¢ä¸ºPolicySegmentå¹¶æ‰“è¡Œä¸šæ ‡ç­¾
# ==========================================
print(f"\n[æ­¥éª¤4] è½¬æ¢ä¸ºPolicySegmentå¹¶æ‰“è¡Œä¸šæ ‡ç­¾...")

segments = []
seen_titles = set()
seen_contents = set()
seen_pairs = set()  # æœ¬æ‰¹æ¬¡å†…çš„ (title, timestamp) å»é‡
skipped_existing_count = 0  # ç»Ÿè®¡è·³è¿‡çš„å·²å­˜åœ¨æ–‡æ¡£æ•°é‡

for i, row in df.iterrows():
    try:
        title = str(row.iloc[0]) if len(row) > 0 else "æœªå‘½åæ–‡æ¡£"
        
        if content_col is not None:
            content = str(row[content_col]) if pd.notna(row[content_col]) else ""
        else:
            content = str(row.iloc[7]) if len(row) > 7 else ""
        
        # è½¬æ¢æ—¶é—´æˆ³
        timestamp_value = None
        timestamp_str_for_check = ""
        if len(row) > 2:
            timestamp_str = str(row.iloc[2]).strip()
            if timestamp_str and timestamp_str.lower() not in ['', 'nan', 'none', 'nat']:
                try:
                    timestamp_value = datetime.fromisoformat(timestamp_str)
                    timestamp_str_for_check = timestamp_value.isoformat()
                except:
                    try:
                        timestamp_value = pd.to_datetime(timestamp_str)
                        timestamp_str_for_check = timestamp_value.isoformat()
                    except:
                        pass
        
        if timestamp_value is None:
            timestamp_value = datetime(2024, 1, 1)
            timestamp_str_for_check = timestamp_value.isoformat()
        
        # â­ æ£€æŸ¥Milvusä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒ (æ ‡é¢˜, æ—¶é—´) çš„æ–‡æ¡£
        check_pair = (title, timestamp_str_for_check)
        if check_pair in existing_pairs_in_milvus:
            skipped_existing_count += 1
            continue
        
        # æœ¬æ‰¹æ¬¡å†…å»é‡ï¼ˆæ ‡é¢˜+æ—¶é—´ç»„åˆï¼‰
        if check_pair in seen_pairs:
            continue
        
        # æ—§çš„å»é‡é€»è¾‘ï¼ˆæ ‡é¢˜æˆ–å†…å®¹ç›¸åŒï¼‰
        if title in seen_titles or content in seen_contents:
            continue
        
        seen_pairs.add(check_pair)
        seen_titles.add(title)
        seen_contents.add(content)
        
        # â­ è®¡ç®—æ–°çš„doc_idç¼–å·ï¼šä»æœ€å¤§ç¼–å·+1å¼€å§‹ï¼ŒæŒ‰é¡ºåºé€’å¢
        doc_id_number = max_doc_id_number + len(segments) + 1
        
        # â­ è¯»å–æŠ¥å‘Šç³»åˆ—
        # - å¦‚æœparquetæœ‰"æŠ¥å‘Šç³»åˆ—"åˆ—ï¼šç›´æ¥ä½¿ç”¨è¯¥åˆ—çš„å€¼ï¼ˆnullåˆ™ä¸ºç©ºï¼‰
        # - å¦‚æœparquetæ²¡æœ‰"æŠ¥å‘Šç³»åˆ—"åˆ—ï¼šè®¾ä¸ºç©ºå­—ç¬¦ä¸²
        report_series = ""
        if report_series_col is not None:
            rs_value = row.get(report_series_col) if hasattr(row, 'get') else row[report_series_col]
            if pd.notna(rs_value):
                report_series = str(rs_value).strip()
                # å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„ null/None
                if report_series.lower() in ['null', 'none', 'nan']:
                    report_series = ""
        
        seg = PolicySegment(
            doc_id=f"doc_{doc_id_number:04d}",
            content=content,
            title=title,
            timestamp=timestamp_value,
            industries=[],
            metadata={'report_series': report_series}  # â­ å­˜å…¥æŠ¥å‘Šç³»åˆ—
        )
        segments.append(seg)
    except Exception as e:
        print(f"  âš ï¸ æ–‡æ¡£ {i} è½¬æ¢å¤±è´¥: {e}")
        continue

print(f"âœ… è½¬æ¢å®Œæˆ: {len(segments)} ä¸ªæ–°æ–‡æ¡£éœ€è¦å…¥åº“")
if skipped_existing_count > 0:
    print(f"   è·³è¿‡ {skipped_existing_count} ä¸ªå·²å­˜åœ¨äºMilvusçš„æ–‡æ¡£ï¼ˆæ ‡é¢˜+æ—¶é—´ç›¸åŒï¼‰")

# å¦‚æœæ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦å…¥åº“ï¼Œç›´æ¥é€€å‡º
if not segments:
    print(f"\nâœ… æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦å…¥åº“ï¼Œæ‰€æœ‰æ•°æ®å·²å­˜åœ¨äºMilvusä¸­")
    stats = db.get_stats()
    print(f"   å½“å‰Chunkæ€»æ•°: {stats['total_chunks']}")
    print("\n" + "="*80)
    print("âœ… å¢é‡å…¥åº“å®Œæˆï¼")
    print("="*80)
    sys.exit(0)

# è¡Œä¸šåˆ†ç±»ï¼ˆåŒ…å«æŠ•èµ„ç›¸å…³æ€§åˆ¤æ–­ï¼Œä¸€æ¬¡DS32Bè°ƒç”¨å®Œæˆï¼‰
industry_agent = IndustryAgent()
segments = industry_agent.process(segments)
print(f"âœ… è¡Œä¸šæ ‡ç­¾å®Œæˆï¼ˆå«æŠ•èµ„ç›¸å…³æ€§åˆ¤æ–­ï¼‰")

# ==========================================
# æ­¥éª¤5: å‘é‡åŒ–å¹¶å­˜å…¥Milvus
# ==========================================
print(f"\n[æ­¥éª¤5] å‘é‡åŒ–å¹¶å­˜å…¥Milvus...")

db.add_documents(segments, batch_size=32)
stats = db.get_stats()

print(f"âœ… å…¥åº“å®Œæˆ")
print(f"   æœ¬æ¬¡æ–°å¢æ–‡æ¡£æ•°: {len(segments)}")
print(f"   å½“å‰Chunkæ€»æ•°: {stats['total_chunks']}")
print(f"   GPU: {stats['gpu_device']}")

print("\n" + "="*80)
print("âœ… çŸ¥è¯†åº“å¢é‡å…¥åº“å®Œæˆï¼")
print("="*80)
